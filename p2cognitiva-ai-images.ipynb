{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30fa7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Alzheimer Early Detection - OASIS-2\n",
    "# Notebook: Imágenes MRI con Deep Learning\n",
    "# ========================================\n",
    "\n",
    "# 1. Importación de librerías\n",
    "import os\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de6fe865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 2. Configuración de paths\n",
    "# ========================================\n",
    "\n",
    "# Carpeta donde tienes las imágenes descargadas/descomprimidas\n",
    "DATA_DIR = \"DATA/OAS2_RAW/\"   # ruta a las imágenes OASIS-2\n",
    "OUTPUT_DIR = \"DATA/processed_images/\"  # donde guardaremos PNGs\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dffc9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total archivos RAW encontrados: 1368\n",
      "Ejemplo: DATA/OAS2_RAW\\OAS2_0001_MR1\\RAW\\mpr-1.nifti.hdr\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 3. Exploración inicial de archivos\n",
    "# ========================================\n",
    "\n",
    "# Vamos a buscar imágenes en formato .hdr/.img dentro de RAW\n",
    "raw_files = glob.glob(os.path.join(DATA_DIR, \"**\", \"RAW\", \"*.hdr\"), recursive=True)\n",
    "print(f\"Total archivos RAW encontrados: {len(raw_files)}\")\n",
    "print(\"Ejemplo:\", raw_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "77a422b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creado mapa de etiquetas para 373 scans.\n",
      "Iniciando conversión de imágenes .hdr a .png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos: 100%|██████████| 1368/1368 [07:09<00:00,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversión completada.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 4. Pre-processing and Conversion\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# --- Cargar y limpiar el dataframe de etiquetas ---\n",
    "labels_df = pd.read_excel(\"oasis_longitudinal_demographics-8d83e569fa2e2d30.xlsx\")\n",
    "\n",
    "# Renombrar columnas\n",
    "rename_map = {\n",
    "    'Subject ID': 'subject_id', 'MRI ID': 'scan_id', 'Group': 'group',\n",
    "    'Visit': 'visit', 'MR Delay': 'mr_delay', 'M/F': 'sex', 'Hand': 'hand',\n",
    "    'Age': 'age', 'EDUC': 'educ', 'SES': 'ses', 'MMSE': 'mmse', 'CDR': 'cdr',\n",
    "    'eTIV': 'etiv', 'nWBV': 'nwbv', 'ASF': 'asf'\n",
    "}\n",
    "labels_df = labels_df.rename(columns=rename_map)\n",
    "\n",
    "# Crear la etiqueta binaria (0=Nondemented, 1=Demented/Converted)\n",
    "labels_df['label'] = labels_df['group'].map({'Nondemented': 0, 'Demented': 1, 'Converted': 1})\n",
    "\n",
    "# Eliminar filas donde la etiqueta es desconocida (si las hubiera)\n",
    "labels_df.dropna(subset=['label'], inplace=True)\n",
    "labels_df['label'] = labels_df['label'].astype(int)\n",
    "\n",
    "# *** CLAVE: Crear un diccionario para búsqueda rápida de etiquetas ***\n",
    "# Esto resuelve el problema de la coincidencia de IDs.\n",
    "label_map = labels_df.set_index('scan_id')['label'].to_dict()\n",
    "print(f\"Creado mapa de etiquetas para {len(label_map)} scans.\")\n",
    "\n",
    "# --- Función para guardar slices con CLAHE + z-score ---\n",
    "def save_slices_from_nifti(img_path, scan_id, max_slices=20):\n",
    "    img = nib.load(img_path)\n",
    "    data = img.get_fdata()\n",
    "\n",
    "    # Normalizar a [0,255]\n",
    "    if data.max() > 0:\n",
    "        data = (data - data.min()) / (data.max() - data.min()) * 255\n",
    "    data = data.astype(np.uint8)\n",
    "\n",
    "    # Seleccionar 20 cortes axiales alrededor del centro\n",
    "    mid_slice = data.shape[2] // 2\n",
    "    start_slice = mid_slice - max_slices // 2\n",
    "    end_slice = start_slice + max_slices\n",
    "    \n",
    "    saved_paths = []\n",
    "    slice_indices = range(start_slice, end_slice)\n",
    "\n",
    "    for i, slice_idx in enumerate(slice_indices):\n",
    "        if 0 <= slice_idx < data.shape[2]:\n",
    "            slice_img = data[:, :, slice_idx]\n",
    "\n",
    "            # --- Aplicar CLAHE y z-score normalización ---\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "            slice_img = clahe.apply(slice_img)\n",
    "\n",
    "            # Normalización z-score\n",
    "            mean, std = slice_img.mean(), slice_img.std()\n",
    "            if std > 0:\n",
    "                slice_img = ((slice_img - mean) / std) * 64 + 128\n",
    "            slice_img = np.clip(slice_img, 0, 255).astype(np.uint8)\n",
    "\n",
    "            out_path = os.path.join(OUTPUT_DIR, f\"{scan_id}_slice{i}.png\")\n",
    "            cv2.imwrite(out_path, slice_img)\n",
    "            saved_paths.append(out_path)\n",
    "\n",
    "    return saved_paths\n",
    "\n",
    "# --- *** BUCLE CORREGIDO: Procesar TODAS las imágenes *** ---\n",
    "print(\"Iniciando conversión de imágenes .hdr a .png...\")\n",
    "# Importa la librería Path\n",
    "from pathlib import Path\n",
    "\n",
    "raw_files = glob.glob(os.path.join(DATA_DIR, \"**\", \"*.hdr\"), recursive=True)\n",
    "\n",
    "for file_path in tqdm(raw_files, desc=\"Procesando archivos\"):\n",
    "    try:\n",
    "        # --- LÍNEA CORREGIDA ---\n",
    "        # Esta es la forma robusta de obtener el ID independientemente del sistema operativo.\n",
    "        # Extrae el nombre de la carpeta que está dos niveles por encima del archivo.\n",
    "        # Ejemplo: para '...\\OAS2_0001_MR1\\RAW\\mpr-1.hdr', el resultado es 'OAS2_0001_MR1'\n",
    "        p = Path(file_path)\n",
    "        scan_id = p.parent.parent.name\n",
    "\n",
    "        if scan_id in label_map: # Solo procesar imágenes que tienen etiqueta\n",
    "            save_slices_from_nifti(file_path, scan_id)\n",
    "\n",
    "    except IndexError:\n",
    "        # Este error ya no debería ocurrir, pero es bueno mantenerlo por si acaso\n",
    "        print(f\"No se pudo extraer scan_id de {file_path}\")\n",
    "\n",
    "print(\"Conversión completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "82b46fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pacientes en Train: 223 | Val: 75 | Test: 75\n",
      "Train: 4460, Val: 1500, Test: 1500\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 5. División Train/Val/Test por PACIENTE\n",
    "# ========================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Lista de pacientes (scan_ids) y sus etiquetas\n",
    "scan_ids = list(label_map.keys())\n",
    "scan_labels = [label_map[s] for s in scan_ids]\n",
    "\n",
    "# Primero separamos Train (60%) y Temp (40%)\n",
    "train_ids, temp_ids, train_labels, temp_labels = train_test_split(\n",
    "    scan_ids, scan_labels,\n",
    "    test_size=0.4, stratify=scan_labels, random_state=42\n",
    ")\n",
    "\n",
    "# Luego dividimos Temp en Val (20%) y Test (20%)\n",
    "val_ids, test_ids = train_test_split(\n",
    "    temp_ids, test_size=0.5, stratify=temp_labels, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Pacientes en Train: {len(train_ids)} | Val: {len(val_ids)} | Test: {len(test_ids)}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Dataset adaptado para aceptar subconjunto de pacientes\n",
    "# ----------------------------------------\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_map, scan_ids, transform=None, return_path=False):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_map = label_map\n",
    "        self.scan_ids = set(scan_ids)   # filtramos por IDs permitidos\n",
    "        self.transform = transform\n",
    "        self.return_path = return_path\n",
    "\n",
    "        # Solo imágenes cuyo scan_id está en el subset (train/val/test)\n",
    "        all_paths = glob.glob(os.path.join(img_dir, \"*.png\"))\n",
    "        self.img_paths = [p for p in all_paths if self.get_scan_id_from_path(p) in self.scan_ids]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def get_scan_id_from_path(self, img_path):\n",
    "        basename = os.path.basename(img_path)\n",
    "        return basename.split(\"_slice\")[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        scan_id = self.get_scan_id_from_path(img_path)\n",
    "\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) \n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = self.label_map[scan_id]\n",
    "        \n",
    "        if self.return_path:\n",
    "            return img, label, img_path\n",
    "        else:\n",
    "            return img, label\n",
    "\n",
    "# ----------------------------------------\n",
    "# Transformaciones\n",
    "# ----------------------------------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ----------------------------------------\n",
    "# Datasets con transform diferentes\n",
    "# ----------------------------------------\n",
    "train_dataset = MRIDataset(OUTPUT_DIR, label_map, train_ids, transform=train_transform)\n",
    "val_dataset   = MRIDataset(OUTPUT_DIR, label_map, val_ids, transform=val_test_transform)\n",
    "test_dataset  = MRIDataset(OUTPUT_DIR, label_map, test_ids, transform=val_test_transform, return_path=True)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c74e555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 6. Modelo Preentrenado (ResNet50)\n",
    "# ========================================\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Modelo preentrenado\n",
    "model = models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  \n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cbe3d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 7. Early Stopping\n",
    "# ========================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = np.inf\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "54c23b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 0.5279 - Train Acc: 0.7186 | Val Loss: 0.8643 - Val Acc: 0.6187\n",
      "Epoch 2/20 - Train Loss: 0.2585 - Train Acc: 0.8996 | Val Loss: 0.8215 - Val Acc: 0.6613\n",
      "Epoch 3/20 - Train Loss: 0.1182 - Train Acc: 0.9558 | Val Loss: 1.0935 - Val Acc: 0.6527\n",
      "Epoch 4/20 - Train Loss: 0.0783 - Train Acc: 0.9711 | Val Loss: 1.0667 - Val Acc: 0.7013\n",
      "Epoch 5/20 - Train Loss: 0.0533 - Train Acc: 0.9825 | Val Loss: 1.6310 - Val Acc: 0.6600\n",
      "Epoch 6/20 - Train Loss: 0.0773 - Train Acc: 0.9724 | Val Loss: 0.9106 - Val Acc: 0.7113\n",
      "⏹️ Early stopping activado\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 8. Entrenamiento del modelo\n",
    "# ========================================\n",
    "\n",
    "EPOCHS = 20\n",
    "early_stopping = EarlyStopping(patience=4)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # ---- Entrenamiento ----\n",
    "    model.train()\n",
    "    running_loss, correct = 0, 0\n",
    "    for imgs, labels_ in train_loader:\n",
    "        imgs, labels_ = imgs.to(device), labels_.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels_).sum().item()\n",
    "    \n",
    "    train_acc = correct / len(train_dataset)\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # ---- Validación ----\n",
    "    model.eval()\n",
    "    val_loss, val_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            preds = outputs.argmax(1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "    \n",
    "    val_acc = val_correct / len(val_dataset)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
    "          f\"Train Loss: {train_loss:.4f} - Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # ---- Early Stopping ----\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"⏹️ Early stopping activado\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b149561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluación en TEST (nivel paciente) ===\n",
      "[[33  5]\n",
      " [10 27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.87      0.81        38\n",
      "           1       0.84      0.73      0.78        37\n",
      "\n",
      "    accuracy                           0.80        75\n",
      "   macro avg       0.81      0.80      0.80        75\n",
      "weighted avg       0.81      0.80      0.80        75\n",
      "\n",
      "ROC-AUC: 0.8577524893314367\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 9. Evaluación del modelo (por paciente)\n",
    "# ========================================\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "model.eval()\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "all_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # El bucle ahora es más limpio, desempaquetando las rutas directamente\n",
    "    for imgs, labels, paths in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        probs = torch.softmax(outputs, dim=1)[:,1].cpu().numpy()\n",
    "        \n",
    "        # Ya no necesitas el cálculo manual de batch_start y batch_end\n",
    "        for p, prob, label in zip(paths, probs, labels):\n",
    "            scan_id = os.path.basename(p).split(\"_slice\")[0]\n",
    "            all_ids.append(scan_id)\n",
    "            all_probs.append(prob)\n",
    "            all_labels.append(label.item())\n",
    "\n",
    "# --- Agrupar por paciente ---\n",
    "patient_probs = defaultdict(list)\n",
    "patient_labels = {}\n",
    "\n",
    "for pid, prob, label in zip(all_ids, all_probs, all_labels):\n",
    "    patient_probs[pid].append(prob)\n",
    "    patient_labels[pid] = label\n",
    "\n",
    "final_probs = {pid: np.mean(probs) for pid, probs in patient_probs.items()}\n",
    "final_preds = {pid: int(prob > 0.5) for pid, prob in final_probs.items()}\n",
    "\n",
    "y_true = [patient_labels[pid] for pid in final_preds.keys()]\n",
    "y_pred = [final_preds[pid] for pid in final_preds.keys()]\n",
    "y_score = [final_probs[pid] for pid in final_preds.keys()]\n",
    "\n",
    "print(\"\\n=== Evaluación en TEST (nivel paciente) ===\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_true, y_score))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
