{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30fa7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Alzheimer Early Detection - OASIS-2\n",
    "# Notebook: Imágenes MRI con Deep Learning\n",
    "# ========================================\n",
    "\n",
    "# 1. Importación de librerías\n",
    "import os\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de6fe865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 2. Configuración de paths\n",
    "# ========================================\n",
    "\n",
    "# Carpeta donde tienes las imágenes descargadas/descomprimidas\n",
    "DATA_DIR = \"DATA/OAS2_RAW/\"   # ruta a las imágenes OASIS-2\n",
    "OUTPUT_DIR = \"DATA/processed_images/\"  # donde guardaremos PNGs\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dffc9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total archivos RAW encontrados: 1368\n",
      "Ejemplo: DATA/OAS2_RAW\\OAS2_0001_MR1\\RAW\\mpr-1.nifti.hdr\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 3. Exploración inicial de archivos\n",
    "# ========================================\n",
    "\n",
    "# Vamos a buscar imágenes en formato .hdr/.img dentro de RAW\n",
    "raw_files = glob.glob(os.path.join(DATA_DIR, \"**\", \"RAW\", \"*.hdr\"), recursive=True)\n",
    "print(f\"Total archivos RAW encontrados: {len(raw_files)}\")\n",
    "print(\"Ejemplo:\", raw_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77a422b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creado mapa de etiquetas para 373 scans.\n",
      "Iniciando conversión de imágenes .hdr a .png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos: 100%|██████████| 1368/1368 [05:44<00:00,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversión completada.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 4. Pre-processing and Conversion\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# --- Cargar y limpiar el dataframe de etiquetas ---\n",
    "labels_df = pd.read_excel(\"oasis_longitudinal_demographics-8d83e569fa2e2d30.xlsx\")\n",
    "\n",
    "# Renombrar columnas\n",
    "rename_map = {\n",
    "    'Subject ID': 'subject_id', 'MRI ID': 'scan_id', 'Group': 'group',\n",
    "    'Visit': 'visit', 'MR Delay': 'mr_delay', 'M/F': 'sex', 'Hand': 'hand',\n",
    "    'Age': 'age', 'EDUC': 'educ', 'SES': 'ses', 'MMSE': 'mmse', 'CDR': 'cdr',\n",
    "    'eTIV': 'etiv', 'nWBV': 'nwbv', 'ASF': 'asf'\n",
    "}\n",
    "labels_df = labels_df.rename(columns=rename_map)\n",
    "\n",
    "# Crear la etiqueta binaria (0=Nondemented, 1=Demented/Converted)\n",
    "labels_df['label'] = labels_df['group'].map({'Nondemented': 0, 'Demented': 1, 'Converted': 1})\n",
    "\n",
    "# Eliminar filas donde la etiqueta es desconocida (si las hubiera)\n",
    "labels_df.dropna(subset=['label'], inplace=True)\n",
    "labels_df['label'] = labels_df['label'].astype(int)\n",
    "\n",
    "# *** CLAVE: Crear un diccionario para búsqueda rápida de etiquetas ***\n",
    "# Esto resuelve el problema de la coincidencia de IDs.\n",
    "label_map = labels_df.set_index('scan_id')['label'].to_dict()\n",
    "print(f\"Creado mapa de etiquetas para {len(label_map)} scans.\")\n",
    "\n",
    "# --- Función para guardar slices (sin cambios) ---\n",
    "def save_slices_from_nifti(img_path, scan_id, max_slices=5):\n",
    "    img = nib.load(img_path)\n",
    "    data = img.get_fdata()\n",
    "    # Normalizar para guardar como imagen de 8 bits\n",
    "    if data.max() > 0:\n",
    "        data = (data - data.min()) / (data.max() - data.min()) * 255\n",
    "    data = data.astype(np.uint8)\n",
    "\n",
    "    # Seleccionar 5 cortes axiales centrales\n",
    "    mid_slice = data.shape[2] // 2\n",
    "    start_slice = mid_slice - max_slices // 2\n",
    "    end_slice = start_slice + max_slices\n",
    "    \n",
    "    saved_paths = []\n",
    "    slice_indices = range(start_slice, end_slice)\n",
    "\n",
    "    for i, slice_idx in enumerate(slice_indices):\n",
    "        if 0 <= slice_idx < data.shape[2]:\n",
    "            slice_img = data[:, :, slice_idx]\n",
    "            out_path = os.path.join(OUTPUT_DIR, f\"{scan_id}_slice{i}.png\")\n",
    "            cv2.imwrite(out_path, slice_img)\n",
    "            saved_paths.append(out_path)\n",
    "    return saved_paths\n",
    "\n",
    "# --- *** BUCLE CORREGIDO: Procesar TODAS las imágenes *** ---\n",
    "print(\"Iniciando conversión de imágenes .hdr a .png...\")\n",
    "# Importa la librería Path\n",
    "from pathlib import Path\n",
    "\n",
    "raw_files = glob.glob(os.path.join(DATA_DIR, \"**\", \"*.hdr\"), recursive=True)\n",
    "\n",
    "for file_path in tqdm(raw_files, desc=\"Procesando archivos\"):\n",
    "    try:\n",
    "        # --- LÍNEA CORREGIDA ---\n",
    "        # Esta es la forma robusta de obtener el ID independientemente del sistema operativo.\n",
    "        # Extrae el nombre de la carpeta que está dos niveles por encima del archivo.\n",
    "        # Ejemplo: para '...\\OAS2_0001_MR1\\RAW\\mpr-1.hdr', el resultado es 'OAS2_0001_MR1'\n",
    "        p = Path(file_path)\n",
    "        scan_id = p.parent.parent.name\n",
    "\n",
    "        if scan_id in label_map: # Solo procesar imágenes que tienen etiqueta\n",
    "            save_slices_from_nifti(file_path, scan_id)\n",
    "\n",
    "    except IndexError:\n",
    "        # Este error ya no debería ocurrir, pero es bueno mantenerlo por si acaso\n",
    "        print(f\"No se pudo extraer scan_id de {file_path}\")\n",
    "\n",
    "print(\"Conversión completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b46fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pacientes en Train: 298 | Pacientes en Test: 75\n",
      "Train: 1490 imágenes | Test: 375 imágenes\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 5. División Train/Test por PACIENTE\n",
    "# ========================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Lista de pacientes (scan_ids) y sus etiquetas\n",
    "scan_ids = list(label_map.keys())\n",
    "scan_labels = [label_map[s] for s in scan_ids]\n",
    "\n",
    "# División estratificada por etiqueta\n",
    "train_ids, test_ids = train_test_split(scan_ids, test_size=0.2, stratify=scan_labels, random_state=42)\n",
    "\n",
    "print(f\"Pacientes en Train: {len(train_ids)} | Pacientes en Test: {len(test_ids)}\")\n",
    "\n",
    "# Dataset adaptado para aceptar subconjunto de pacientes\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_map, scan_ids, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_map = label_map\n",
    "        self.scan_ids = set(scan_ids)   # <- filtramos por IDs permitidos\n",
    "        self.transform = transform\n",
    "        # Solo imágenes cuyo scan_id está en el subset (train/test)\n",
    "        all_paths = glob.glob(os.path.join(img_dir, \"*.png\"))\n",
    "        self.img_paths = [p for p in all_paths if self.get_scan_id_from_path(p) in self.scan_ids]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def get_scan_id_from_path(self, img_path):\n",
    "        basename = os.path.basename(img_path)\n",
    "        return basename.split(\"_slice\")[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        scan_id = self.get_scan_id_from_path(img_path)\n",
    "\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) \n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = self.label_map[scan_id]\n",
    "        \n",
    "        if self.return_path:\n",
    "            return img, label, img_path\n",
    "        else:\n",
    "            return img, label\n",
    "\n",
    "# --- Transforms (con augmentación en train) ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = MRIDataset(OUTPUT_DIR, label_map, train_ids, transform=train_transform)\n",
    "test_dataset = MRIDataset(OUTPUT_DIR, label_map, test_ids, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} imágenes | Test: {len(test_dataset)} imágenes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c74e555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 6. Modelo Preentrenado (ResNet50)\n",
    "# ========================================\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Modelo preentrenado\n",
    "model = models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  \n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54c23b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.6558 - Train Acc: 0.6128\n",
      "Epoch 2/5 - Loss: 0.4656 - Train Acc: 0.7859\n",
      "Epoch 3/5 - Loss: 0.3206 - Train Acc: 0.8691\n",
      "Epoch 4/5 - Loss: 0.1587 - Train Acc: 0.9396\n",
      "Epoch 5/5 - Loss: 0.1634 - Train Acc: 0.9443\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 7. Entrenamiento del modelo\n",
    "# ========================================\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss, correct = 0, 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "    \n",
    "    acc = correct / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {running_loss/len(train_loader):.4f} - Train Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b149561",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m all_ids = []\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels, paths \u001b[38;5;129;01min\u001b[39;00m test_loader:  \u001b[38;5;66;03m# Dataset ahora devuelve también paths\u001b[39;00m\n\u001b[32m     15\u001b[39m         imgs, labels = imgs.to(device), labels.to(device)\n\u001b[32m     16\u001b[39m         outputs = model(imgs)\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 8. Evaluación del modelo (por paciente)\n",
    "# ========================================\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "model.eval()\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "all_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels, paths in test_loader:  # Dataset ahora devuelve también paths\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        probs = torch.softmax(outputs, dim=1)[:,1].cpu().numpy()\n",
    "        \n",
    "        for p, prob, label in zip(paths, probs, labels):\n",
    "            scan_id = os.path.basename(p).split(\"_slice\")[0]  # OAS2_0001_MR1\n",
    "            all_ids.append(scan_id)\n",
    "            all_probs.append(prob)\n",
    "            all_labels.append(label.item())\n",
    "\n",
    "# --- Agrupar por paciente ---\n",
    "patient_probs = defaultdict(list)\n",
    "patient_labels = {}\n",
    "\n",
    "for pid, prob, label in zip(all_ids, all_probs, all_labels):\n",
    "    patient_probs[pid].append(prob)\n",
    "    patient_labels[pid] = label\n",
    "\n",
    "final_probs = {pid: np.mean(probs) for pid, probs in patient_probs.items()}\n",
    "final_preds = {pid: int(prob > 0.5) for pid, prob in final_probs.items()}\n",
    "\n",
    "y_true = [patient_labels[pid] for pid in final_preds.keys()]\n",
    "y_pred = [final_preds[pid] for pid in final_preds.keys()]\n",
    "y_score = [final_probs[pid] for pid in final_preds.keys()]\n",
    "\n",
    "print(\"\\n=== Evaluación en TEST (nivel paciente) ===\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_true, y_score))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
