{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3B9fiz5b2pD",
        "outputId": "907ee679-94bc-4096-870e-a4fcf953d775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# A) Setup: montado de Drive, imports, rutas y utils\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, json, math, hashlib\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, accuracy_score, precision_score,\n",
        "    recall_score, f1_score, brier_score_loss\n",
        ")\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "# Paths\n",
        "ROOT = Path(\"/content/drive/MyDrive/CognitivaAI\")\n",
        "P11  = ROOT / \"p11_alt_backbones\"\n",
        "P20  = ROOT / \"p20_meta_calibration\"\n",
        "\n",
        "P20.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "VAL_FEATS  = P11 / \"val_patient_features_backbones.csv\"\n",
        "TEST_FEATS = P11 / \"test_patient_features_backbones.csv\"\n",
        "\n",
        "assert VAL_FEATS.exists() and TEST_FEATS.exists(), \"No encuentro los features VAL/TEST en p11_alt_backbones\"\n",
        "\n",
        "# Helpers de métrica\n",
        "def metrics_from_scores(y_true, y_score, thr=0.5, cohort_name=\"ALL\"):\n",
        "    y_pred = (y_score >= thr).astype(int)\n",
        "    out = {\n",
        "        \"AUC\": float(roc_auc_score(y_true, y_score)) if len(np.unique(y_true))>1 else float(\"nan\"),\n",
        "        \"PRAUC\": float(average_precision_score(y_true, y_score)),\n",
        "        \"Acc\": float(accuracy_score(y_true, y_pred)),\n",
        "        \"P\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
        "        \"R\": float(recall_score(y_true, y_pred,  zero_division=0)),\n",
        "        \"F1\": float(f1_score(y_true, y_pred,     zero_division=0)),\n",
        "        \"Brier\": float(brier_score_loss(y_true, y_score)),\n",
        "        \"thr\": float(thr),\n",
        "        \"n\": int(len(y_true)),\n",
        "        \"cohort\": cohort_name,\n",
        "    }\n",
        "    return out\n",
        "\n",
        "def find_best_threshold(y_true, y_score, objective=\"f1\", costs=(5.0, 1.0)):\n",
        "    \"\"\"\n",
        "    objective:\n",
        "      - 'f1'  : maximiza F1\n",
        "      - 'cost': minimiza FN*costs[0] + FP*costs[1]\n",
        "    \"\"\"\n",
        "    ts = np.linspace(0.05, 0.95, 19)\n",
        "    best = None\n",
        "    for t in ts:\n",
        "        y_pred = (y_score >= t).astype(int)\n",
        "        if objective == \"f1\":\n",
        "            val = f1_score(y_true, y_pred, zero_division=0)\n",
        "            key = -val  # minimizar negativo de F1\n",
        "        else:\n",
        "            # coste FN vs FP\n",
        "            fn = np.sum((y_true==1) & (y_pred==0))\n",
        "            fp = np.sum((y_true==0) & (y_pred==1))\n",
        "            val = costs[0]*fn + costs[1]*fp\n",
        "            key = val\n",
        "        cand = (key, t)\n",
        "        if (best is None) or (cand < best):\n",
        "            best = cand\n",
        "    return best[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# B) Carga de features, limpieza de NaNs y selección de columnas\n",
        "val = pd.read_csv(VAL_FEATS)\n",
        "test = pd.read_csv(TEST_FEATS)\n",
        "\n",
        "# Cohorte por ID (heurístico OAS1 / OAS2)\n",
        "def cohort_from_pid(pid: str) -> str:\n",
        "    return \"OAS2\" if str(pid).startswith(\"OAS2\") else \"OAS1\"\n",
        "\n",
        "val[\"cohort\"]  = val[\"patient_id\"].apply(cohort_from_pid)\n",
        "test[\"cohort\"] = test[\"patient_id\"].apply(cohort_from_pid)\n",
        "\n",
        "# Columnas base que no son features\n",
        "meta_cols = [\"patient_id\", \"y_true\", \"cohort\"]\n",
        "\n",
        "# Detectar columnas de probas (features) = todo menos meta\n",
        "feat_cols = [c for c in val.columns if c not in meta_cols]\n",
        "\n",
        "# Filtro por fracción de NaN > 0.4 en VAL (aplica a TEST para coherencia)\n",
        "nan_ratio = val[feat_cols].isna().mean().sort_values(ascending=False)\n",
        "keep_cols = [c for c in feat_cols if nan_ratio[c] <= 0.40]\n",
        "\n",
        "print(\"Top-10 NaN ratio (VAL):\\n\", nan_ratio.head(10), \"\\n\")\n",
        "print(f\"✅ Mantengo {len(keep_cols)} columnas; ❌ descarto por NaN>0.4: {len(feat_cols)-len(keep_cols)}\")\n",
        "\n",
        "# Datas finales\n",
        "X_val = val[keep_cols].copy()\n",
        "X_tst = test[keep_cols].copy()\n",
        "y_val = val[\"y_true\"].astype(int).values\n",
        "y_tst = test[\"y_true\"].astype(int).values\n",
        "\n",
        "val.shape, test.shape, len(keep_cols)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm14pnKScmEY",
        "outputId": "82a6301a-9990-49ab-d4df-ec788868e077"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-10 NaN ratio (VAL):\n",
            " patient_preds_ensemble_trimmed20    0.855072\n",
            "patient_preds_ensemble_top7         0.855072\n",
            "patient_preds_ensemble_p2           0.855072\n",
            "patient_preds_ensemble_mean         0.855072\n",
            "patient_preds_trimmed20             0.855072\n",
            "patient_preds_top7                  0.855072\n",
            "patient_preds_p2                    0.855072\n",
            "patient_preds_mean                  0.855072\n",
            "slices_preds_mean                   0.855072\n",
            "slices_preds_p2                     0.855072\n",
            "dtype: float64 \n",
            "\n",
            "✅ Mantengo 36 columnas; ❌ descarto por NaN>0.4: 20\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((69, 59), (70, 59), 36)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C) OOF sin fuga para obtener scores meta \"crudos\" (antes de calibrar)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Pipelines (imputación + escalado donde aplica)\n",
        "pipe_lr = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"sc\", StandardScaler()),\n",
        "    (\"lr\", LogisticRegression(max_iter=2000, solver=\"lbfgs\", class_weight=\"balanced\"))\n",
        "])\n",
        "\n",
        "pipe_hgb = HistGradientBoostingClassifier(\n",
        "    max_depth=3, learning_rate=0.06, max_iter=200,\n",
        "    l2_regularization=0.0, random_state=42\n",
        ")\n",
        "\n",
        "oof_lr  = np.zeros(len(X_val))\n",
        "oof_hgb = np.zeros(len(X_val))\n",
        "\n",
        "for tr, va in skf.split(X_val, y_val):\n",
        "    X_tr, X_va = X_val.iloc[tr], X_val.iloc[va]\n",
        "    y_tr, y_va = y_val[tr], y_val[va]\n",
        "\n",
        "    pipe_lr.fit(X_tr, y_tr)\n",
        "    oof_lr[va] = pipe_lr.predict_proba(X_va)[:,1]\n",
        "\n",
        "    pipe_hgb.fit(X_tr, y_tr)\n",
        "    oof_hgb[va] = pipe_hgb.predict_proba(X_va)[:,1]\n",
        "\n",
        "# Entrenar modelos finales en todo VAL para inferir sobre TEST\n",
        "pipe_lr.fit(X_val, y_val)\n",
        "pipe_hgb.fit(X_val, y_val)\n",
        "\n",
        "test_lr  = pipe_lr.predict_proba(X_tst)[:,1]\n",
        "test_hgb = pipe_hgb.predict_proba(X_tst)[:,1]\n",
        "\n",
        "print(\"OOF listo:\", {\"lr\": oof_lr.shape, \"hgb\": oof_hgb.shape})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niaOZkF4csLr",
        "outputId": "843d7f73-cc1d-41e9-8725-d924d5286053"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF listo: {'lr': (69,), 'hgb': (69,)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# D) Calibración: Platt (sigmoide con LR) y Isotónica (no-paramétrica)\n",
        "#  - Global (ALL)\n",
        "#  - Por cohorte (OAS1, OAS2)\n",
        "# Se calibra a partir de OOF (y_true de VAL) y se aplica a TEST\n",
        "\n",
        "def fit_platt(y_true, scores):\n",
        "    # Platt vía LR con 1 feature (scores)\n",
        "    X = scores.reshape(-1,1)\n",
        "    pl = LogisticRegression(max_iter=1000)\n",
        "    pl.fit(X, y_true)\n",
        "    return lambda s: pl.predict_proba(np.asarray(s).reshape(-1,1))[:,1]\n",
        "\n",
        "def fit_isotonic(y_true, scores):\n",
        "    # Isotónica requiere y_true con ambas clases\n",
        "    if len(np.unique(y_true)) < 2:\n",
        "        # fallback: identidad si no hay dos clases\n",
        "        return lambda s: np.asarray(s, dtype=float)\n",
        "    ir = IsotonicRegression(out_of_bounds=\"clip\")\n",
        "    ir.fit(scores, y_true)\n",
        "    return lambda s: ir.predict(np.asarray(s, dtype=float))\n",
        "\n",
        "def calibrate_by_group(val_df, base_scores, test_df, test_scores, group_col=\"cohort\"):\n",
        "    # val_df / test_df con columnas: y_true, cohort\n",
        "    df_v = val_df[[\"y_true\", group_col]].copy()\n",
        "    df_t = test_df[[group_col]].copy()\n",
        "\n",
        "    # 1) Global fit\n",
        "    cal_platt_all   = fit_platt(df_v[\"y_true\"].values, base_scores)\n",
        "    cal_isotonic_all= fit_isotonic(df_v[\"y_true\"].values, base_scores)\n",
        "\n",
        "    out = {\n",
        "        \"global\": {\n",
        "            \"platt\":   cal_platt_all,\n",
        "            \"isotonic\":cal_isotonic_all\n",
        "        },\n",
        "        \"by_group\": {}\n",
        "    }\n",
        "\n",
        "    # 2) Por grupo\n",
        "    for g in df_v[group_col].unique():\n",
        "        m = (df_v[group_col]==g).values\n",
        "        cal_p = fit_platt(df_v.loc[m,\"y_true\"].values,  base_scores[m])\n",
        "        cal_i = fit_isotonic(df_v.loc[m,\"y_true\"].values,base_scores[m])\n",
        "        out[\"by_group\"][g] = {\"platt\":cal_p, \"isotonic\":cal_i}\n",
        "\n",
        "    # Aplicadores convenientes sobre arrays\n",
        "    def apply_cal(mode=\"global\", method=\"platt\", scores=None, cohort=None):\n",
        "        if mode==\"global\":\n",
        "            f = out[\"global\"][method]\n",
        "            return f(scores)\n",
        "        else:\n",
        "            # por cohorte (si no existe calibrador para ese grupo, cae a global)\n",
        "            res = np.zeros_like(scores, dtype=float)\n",
        "            for g in np.unique(cohort):\n",
        "                idx = (cohort==g)\n",
        "                if g in out[\"by_group\"]:\n",
        "                    res[idx] = out[\"by_group\"][g][method](scores[idx])\n",
        "                else:\n",
        "                    res[idx] = out[\"global\"][method](scores[idx])\n",
        "            return res\n",
        "\n",
        "    return out, apply_cal\n",
        "\n",
        "# Prepara DataFrames con y_true/cohort para reuso\n",
        "val_ref  = val[[\"patient_id\",\"y_true\",\"cohort\"]].copy()\n",
        "test_ref = test[[\"patient_id\",\"y_true\",\"cohort\"]].copy()\n",
        "\n",
        "# Calibración de HGB (ejemplo principal) y de LR (secundario)\n",
        "cal_hgb, hgb_apply = calibrate_by_group(val_ref, oof_hgb, test_ref, test_hgb, group_col=\"cohort\")\n",
        "cal_lr,  lr_apply  = calibrate_by_group(val_ref, oof_lr,  test_ref, test_lr,  group_col=\"cohort\")\n",
        "\n",
        "# Versiones de scores calibrados (global vs per-cohort)\n",
        "val_hgb_platt_g   = hgb_apply(\"global\", \"platt\", oof_hgb)\n",
        "val_hgb_iso_g     = hgb_apply(\"global\", \"isotonic\", oof_hgb)\n",
        "tst_hgb_platt_g   = hgb_apply(\"global\", \"platt\", test_hgb)\n",
        "tst_hgb_iso_g     = hgb_apply(\"global\", \"isotonic\", test_hgb)\n",
        "\n",
        "val_hgb_platt_c   = hgb_apply(\"by_group\",\"platt\", oof_hgb, cohort=val[\"cohort\"].values)\n",
        "val_hgb_iso_c     = hgb_apply(\"by_group\",\"isotonic\", oof_hgb, cohort=val[\"cohort\"].values)\n",
        "tst_hgb_platt_c   = hgb_apply(\"by_group\",\"platt\", test_hgb, cohort=test[\"cohort\"].values)\n",
        "tst_hgb_iso_c     = hgb_apply(\"by_group\",\"isotonic\", test_hgb, cohort=test[\"cohort\"].values)\n",
        "\n",
        "print(\"Calibraciones listas (HGB/LR; global y por cohorte).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF88uavHcx4L",
        "outputId": "2a085524-bc52-45b1-934e-4a268d9f2e2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calibraciones listas (HGB/LR; global y por cohorte).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# E) Umbrales: F1 y coste (FN:FP=5:1 por defecto)\n",
        "def eval_all(y_v, y_t, s_v, s_t, cohorts_v, cohorts_t, tag):\n",
        "    # 1) Global - F1\n",
        "    thr_f1 = find_best_threshold(y_v, s_v, objective=\"f1\")\n",
        "    m_val  = metrics_from_scores(y_v, s_v, thr=thr_f1, cohort_name=\"ALL\")\n",
        "    m_tst  = metrics_from_scores(y_t, s_t, thr=thr_f1, cohort_name=\"ALL\")\n",
        "\n",
        "    # 2) Global - Coste\n",
        "    thr_cost = find_best_threshold(y_v, s_v, objective=\"cost\", costs=(5.0,1.0))\n",
        "    mc_val   = metrics_from_scores(y_v, s_v, thr=thr_cost, cohort_name=\"ALL\")\n",
        "    mc_tst   = metrics_from_scores(y_t, s_t, thr=thr_cost, cohort_name=\"ALL\")\n",
        "\n",
        "    # 3) Por cohorte (F1 y Coste)\n",
        "    def per_cohort(y, s, cohorts, thr_map=None, objective=\"f1\"):\n",
        "        res = {}\n",
        "        for g in [\"OAS1\",\"OAS2\"]:\n",
        "            idx = (cohorts==g)\n",
        "            if idx.sum()==0 or len(np.unique(y[idx]))<1:\n",
        "                res[g] = {\"AUC\": np.nan, \"PRAUC\": np.nan, \"Acc\": np.nan, \"P\": np.nan, \"R\": np.nan, \"F1\": np.nan, \"Brier\": np.nan, \"thr\": np.nan, \"n\": int(idx.sum()), \"cohort\": g}\n",
        "                continue\n",
        "            if thr_map is None:\n",
        "                t = find_best_threshold(y[idx], s[idx], objective=objective, costs=(5.0,1.0))\n",
        "            else:\n",
        "                t = thr_map[g]\n",
        "            res[g] = metrics_from_scores(y[idx], s[idx], thr=t, cohort_name=g)\n",
        "        return res\n",
        "\n",
        "    # Cohort thresholds from VAL (F1 and Cost)\n",
        "    thr_map_f1   = {}\n",
        "    thr_map_cost = {}\n",
        "    for g in [\"OAS1\",\"OAS2\"]:\n",
        "        mv = per_cohort(y_v, s_v, cohorts_v, thr_map=None, objective=\"f1\")[g]\n",
        "        thr_map_f1[g] = mv[\"thr\"] if not math.isnan(mv[\"thr\"]) else 0.5\n",
        "        mv = per_cohort(y_v, s_v, cohorts_v, thr_map=None, objective=\"cost\")[g]\n",
        "        thr_map_cost[g] = mv[\"thr\"] if not math.isnan(mv[\"thr\"]) else 0.5\n",
        "\n",
        "    # Evaluación por cohorte aplicando los thr de VAL a TEST\n",
        "    pv_f1  = per_cohort(y_v, s_v, cohorts_v, thr_map=thr_map_f1,   objective=\"f1\")\n",
        "    pt_f1  = per_cohort(y_t, s_t, cohorts_t, thr_map=thr_map_f1,   objective=\"f1\")\n",
        "    pv_cst = per_cohort(y_v, s_v, cohorts_v, thr_map=thr_map_cost, objective=\"cost\")\n",
        "    pt_cst = per_cohort(y_t, s_t, cohorts_t, thr_map=thr_map_cost, objective=\"cost\")\n",
        "\n",
        "    summary = {\n",
        "        \"tag\": tag,\n",
        "        \"GLOBAL_F1\":   {\"VAL\": m_val, \"TEST\": m_tst, \"thr\": thr_f1},\n",
        "        \"GLOBAL_COST\": {\"VAL\": mc_val, \"TEST\": mc_tst, \"thr\": thr_cost},\n",
        "        \"COHORT_F1\":   {\"VAL\": pv_f1, \"TEST\": pt_f1, \"thr_map\": thr_map_f1},\n",
        "        \"COHORT_COST\": {\"VAL\": pv_cst, \"TEST\": pt_cst, \"thr_map\": thr_map_cost},\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "val_coh = val[\"cohort\"].values\n",
        "tst_coh = test[\"cohort\"].values\n",
        "\n",
        "SUMMARIES = []\n",
        "# HGB calibrated: global/platt, global/isotonic, cohort/platt, cohort/isotonic\n",
        "SUMMARIES.append(eval_all(y_val, y_tst, val_hgb_platt_g, tst_hgb_platt_g, val_coh, tst_coh, tag=\"HGB|Platt|Global\"))\n",
        "SUMMARIES.append(eval_all(y_val, y_tst, val_hgb_iso_g,   tst_hgb_iso_g,   val_coh, tst_coh, tag=\"HGB|Isotonic|Global\"))\n",
        "SUMMARIES.append(eval_all(y_val, y_tst, val_hgb_platt_c, tst_hgb_platt_c, val_coh, tst_coh, tag=\"HGB|Platt|PerCohort\"))\n",
        "SUMMARIES.append(eval_all(y_val, y_tst, val_hgb_iso_c,   tst_hgb_iso_c,   val_coh, tst_coh, tag=\"HGB|Isotonic|PerCohort\"))\n",
        "\n",
        "# También LR calibrado como referencia\n",
        "val_lr_platt_g = lr_apply(\"global\", \"platt\", oof_lr)\n",
        "tst_lr_platt_g = lr_apply(\"global\", \"platt\", test_lr)\n",
        "SUMMARIES.append(eval_all(y_val, y_tst, val_lr_platt_g, tst_lr_platt_g, val_coh, tst_coh, tag=\"LR|Platt|Global\"))\n",
        "\n",
        "# Vista rápida por consola\n",
        "for s in SUMMARIES:\n",
        "    print(\"\\n===\", s[\"tag\"], \"===\")\n",
        "    print(\"[GLOBAL_F1][VAL]\",  s[\"GLOBAL_F1\"][\"VAL\"])\n",
        "    print(\"[GLOBAL_F1][TEST]\", s[\"GLOBAL_F1\"][\"TEST\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju6g818wc3I5",
        "outputId": "822200ba-aa6d-4bf7-9bd8-8abca7a48471"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== HGB|Platt|Global ===\n",
            "[GLOBAL_F1][VAL] {'AUC': 0.7894736842105263, 'PRAUC': 0.755755246104314, 'Acc': 0.7101449275362319, 'P': 0.6170212765957447, 'R': 0.9354838709677419, 'F1': 0.7435897435897436, 'Brier': 0.18579872787793636, 'thr': 0.3, 'n': 69, 'cohort': 'ALL'}\n",
            "[GLOBAL_F1][TEST] {'AUC': 0.6800986842105263, 'PRAUC': 0.6176791928144115, 'Acc': 0.6, 'P': 0.5434782608695652, 'R': 0.78125, 'F1': 0.6410256410256411, 'Brier': 0.22502621998297748, 'thr': 0.3, 'n': 70, 'cohort': 'ALL'}\n",
            "\n",
            "=== HGB|Isotonic|Global ===\n",
            "[GLOBAL_F1][VAL] {'AUC': 0.8234295415959253, 'PRAUC': 0.7489112881819754, 'Acc': 0.7246376811594203, 'P': 0.6304347826086957, 'R': 0.9354838709677419, 'F1': 0.7532467532467533, 'Brier': 0.15869148758953858, 'thr': 0.15, 'n': 69, 'cohort': 'ALL'}\n",
            "[GLOBAL_F1][TEST] {'AUC': 0.6846217105263158, 'PRAUC': 0.5947630494505495, 'Acc': 0.6, 'P': 0.5434782608695652, 'R': 0.78125, 'F1': 0.6410256410256411, 'Brier': 0.24805148633873583, 'thr': 0.15, 'n': 70, 'cohort': 'ALL'}\n",
            "\n",
            "=== HGB|Platt|PerCohort ===\n",
            "[GLOBAL_F1][VAL] {'AUC': 0.830220713073005, 'PRAUC': 0.7927178012388632, 'Acc': 0.7101449275362319, 'P': 0.6170212765957447, 'R': 0.9354838709677419, 'F1': 0.7435897435897436, 'Brier': 0.1823710660361808, 'thr': 0.3, 'n': 69, 'cohort': 'ALL'}\n",
            "[GLOBAL_F1][TEST] {'AUC': 0.6800986842105263, 'PRAUC': 0.6176791928144115, 'Acc': 0.6, 'P': 0.5434782608695652, 'R': 0.78125, 'F1': 0.6410256410256411, 'Brier': 0.22497793049843057, 'thr': 0.3, 'n': 70, 'cohort': 'ALL'}\n",
            "\n",
            "=== HGB|Isotonic|PerCohort ===\n",
            "[GLOBAL_F1][VAL] {'AUC': 0.8399830220713074, 'PRAUC': 0.780184396207744, 'Acc': 0.7246376811594203, 'P': 0.6304347826086957, 'R': 0.9354838709677419, 'F1': 0.7532467532467533, 'Brier': 0.15567632850241547, 'thr': 0.15, 'n': 69, 'cohort': 'ALL'}\n",
            "[GLOBAL_F1][TEST] {'AUC': 0.6788651315789473, 'PRAUC': 0.5915872364424194, 'Acc': 0.6, 'P': 0.5434782608695652, 'R': 0.78125, 'F1': 0.6410256410256411, 'Brier': 0.25292530441295447, 'thr': 0.15, 'n': 70, 'cohort': 'ALL'}\n",
            "\n",
            "=== LR|Platt|Global ===\n",
            "[GLOBAL_F1][VAL] {'AUC': 0.7427843803056027, 'PRAUC': 0.7124693989578967, 'Acc': 0.6376811594202898, 'P': 0.56, 'R': 0.9032258064516129, 'F1': 0.691358024691358, 'Brier': 0.20921207566029942, 'thr': 0.39999999999999997, 'n': 69, 'cohort': 'ALL'}\n",
            "[GLOBAL_F1][TEST] {'AUC': 0.6858552631578948, 'PRAUC': 0.6711774897027276, 'Acc': 0.6285714285714286, 'P': 0.5681818181818182, 'R': 0.78125, 'F1': 0.6578947368421053, 'Brier': 0.22149064782053182, 'thr': 0.39999999999999997, 'n': 70, 'cohort': 'ALL'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# F) Guardado de salidas: CSVs con predicciones calibradas y JSON resumen\n",
        "\n",
        "def save_preds(tag, pid, y_true, cohort, scores, prefix=\"p20\"):\n",
        "    df = pd.DataFrame({\n",
        "        \"patient_id\": pid,\n",
        "        \"cohort\": cohort,\n",
        "        \"y_true\": y_true,\n",
        "        \"y_score\": scores\n",
        "    })\n",
        "    out = P20 / f\"{prefix}_{tag.replace('|','_')}_preds.csv\"\n",
        "    df.to_csv(out, index=False)\n",
        "    return str(out)\n",
        "\n",
        "# Guardar algunas variantes útiles\n",
        "paths = {}\n",
        "paths[\"VAL_HGB_Platt_Global\"]   = save_preds(\"VAL_HGB_Platt_Global\",   val[\"patient_id\"].values, y_val, val_coh, val_hgb_platt_g)\n",
        "paths[\"TEST_HGB_Platt_Global\"]  = save_preds(\"TEST_HGB_Platt_Global\",  test[\"patient_id\"].values, y_tst, tst_coh, tst_hgb_platt_g)\n",
        "paths[\"VAL_HGB_Isotonic_Global\"]= save_preds(\"VAL_HGB_Isotonic_Global\",val[\"patient_id\"].values, y_val, val_coh, val_hgb_iso_g)\n",
        "paths[\"TEST_HGB_Isotonic_Global\"]=save_preds(\"TEST_HGB_Isotonic_Global\",test[\"patient_id\"].values, y_tst, tst_coh, tst_hgb_iso_g)\n",
        "paths[\"VAL_HGB_Platt_PerC\"]     = save_preds(\"VAL_HGB_Platt_PerC\",     val[\"patient_id\"].values, y_val, val_coh, val_hgb_platt_c)\n",
        "paths[\"TEST_HGB_Platt_PerC\"]    = save_preds(\"TEST_HGB_Platt_PerC\",    test[\"patient_id\"].values, y_tst, tst_coh, tst_hgb_platt_c)\n",
        "paths[\"VAL_HGB_Isotonic_PerC\"]  = save_preds(\"VAL_HGB_Isotonic_PerC\",  val[\"patient_id\"].values, y_val, val_coh, val_hgb_iso_c)\n",
        "paths[\"TEST_HGB_Isotonic_PerC\"] = save_preds(\"TEST_HGB_Isotonic_PerC\", test[\"patient_id\"].values, y_tst, tst_coh, tst_hgb_iso_c)\n",
        "\n",
        "# Resumen JSON con todas las tablas de métricas y rutas de salida\n",
        "summary = {\n",
        "    \"inputs\": {\n",
        "        \"VAL_FEATS\": str(VAL_FEATS),\n",
        "        \"TEST_FEATS\": str(TEST_FEATS),\n",
        "        \"kept_features\": keep_cols\n",
        "    },\n",
        "    \"summaries\": SUMMARIES,\n",
        "    \"paths\": paths\n",
        "}\n",
        "SUM_PATH = P20 / \"p20_calibration_thresholding_summary.json\"\n",
        "with open(SUM_PATH, \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"💾 Guardados en:\", P20)\n",
        "print(\"• JSON resumen:\", SUM_PATH)\n",
        "for k,v in paths.items():\n",
        "    print(\"•\", k, \"->\", v)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdrXKJFTdBz8",
        "outputId": "e5843424-87e7-45eb-ce8c-f0813e0b3116"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Guardados en: /content/drive/MyDrive/CognitivaAI/p20_meta_calibration\n",
            "• JSON resumen: /content/drive/MyDrive/CognitivaAI/p20_meta_calibration/p20_calibration_thresholding_summary.json\n",
            "• VAL_HGB_Platt_Global -> /content/drive/MyDrive/CognitivaAI/p20_meta_calibration/p20_VAL_HGB_Platt_Global_preds.csv\n",
            "• TEST_HGB_Platt_Global -> /content/drive/MyDrive/CognitivaAI/p20_meta_calibration/p20_TEST_HGB_Platt_Global_preds.csv\n",
            "• VAL_HGB_Isotonic_Global -> /content/drive/MyDrive/CognitivaAI/p20_meta_calibration/p20_VAL_HGB_Isotonic_Global_preds.csv\n",
            "• TEST_HGB_Isotonic_Global -> /content/drive/MyDrive/CognitivaAI/p20_meta_calibration/p20_TEST_HGB_Isotonic_Global_preds.csv\n",
            "• VAL_HGB_Platt_PerC -> /content/drive/MyDrive/CognitivaAI/p20_meta_calibration/p20_VAL_HGB_Platt_PerC_preds.csv\n",
            "• TEST_HGB_Platt_PerC -> /content/drive/MyDrive/CognitivaAI/p20_meta_calibration/p20_TEST_HGB_Platt_PerC_preds.csv\n",
            "• VAL_HGB_Isotonic_PerC -> /content/drive/MyDrive/CognitivaAI/p20_meta_calibration/p20_VAL_HGB_Isotonic_PerC_preds.csv\n",
            "• TEST_HGB_Isotonic_PerC -> /content/drive/MyDrive/CognitivaAI/p20_meta_calibration/p20_TEST_HGB_Isotonic_PerC_preds.csv\n"
          ]
        }
      ]
    }
  ]
}