{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bsj6lGhFw_Yr",
        "outputId": "2fb06403-6b9b-4bfa-f920-91efe35c18ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Rutas listas: /content/drive/MyDrive/CognitivaAI/p11_alt_backbones /content/drive/MyDrive/CognitivaAI/p22_meta_elastic\n"
          ]
        }
      ],
      "source": [
        "# === p22_A: Setup & paths ===\n",
        "from pathlib import Path\n",
        "import os, json, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, accuracy_score,\n",
        "    precision_score, recall_score, f1_score, brier_score_loss\n",
        ")\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# Monta Drive si hace falta\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Rutas\n",
        "P11 = Path(\"/content/drive/MyDrive/CognitivaAI/p11_alt_backbones\")\n",
        "P22 = Path(\"/content/drive/MyDrive/CognitivaAI/p22_meta_elastic\")\n",
        "P22.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "VAL_PATH  = P11/\"val_patient_features_backbones.csv\"\n",
        "TEST_PATH = P11/\"test_patient_features_backbones.csv\"\n",
        "\n",
        "assert VAL_PATH.exists() and TEST_PATH.exists(), \"No encuentro los features de p11 (VAL/TEST).\"\n",
        "\n",
        "print(\"Rutas listas:\", P11, P22)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === p22_B: Load + sanitize + cohort ===\n",
        "val = pd.read_csv(VAL_PATH)\n",
        "test = pd.read_csv(TEST_PATH)\n",
        "\n",
        "# Esperamos columnas: ['patient_id','y_true', <features...>]\n",
        "assert \"patient_id\" in val.columns and \"y_true\" in val.columns, \"Faltan columnas clave en VAL\"\n",
        "assert \"patient_id\" in test.columns and \"y_true\" in test.columns, \"Faltan columnas clave en TEST\"\n",
        "\n",
        "# Añade cohorte desde patient_id (OAS1/OAS2)\n",
        "def cohort_from_pid(pid: str) -> str:\n",
        "    pid = str(pid)\n",
        "    if pid.startswith(\"OAS1\"): return \"OAS1\"\n",
        "    if pid.startswith(\"OAS2\"): return \"OAS2\"\n",
        "    return \"UNK\"\n",
        "\n",
        "val[\"cohort\"]  = val[\"patient_id\"].map(cohort_from_pid)\n",
        "test[\"cohort\"] = test[\"patient_id\"].map(cohort_from_pid)\n",
        "\n",
        "# Selección de columnas numéricas de features (excluye id/target/cohort)\n",
        "drop_like = {\"patient_id\",\"y_true\",\"cohort\"}\n",
        "num_cols  = [c for c in val.columns if c not in drop_like]\n",
        "\n",
        "# Filtra por NaN ratio≤0.4 (en VAL, que es nuestro set “de diseño”)\n",
        "nan_ratio = val[num_cols].isna().mean().sort_values(ascending=False)\n",
        "keep_cols = [c for c in num_cols if nan_ratio.get(c,0.0) <= 0.40]\n",
        "\n",
        "print(\"VAL:\", val.shape, \"| TEST:\", test.shape)\n",
        "print(\"Cols totales:\", len(num_cols), \"| Mantengo:\", len(keep_cols), \"| Descarto:\", len(num_cols) - len(keep_cols))\n",
        "\n",
        "print(\"\\nTop-10 NaN ratio (VAL):\\n\", nan_ratio.head(10))\n",
        "\n",
        "# Construye matrices\n",
        "X_val  = val[keep_cols].copy()\n",
        "y_val  = val[\"y_true\"].astype(int).values\n",
        "X_test = test[keep_cols].copy()\n",
        "y_test = test[\"y_true\"].astype(int).values\n",
        "\n",
        "val_ids, test_ids = val[\"patient_id\"].values, test[\"patient_id\"].values\n",
        "val_coh, test_coh = val[\"cohort\"].values, test[\"cohort\"].values\n",
        "\n",
        "print(f\"\\nShapes -> X_val:{X_val.shape} | X_test:{X_test.shape} | y_val:{y_val.shape} | y_test:{y_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3Pzf26nxd3_",
        "outputId": "8390e08c-762b-4166-a515-2cfbe6f7eecf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL: (69, 59) | TEST: (70, 59)\n",
            "Cols totales: 56 | Mantengo: 36 | Descarto: 20\n",
            "\n",
            "Top-10 NaN ratio (VAL):\n",
            " patient_preds_ensemble_trimmed20    0.855072\n",
            "patient_preds_ensemble_top7         0.855072\n",
            "patient_preds_ensemble_p2           0.855072\n",
            "patient_preds_ensemble_mean         0.855072\n",
            "patient_preds_trimmed20             0.855072\n",
            "patient_preds_top7                  0.855072\n",
            "patient_preds_p2                    0.855072\n",
            "patient_preds_mean                  0.855072\n",
            "slices_preds_mean                   0.855072\n",
            "slices_preds_p2                     0.855072\n",
            "dtype: float64\n",
            "\n",
            "Shapes -> X_val:(69, 36) | X_test:(70, 36) | y_val:(69,) | y_test:(70,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === p22_C: Utils (metrics + threshold search) ===\n",
        "def eval_at_threshold(y_true, y_score, thr, cohort=None, tag=\"ALL\"):\n",
        "    y_pred = (y_score >= thr).astype(int)\n",
        "    out = dict(\n",
        "        AUC   = float(roc_auc_score(y_true, y_score)) if len(np.unique(y_true))>1 else np.nan,\n",
        "        PRAUC = float(average_precision_score(y_true, y_score)),\n",
        "        Acc   = float(accuracy_score(y_true, y_pred)),\n",
        "        P     = float(precision_score(y_true, y_pred, zero_division=0)),\n",
        "        R     = float(recall_score(y_true, y_pred, zero_division=0)),\n",
        "        F1    = float(f1_score(y_true, y_pred, zero_division=0)),\n",
        "        Brier = float(brier_score_loss(y_true, y_score)),\n",
        "        thr   = float(thr),\n",
        "        n     = int(len(y_true)),\n",
        "        cohort= tag if cohort is None else str(cohort),\n",
        "    )\n",
        "    return out\n",
        "\n",
        "def best_threshold_by_f1(y_true, y_score):\n",
        "    # Barrido fino de thr (0..1)\n",
        "    thrs = np.linspace(0.05, 0.95, 19)\n",
        "    f1s  = []\n",
        "    for t in thrs:\n",
        "        pred = (y_score >= t).astype(int)\n",
        "        f1s.append(f1_score(y_true, pred, zero_division=0))\n",
        "    best_idx = int(np.argmax(f1s))\n",
        "    return float(thrs[best_idx])\n",
        "\n",
        "def report_split(y_true, y_score, thr, cohorts=None, title=\"GLOBAL_F1\"):\n",
        "    res = {\"ALL\": eval_at_threshold(y_true, y_score, thr, tag=\"ALL\")}\n",
        "    if cohorts is not None:\n",
        "        for tag in sorted(np.unique(cohorts)):\n",
        "            mask = (cohorts == tag)\n",
        "            res[tag] = eval_at_threshold(y_true[mask], y_score[mask], thr, tag=tag)\n",
        "    print(f\"[{title}][VAL] {res['ALL']}\")\n",
        "    return res\n"
      ],
      "metadata": {
        "id": "TI4-IJEAxlVJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# D) Calibración robusta (Platt / Isotónica) para base learners (LR y HGB)\n",
        "# - Sin fugas (KFold en VAL)\n",
        "# - Tolerante a NaNs (imputación donde hace falta)\n",
        "# - Compatible con scikit-learn >=1.4 (usa 'estimator=' en CalibratedClassifierCV)\n",
        "# - Guarda outputs y muestra métricas por cohorte y globales\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa: F401\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, precision_score, recall_score, f1_score, brier_score_loss\n",
        "\n",
        "# === 1) Detecta VAL/TEST y columnas ===\n",
        "assert 'val' in globals() and 'test' in globals(), \"Se esperaba DataFrames VAL/TEST desde celdas anteriores.\"\n",
        "VAL = val.copy()\n",
        "TEST = test.copy()\n",
        "\n",
        "# Asegura columnas obligatorias\n",
        "for df, name in [(VAL,'VAL'), (TEST,'TEST')]:\n",
        "    for col in ['patient_id','y_true']:\n",
        "        assert col in df.columns, f\"{name} debe tener columna '{col}'\"\n",
        "\n",
        "# Si no viene 'cohort', la creamos en base al prefijo del patient_id\n",
        "if 'cohort' not in VAL.columns:\n",
        "    VAL['cohort'] = np.where(VAL['patient_id'].astype(str).str.startswith('OAS2'), 'OAS2', 'OAS1')\n",
        "if 'cohort' not in TEST.columns:\n",
        "    TEST['cohort'] = np.where(TEST['patient_id'].astype(str).str.startswith('OAS2'), 'OAS2', 'OAS1')\n",
        "\n",
        "# Selección de columnas de features\n",
        "if 'cols_keep' in globals() and isinstance(keep_cols, (list, tuple)) and len(keep_cols) > 0:\n",
        "    feat_cols = [c for c in keep_cols if c not in ['patient_id','y_true','cohort']]\n",
        "else:\n",
        "    # autoselección: numéricas no meta/ID\n",
        "    black = {'patient_id','y_true','cohort'}\n",
        "    feat_cols = [c for c in VAL.columns if c not in black and pd.api.types.is_numeric_dtype(VAL[c])]\n",
        "    # filtra columnas con demasiados NaNs (como en tus pipelines previos)\n",
        "    nan_ratio = VAL[feat_cols].isna().mean().sort_values(ascending=False)\n",
        "    feat_cols = [c for c in feat_cols if nan_ratio.get(c,0.0) <= 0.40]\n",
        "\n",
        "assert len(feat_cols) > 0, \"No hay columnas de features tras el filtrado.\"\n",
        "\n",
        "X_val  = VAL[feat_cols].copy()\n",
        "y_val  = VAL['y_true'].astype(int).values\n",
        "pid_val = VAL['patient_id'].astype(str).values\n",
        "coh_val = VAL['cohort'].astype(str).values\n",
        "\n",
        "X_test = TEST[feat_cols].copy()\n",
        "y_test = TEST['y_true'].astype(int).values\n",
        "pid_test = TEST['patient_id'].astype(str).values\n",
        "coh_test = TEST['cohort'].astype(str).values\n",
        "\n",
        "print(f\"VAL: {X_val.shape} | TEST: {X_test.shape} | feats={len(feat_cols)}\")\n",
        "\n",
        "# === 2) Helpers ===\n",
        "def metrics(y_true, y_score, thr=0.5):\n",
        "    y_pred = (y_score >= thr).astype(int)\n",
        "    out = {\n",
        "        'AUC': float(roc_auc_score(y_true, y_score)) if len(np.unique(y_true))==2 else np.nan,\n",
        "        'PRAUC': float(average_precision_score(y_true, y_score)),\n",
        "        'Acc': float(accuracy_score(y_true, y_pred)),\n",
        "        'P': float(precision_score(y_true, y_pred, zero_division=0)),\n",
        "        'R': float(recall_score(y_true, y_pred, zero_division=0)),\n",
        "        'F1': float(f1_score(y_true, y_pred, zero_division=0)),\n",
        "        'Brier': float(brier_score_loss(y_true, y_score)),\n",
        "        'thr': float(thr),\n",
        "        'n': int(len(y_true))\n",
        "    }\n",
        "    return out\n",
        "\n",
        "def best_f1_threshold(y_true, y_score):\n",
        "    # barrido fino; si quieres más fino, baja el paso\n",
        "    grid = np.linspace(0.05, 0.95, 19)\n",
        "    f1s = [f1_score(y_true, (y_score>=t).astype(int), zero_division=0) for t in grid]\n",
        "    return float(grid[int(np.argmax(f1s))])\n",
        "\n",
        "def oof_and_calibrated(X, y, Xte, base, method='sigmoid', n_splits=5, random_state=42):\n",
        "    \"\"\"\n",
        "    - Entrena OOF del modelo base (sin calibrar).\n",
        "    - Calibra con CalibratedClassifierCV(estimator=base, cv='prefit') por fold.\n",
        "    - Devuelve: oof_base, oof_cal, test_cal (promedio de calibradores).\n",
        "    \"\"\"\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    oof_base = np.zeros(len(X))\n",
        "    oof_cal  = np.zeros(len(X))\n",
        "    test_cals = []\n",
        "\n",
        "    for tr, va in skf.split(X, y):\n",
        "        Xtr, Xva = X.iloc[tr], X.iloc[va]\n",
        "        ytr, yva = y[tr], y[va]\n",
        "\n",
        "        # clona base limpio por fold\n",
        "        # (para pipelines, los re-creamos por tipo)\n",
        "        if isinstance(base, str) and base=='LR':\n",
        "            model = Pipeline([\n",
        "                ('imp', SimpleImputer(strategy='median')),\n",
        "                ('scaler', StandardScaler(with_mean=False)),  # robusto con columnas con var ~0\n",
        "                ('lr', LogisticRegression(max_iter=2000, C=1.0, class_weight=None))\n",
        "            ])\n",
        "        elif isinstance(base, str) and base=='HGB':\n",
        "            model = HistGradientBoostingClassifier(\n",
        "                max_depth=None, learning_rate=0.06, max_iter=200,\n",
        "                l2_regularization=0.0, random_state=42\n",
        "            )\n",
        "        else:\n",
        "            # si pasaron un estimador ya construido\n",
        "            from sklearn.base import clone\n",
        "            model = clone(base)\n",
        "\n",
        "        # fit base\n",
        "        model.fit(Xtr, ytr)\n",
        "\n",
        "        # OOF base\n",
        "        oof_base[va] = model.predict_proba(Xva)[:,1]\n",
        "\n",
        "        # calibración sobre modelo prefit\n",
        "        cal = CalibratedClassifierCV(estimator=model, method=method, cv='prefit')\n",
        "        cal.fit(Xva, yva)  # se ajusta la función de calibración con el fold de validación\n",
        "\n",
        "        # OOF calibrado\n",
        "        oof_cal[va] = cal.predict_proba(Xva)[:,1]\n",
        "\n",
        "        # pred test calibrado por fold\n",
        "        test_cals.append(cal.predict_proba(Xte)[:,1])\n",
        "\n",
        "    test_cal = np.mean(np.stack(test_cals, axis=1), axis=1)\n",
        "    return oof_base, oof_cal, test_cal\n",
        "\n",
        "# === 3) Ejecuta calibraciones (LR e HGB; Platt e Isotónica) ===\n",
        "print(\"Calibrando LR (Platt/Isotonic)...\")\n",
        "lr_base = 'LR'  # se construye dentro por fold con imputación\n",
        "lr_oof_base_sig, lr_oof_cal_sig, lr_test_cal_sig = oof_and_calibrated(X_val, y_val, X_test, base=lr_base, method='sigmoid')\n",
        "lr_oof_base_iso, lr_oof_cal_iso, lr_test_cal_iso = oof_and_calibrated(X_val, y_val, X_test, base=lr_base, method='isotonic')\n",
        "\n",
        "print(\"Calibrando HGB (Platt/Isotonic)...\")\n",
        "hgb_base = 'HGB'  # HGB tolera NaNs\n",
        "hgb_oof_base_sig, hgb_oof_cal_sig, hgb_test_cal_sig = oof_and_calibrated(X_val, y_val, X_test, base=hgb_base, method='sigmoid')\n",
        "hgb_oof_base_iso, hgb_oof_cal_iso, hgb_test_cal_iso = oof_and_calibrated(X_val, y_val, X_test, base=hgb_base, method='isotonic')\n",
        "\n",
        "# === 4) Métricas (umbral F1 óptimo en VAL por cada setting) ===\n",
        "def report_block(name, oof_cal, test_cal):\n",
        "    thr = best_f1_threshold(y_val, oof_cal)\n",
        "    print(f\"\\n== {name} | thr@F1(VAL)={thr:.2f} ==\")\n",
        "    res_val  = metrics(y_val,  oof_cal,  thr)\n",
        "    res_test = metrics(y_test, test_cal, thr)\n",
        "    print(\"[VAL]\",  res_val)\n",
        "    print(\"[TEST]\", res_test)\n",
        "    return name, thr, res_val, res_test\n",
        "\n",
        "results = []\n",
        "results.append(report_block(\"LR|Platt\",     lr_oof_cal_sig,  lr_test_cal_sig))\n",
        "results.append(report_block(\"LR|Isotonic\",  lr_oof_cal_iso,  lr_test_cal_iso))\n",
        "results.append(report_block(\"HGB|Platt\",    hgb_oof_cal_sig, hgb_test_cal_sig))\n",
        "results.append(report_block(\"HGB|Isotonic\", hgb_oof_cal_iso, hgb_test_cal_iso))\n",
        "\n",
        "# === 5) Guardado en Drive ===\n",
        "OUT = Path(\"/content/drive/MyDrive/CognitivaAI/p22_meta_ablation\")\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# VAL / TEST por método\n",
        "pd.DataFrame({\n",
        "    \"patient_id\": pid_val, \"y_true\": y_val,\n",
        "    \"LR_platt\": lr_oof_cal_sig, \"LR_iso\": lr_oof_cal_iso,\n",
        "    \"HGB_platt\": hgb_oof_cal_sig, \"HGB_iso\": hgb_oof_cal_iso\n",
        "}).to_csv(OUT / \"p22_val_calibrations.csv\", index=False)\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"patient_id\": pid_test, \"y_true\": y_test,\n",
        "    \"LR_platt\": lr_test_cal_sig, \"LR_iso\": lr_test_cal_iso,\n",
        "    \"HGB_platt\": hgb_test_cal_sig, \"HGB_iso\": hgb_test_cal_iso\n",
        "}).to_csv(OUT / \"p22_test_calibrations.csv\", index=False)\n",
        "\n",
        "# Resumen JSON\n",
        "import json\n",
        "json.dump(\n",
        "    [\n",
        "        {\"name\": n, \"thr\": t, \"VAL\": v, \"TEST\": te}\n",
        "        for (n, t, v, te) in results\n",
        "    ],\n",
        "    open(OUT / \"p22_calibration_summary.json\", \"w\"),\n",
        "    indent=2\n",
        ")\n",
        "\n",
        "print(\"\\n💾 Guardados en:\", str(OUT))\n",
        "print(\"• p22_val_calibrations.csv\")\n",
        "print(\"• p22_test_calibrations.csv\")\n",
        "print(\"• p22_calibration_summary.json\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPBT_VIvxqOG",
        "outputId": "d63655b5-b3d3-4b14-d858-e7bbf6f79deb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL: (69, 36) | TEST: (70, 36) | feats=36\n",
            "Calibrando LR (Platt/Isotonic)...\n",
            "Calibrando HGB (Platt/Isotonic)...\n",
            "\n",
            "== LR|Platt | thr@F1(VAL)=0.35 ==\n",
            "[VAL] {'AUC': 0.7321731748726655, 'PRAUC': 0.7145407614908541, 'Acc': 0.6086956521739131, 'P': 0.5370370370370371, 'R': 0.9354838709677419, 'F1': 0.6823529411764706, 'Brier': 0.20752393411010403, 'thr': 0.35, 'n': 69}\n",
            "[TEST] {'AUC': 0.6681743421052632, 'PRAUC': 0.6464341487486046, 'Acc': 0.6142857142857143, 'P': 0.5454545454545454, 'R': 0.9375, 'F1': 0.6896551724137931, 'Brier': 0.21893862322613, 'thr': 0.35, 'n': 70}\n",
            "\n",
            "== LR|Isotonic | thr@F1(VAL)=0.35 ==\n",
            "[VAL] {'AUC': 0.8590831918505942, 'PRAUC': 0.8251318361887122, 'Acc': 0.7536231884057971, 'P': 0.6842105263157895, 'R': 0.8387096774193549, 'F1': 0.7536231884057971, 'Brier': 0.14533011272141705, 'thr': 0.35, 'n': 69}\n",
            "[TEST] {'AUC': 0.6669407894736842, 'PRAUC': 0.6178296133776486, 'Acc': 0.6, 'P': 0.5416666666666666, 'R': 0.8125, 'F1': 0.65, 'Brier': 0.23112782563176681, 'thr': 0.35, 'n': 70}\n",
            "\n",
            "== HGB|Platt | thr@F1(VAL)=0.30 ==\n",
            "[VAL] {'AUC': 0.8174872665534805, 'PRAUC': 0.7816516728423423, 'Acc': 0.7246376811594203, 'P': 0.6304347826086957, 'R': 0.9354838709677419, 'F1': 0.7532467532467533, 'Brier': 0.17398895968378897, 'thr': 0.3, 'n': 69}\n",
            "[TEST] {'AUC': 0.7023026315789473, 'PRAUC': 0.6286053532146552, 'Acc': 0.5857142857142857, 'P': 0.5319148936170213, 'R': 0.78125, 'F1': 0.6329113924050633, 'Brier': 0.2222752644690013, 'thr': 0.3, 'n': 70}\n",
            "\n",
            "== HGB|Isotonic | thr@F1(VAL)=0.30 ==\n",
            "[VAL] {'AUC': 0.8866723259762309, 'PRAUC': 0.8501090268794611, 'Acc': 0.7536231884057971, 'P': 0.6590909090909091, 'R': 0.9354838709677419, 'F1': 0.7733333333333333, 'Brier': 0.1326086956521739, 'thr': 0.3, 'n': 69}\n",
            "[TEST] {'AUC': 0.6657072368421053, 'PRAUC': 0.6052309600473085, 'Acc': 0.6, 'P': 0.5434782608695652, 'R': 0.78125, 'F1': 0.6410256410256411, 'Brier': 0.23908740280228402, 'thr': 0.3, 'n': 70}\n",
            "\n",
            "💾 Guardados en: /content/drive/MyDrive/CognitivaAI/p22_meta_ablation\n",
            "• p22_val_calibrations.csv\n",
            "• p22_test_calibrations.csv\n",
            "• p22_calibration_summary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# E) Consolidación y guardado de calibraciones (compatible con variables de la Celda D actual)\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, json\n",
        "\n",
        "# --- 1) Verificaciones mínimas\n",
        "required_vars = [\n",
        "    'pid_val','y_val','pid_test','y_test',\n",
        "    'lr_oof_cal_sig','lr_test_cal_sig','lr_oof_cal_iso','lr_test_cal_iso',\n",
        "    'hgb_oof_cal_sig','hgb_test_cal_sig','hgb_oof_cal_iso','hgb_test_cal_iso'\n",
        "]\n",
        "missing = [v for v in required_vars if v not in globals()]\n",
        "assert not missing, f\"Faltan variables de la Celda D: {missing}\"\n",
        "\n",
        "# --- 2) Helpers de métrica (idénticos a D, por consistencia)\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, precision_score, recall_score, f1_score, brier_score_loss\n",
        "\n",
        "def metrics(y_true, y_score, thr=0.5):\n",
        "    y_pred = (y_score >= thr).astype(int)\n",
        "    return {\n",
        "        'AUC': float(roc_auc_score(y_true, y_score)) if len(np.unique(y_true))==2 else np.nan,\n",
        "        'PRAUC': float(average_precision_score(y_true, y_score)),\n",
        "        'Acc': float(accuracy_score(y_true, y_pred)),\n",
        "        'P': float(precision_score(y_true, y_pred, zero_division=0)),\n",
        "        'R': float(recall_score(y_true, y_pred, zero_division=0)),\n",
        "        'F1': float(f1_score(y_true, y_pred, zero_division=0)),\n",
        "        'Brier': float(brier_score_loss(y_true, y_score)),\n",
        "        'thr': float(thr),\n",
        "        'n': int(len(y_true))\n",
        "    }\n",
        "\n",
        "def best_f1_threshold(y_true, y_score):\n",
        "    grid = np.linspace(0.05, 0.95, 19)\n",
        "    f1s = [f1_score(y_true, (y_score>=t).astype(int), zero_division=0) for t in grid]\n",
        "    return float(grid[int(np.argmax(f1s))])\n",
        "\n",
        "# --- 3) Construye DataFrames VAL/TEST con todas las columnas calibradas\n",
        "VAL_df = pd.DataFrame({\n",
        "    \"patient_id\": pid_val, \"y_true\": y_val,\n",
        "    \"LR_platt\":  lr_oof_cal_sig, \"LR_iso\":  lr_oof_cal_iso,\n",
        "    \"HGB_platt\": hgb_oof_cal_sig, \"HGB_iso\": hgb_oof_cal_iso\n",
        "})\n",
        "TEST_df = pd.DataFrame({\n",
        "    \"patient_id\": pid_test, \"y_true\": y_test,\n",
        "    \"LR_platt\":  lr_test_cal_sig, \"LR_iso\":  lr_test_cal_iso,\n",
        "    \"HGB_platt\": hgb_test_cal_sig, \"HGB_iso\": hgb_test_cal_iso\n",
        "})\n",
        "\n",
        "# --- 4) Métricas resumen usando umbral óptimo por configuración (en VAL) y aplicándolo en TEST\n",
        "def eval_pair(name, yv, sv, yt, st):\n",
        "    thr = best_f1_threshold(yv, sv)\n",
        "    return {\n",
        "        \"name\": name,\n",
        "        \"thr\": thr,\n",
        "        \"VAL\": metrics(yv, sv, thr),\n",
        "        \"TEST\": metrics(yt, st, thr)\n",
        "    }\n",
        "\n",
        "summary = [\n",
        "    eval_pair(\"LR|Platt\",     y_val, VAL_df[\"LR_platt\"].values,  y_test, TEST_df[\"LR_platt\"].values),\n",
        "    eval_pair(\"LR|Isotonic\",  y_val, VAL_df[\"LR_iso\"].values,    y_test, TEST_df[\"LR_iso\"].values),\n",
        "    eval_pair(\"HGB|Platt\",    y_val, VAL_df[\"HGB_platt\"].values, y_test, TEST_df[\"HGB_platt\"].values),\n",
        "    eval_pair(\"HGB|Isotonic\", y_val, VAL_df[\"HGB_iso\"].values,   y_test, TEST_df[\"HGB_iso\"].values),\n",
        "]\n",
        "\n",
        "# --- 5) Guardado\n",
        "OUT = Path(\"/content/drive/MyDrive/CognitivaAI/p22_meta_ablation\")\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "VAL_df.to_csv(OUT / \"p22_val_calibrations.csv\", index=False)\n",
        "TEST_df.to_csv(OUT / \"p22_test_calibrations.csv\", index=False)\n",
        "with open(OUT / \"p22_calibration_summary.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"💾 Guardados en:\", OUT)\n",
        "print(\" - p22_val_calibrations.csv\")\n",
        "print(\" - p22_test_calibrations.csv\")\n",
        "print(\" - p22_calibration_summary.json\")\n",
        "\n",
        "print(\"\\nResumen rápido:\")\n",
        "for row in summary:\n",
        "    print(f\"{row['name']} | thr={row['thr']:.2f} | \"\n",
        "          f\"VAL F1={row['VAL']['F1']:.3f} AUC={row['VAL']['AUC']:.3f} | \"\n",
        "          f\"TEST F1={row['TEST']['F1']:.3f} AUC={row['TEST']['AUC']:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV1EHHDJQ9EX",
        "outputId": "dcc3d48f-a648-456b-8d78-43138a1c604b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Guardados en: /content/drive/MyDrive/CognitivaAI/p22_meta_ablation\n",
            " - p22_val_calibrations.csv\n",
            " - p22_test_calibrations.csv\n",
            " - p22_calibration_summary.json\n",
            "\n",
            "Resumen rápido:\n",
            "LR|Platt | thr=0.35 | VAL F1=0.682 AUC=0.732 | TEST F1=0.690 AUC=0.668\n",
            "LR|Isotonic | thr=0.35 | VAL F1=0.754 AUC=0.859 | TEST F1=0.650 AUC=0.667\n",
            "HGB|Platt | thr=0.30 | VAL F1=0.753 AUC=0.817 | TEST F1=0.633 AUC=0.702\n",
            "HGB|Isotonic | thr=0.30 | VAL F1=0.773 AUC=0.887 | TEST F1=0.641 AUC=0.666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# F) Evaluación y reporte final (usa variables en memoria o carga desde CSV)\n",
        "# - No depende de nombres previos; es robusto\n",
        "# - Calcula umbral F1 en VAL por método y evalúa en TEST\n",
        "# - Opcional: añade un blend simple LR_iso/HGB_iso\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, precision_score, recall_score, f1_score, brier_score_loss\n",
        "\n",
        "# ==== 1) Helpers ====\n",
        "def metrics(y_true, y_score, thr=0.5):\n",
        "    y_pred = (y_score >= thr).astype(int)\n",
        "    return {\n",
        "        \"AUC\": float(roc_auc_score(y_true, y_score)) if len(np.unique(y_true))==2 else np.nan,\n",
        "        \"PRAUC\": float(average_precision_score(y_true, y_score)),\n",
        "        \"Acc\": float(accuracy_score(y_true, y_pred)),\n",
        "        \"P\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
        "        \"R\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
        "        \"F1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
        "        \"Brier\": float(brier_score_loss(y_true, y_score)),\n",
        "        \"thr\": float(thr),\n",
        "        \"n\": int(len(y_true))\n",
        "    }\n",
        "\n",
        "def best_f1_threshold(y_true, y_score):\n",
        "    grid = np.linspace(0.05, 0.95, 19)\n",
        "    f1s = [f1_score(y_true, (y_score>=t).astype(int), zero_division=0) for t in grid]\n",
        "    return float(grid[int(np.argmax(f1s))])\n",
        "\n",
        "# ==== 2) Detectar datos VAL/TEST y/o cargar CSV de D ====\n",
        "OUT = Path(\"/content/drive/MyDrive/CognitivaAI/p22_meta_ablation\")\n",
        "val_csv  = OUT/\"p22_val_calibrations.csv\"\n",
        "test_csv = OUT/\"p22_test_calibrations.csv\"\n",
        "\n",
        "# y_true / patient_id\n",
        "if 'VAL' in globals() and 'TEST' in globals():\n",
        "    y_val   = VAL['y_true'].astype(int).values\n",
        "    y_test  = TEST['y_true'].astype(int).values\n",
        "    pid_val = VAL['patient_id'].astype(str).values\n",
        "    pid_test= TEST['patient_id'].astype(str).values\n",
        "    coh_val = (VAL['cohort'] if 'cohort' in VAL.columns else VAL['patient_id'].astype(str).str.startswith('OAS2').map({True:'OAS2',False:'OAS1'})).values\n",
        "    coh_test= (TEST['cohort'] if 'cohort' in TEST.columns else TEST['patient_id'].astype(str).str.startswith('OAS2').map({True:'OAS2',False:'OAS1'})).values\n",
        "else:\n",
        "    # si no están en memoria, los recuperamos de los CSV generados en D\n",
        "    assert val_csv.exists() and test_csv.exists(), \"No encuentro VAL/TEST ni los CSV de calibración. Ejecuta la celda D primero.\"\n",
        "    _v = pd.read_csv(val_csv)\n",
        "    _t = pd.read_csv(test_csv)\n",
        "    y_val   = _v['y_true'].astype(int).values\n",
        "    y_test  = _t['y_true'].astype(int).values\n",
        "    pid_val = _v['patient_id'].astype(str).values\n",
        "    pid_test= _t['patient_id'].astype(str).values\n",
        "    # inferimos cohort a partir del prefijo del ID\n",
        "    coh_val = np.where(_v['patient_id'].astype(str).str.startswith('OAS2'), 'OAS2', 'OAS1')\n",
        "    coh_test= np.where(_t['patient_id'].astype(str).str.startswith('OAS2'), 'OAS2', 'OAS1')\n",
        "\n",
        "# ==== 3) Recolectar scores calibrados (desde memoria o CSV) ====\n",
        "def get_vec(varname, fallback_df=None, fallback_col=None, n=None):\n",
        "    if varname in globals():\n",
        "        return globals()[varname]\n",
        "    if fallback_df is not None and fallback_col is not None:\n",
        "        return fallback_df[fallback_col].values\n",
        "    raise KeyError(f\"No se encontró '{varname}' ni columna '{fallback_col}' para cargar.\")\n",
        "\n",
        "# Cargar CSV si hace falta\n",
        "_vdf = pd.read_csv(val_csv)  if val_csv.exists()  else None\n",
        "_tdf = pd.read_csv(test_csv) if test_csv.exists() else None\n",
        "\n",
        "scores = {}\n",
        "\n",
        "# LR Platt / Iso\n",
        "scores['LR_platt'] = (\n",
        "    get_vec('lr_oof_cal_sig', _vdf, 'LR_platt', len(y_val)),\n",
        "    get_vec('lr_test_cal_sig', _tdf, 'LR_platt', len(y_test))\n",
        ")\n",
        "scores['LR_iso'] = (\n",
        "    get_vec('lr_oof_cal_iso', _vdf, 'LR_iso', len(y_val)),\n",
        "    get_vec('lr_test_cal_iso', _tdf, 'LR_iso', len(y_test))\n",
        ")\n",
        "\n",
        "# HGB Platt / Iso\n",
        "scores['HGB_platt'] = (\n",
        "    get_vec('hgb_oof_cal_sig', _vdf, 'HGB_platt', len(y_val)),\n",
        "    get_vec('hgb_test_cal_sig', _tdf, 'HGB_platt', len(y_test))\n",
        ")\n",
        "scores['HGB_iso'] = (\n",
        "    get_vec('hgb_oof_cal_iso', _vdf, 'HGB_iso', len(y_val)),\n",
        "    get_vec('hgb_test_cal_iso', _tdf, 'HGB_iso', len(y_test))\n",
        ")\n",
        "\n",
        "# Blend simple (por si queremos comparativa adicional)\n",
        "scores['BLEND_iso'] = (\n",
        "    0.5 * scores['LR_iso'][0] + 0.5 * scores['HGB_iso'][0],\n",
        "    0.5 * scores['LR_iso'][1] + 0.5 * scores['HGB_iso'][1]\n",
        ")\n",
        "\n",
        "# ==== 4) Métricas global y por cohortes ====\n",
        "def eval_block(name, val_scores, test_scores):\n",
        "    thr = best_f1_threshold(y_val, val_scores)\n",
        "    res_val  = metrics(y_val,  val_scores,  thr)\n",
        "    res_test = metrics(y_test, test_scores, thr)\n",
        "\n",
        "    # por cohortes\n",
        "    out = {\"name\": name, \"thr\": thr, \"VAL\": res_val, \"TEST\": res_test, \"COHORTS\": {}}\n",
        "    for cohort, mask in {\"OAS1\": coh_val==\"OAS1\", \"OAS2\": coh_val==\"OAS2\"}.items():\n",
        "        if mask.sum() > 0:\n",
        "            out[\"COHORTS\"][f\"VAL_{cohort}\"] = metrics(y_val[mask],  val_scores[mask], thr)\n",
        "    for cohort, mask in {\"OAS1\": coh_test==\"OAS1\", \"OAS2\": coh_test==\"OAS2\"}.items():\n",
        "        if mask.sum() > 0:\n",
        "            out[\"COHORTS\"][f\"TEST_{cohort}\"] = metrics(y_test[mask], test_scores[mask], thr)\n",
        "    return out\n",
        "\n",
        "summary = []\n",
        "for name, (v,t) in scores.items():\n",
        "    print(f\"\\n== {name} ==\")\n",
        "    block = eval_block(name, v, t)\n",
        "    print(\"[VAL]\",  block[\"VAL\"])\n",
        "    print(\"[TEST]\", block[\"TEST\"])\n",
        "    summary.append(block)\n",
        "\n",
        "# ==== 5) Guardados ====\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "pd.DataFrame({\n",
        "    \"patient_id\": pid_val, \"y_true\": y_val,\n",
        "    **{k: scores[k][0] for k in scores}\n",
        "}).to_csv(OUT/\"p22_val_all_methods.csv\", index=False)\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"patient_id\": pid_test, \"y_true\": y_test,\n",
        "    **{k: scores[k][1] for k in scores}\n",
        "}).to_csv(OUT/\"p22_test_all_methods.csv\", index=False)\n",
        "\n",
        "import json\n",
        "with open(OUT/\"p22_final_summary.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"\\n💾 Guardado en:\", OUT)\n",
        "print(\" - p22_val_all_methods.csv\")\n",
        "print(\" - p22_test_all_methods.csv\")\n",
        "print(\" - p22_final_summary.json\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qCq3QDxUjeH",
        "outputId": "e4e9f5f6-4d9f-4f38-f996-6864a4b11269"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== LR_platt ==\n",
            "[VAL] {'AUC': 0.7321731748726655, 'PRAUC': 0.7145407614908541, 'Acc': 0.6086956521739131, 'P': 0.5370370370370371, 'R': 0.9354838709677419, 'F1': 0.6823529411764706, 'Brier': 0.20752393411010403, 'thr': 0.35, 'n': 69}\n",
            "[TEST] {'AUC': 0.6681743421052632, 'PRAUC': 0.6464341487486046, 'Acc': 0.6142857142857143, 'P': 0.5454545454545454, 'R': 0.9375, 'F1': 0.6896551724137931, 'Brier': 0.21893862322613, 'thr': 0.35, 'n': 70}\n",
            "\n",
            "== LR_iso ==\n",
            "[VAL] {'AUC': 0.8590831918505942, 'PRAUC': 0.8251318361887122, 'Acc': 0.7536231884057971, 'P': 0.6842105263157895, 'R': 0.8387096774193549, 'F1': 0.7536231884057971, 'Brier': 0.14533011272141705, 'thr': 0.35, 'n': 69}\n",
            "[TEST] {'AUC': 0.6669407894736842, 'PRAUC': 0.6178296133776486, 'Acc': 0.6, 'P': 0.5416666666666666, 'R': 0.8125, 'F1': 0.65, 'Brier': 0.23112782563176681, 'thr': 0.35, 'n': 70}\n",
            "\n",
            "== HGB_platt ==\n",
            "[VAL] {'AUC': 0.8174872665534805, 'PRAUC': 0.7816516728423423, 'Acc': 0.7246376811594203, 'P': 0.6304347826086957, 'R': 0.9354838709677419, 'F1': 0.7532467532467533, 'Brier': 0.17398895968378897, 'thr': 0.3, 'n': 69}\n",
            "[TEST] {'AUC': 0.7023026315789473, 'PRAUC': 0.6286053532146552, 'Acc': 0.5857142857142857, 'P': 0.5319148936170213, 'R': 0.78125, 'F1': 0.6329113924050633, 'Brier': 0.2222752644690013, 'thr': 0.3, 'n': 70}\n",
            "\n",
            "== HGB_iso ==\n",
            "[VAL] {'AUC': 0.8866723259762309, 'PRAUC': 0.8501090268794611, 'Acc': 0.7536231884057971, 'P': 0.6590909090909091, 'R': 0.9354838709677419, 'F1': 0.7733333333333333, 'Brier': 0.1326086956521739, 'thr': 0.3, 'n': 69}\n",
            "[TEST] {'AUC': 0.6657072368421053, 'PRAUC': 0.6052309600473085, 'Acc': 0.6, 'P': 0.5434782608695652, 'R': 0.78125, 'F1': 0.6410256410256411, 'Brier': 0.23908740280228402, 'thr': 0.3, 'n': 70}\n",
            "\n",
            "== BLEND_iso ==\n",
            "[VAL] {'AUC': 0.8994057724957555, 'PRAUC': 0.8810707133803413, 'Acc': 0.782608695652174, 'P': 0.7, 'R': 0.9032258064516129, 'F1': 0.7887323943661971, 'Brier': 0.12994162640901774, 'thr': 0.35, 'n': 69}\n",
            "[TEST] {'AUC': 0.6817434210526316, 'PRAUC': 0.6341159486368488, 'Acc': 0.5857142857142857, 'P': 0.5333333333333333, 'R': 0.75, 'F1': 0.6233766233766234, 'Brier': 0.22922998879386713, 'thr': 0.35, 'n': 70}\n",
            "\n",
            "💾 Guardado en: /content/drive/MyDrive/CognitivaAI/p22_meta_ablation\n",
            " - p22_val_all_methods.csv\n",
            " - p22_test_all_methods.csv\n",
            " - p22_final_summary.json\n"
          ]
        }
      ]
    }
  ]
}