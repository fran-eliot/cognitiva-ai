{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "042af11c026944798753d51c2a2ebc24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15d13208898e40f390c25b1359b1b4c9",
              "IPY_MODEL_b590d03e7eaa4d609bd4b207d25a3598",
              "IPY_MODEL_c5a5349006ca40178b9be075aa884457"
            ],
            "layout": "IPY_MODEL_026031811f914c2ab79894ef45e786e3"
          }
        },
        "15d13208898e40f390c25b1359b1b4c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d65086aa41b4a58807fbceeedd94943",
            "placeholder": "​",
            "style": "IPY_MODEL_75c36a3c78054ac1a6ac50ffffebf623",
            "value": "model.safetensors: 100%"
          }
        },
        "b590d03e7eaa4d609bd4b207d25a3598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7692e37471c49c5bbea1fbd7365fad2",
            "max": 49335454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f47b1b4a0274f6ebec7c055472b34e3",
            "value": 49335454
          }
        },
        "c5a5349006ca40178b9be075aa884457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d4754483f5a4d6d97102cc3e67315e4",
            "placeholder": "​",
            "style": "IPY_MODEL_ffd4b081456240e1bed56cb4ff2c1e1f",
            "value": " 49.3M/49.3M [00:00&lt;00:00, 165MB/s]"
          }
        },
        "026031811f914c2ab79894ef45e786e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d65086aa41b4a58807fbceeedd94943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75c36a3c78054ac1a6ac50ffffebf623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7692e37471c49c5bbea1fbd7365fad2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f47b1b4a0274f6ebec7c055472b34e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d4754483f5a4d6d97102cc3e67315e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffd4b081456240e1bed56cb4ff2c1e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgl_cS5TQb4p",
        "outputId": "01381dbc-7fcc-4d9c-ea30-19a48fd87087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Celda 0: Dependencias y configuración\n",
        "!pip -q install timm==1.0.9 --no-deps\n",
        "\n",
        "import os, math, time, json, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "\n",
        "import timm\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, accuracy_score, precision_score, recall_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# Rutas\n",
        "DATA_DIR = \"/content/drive/MyDrive/CognitivaAI/oas1_data\"\n",
        "CSV_TRAIN = f\"{DATA_DIR}/oas1_train_colab_mapped.csv\"\n",
        "CSV_VAL   = f\"{DATA_DIR}/oas1_val_colab_mapped.csv\"\n",
        "CSV_TEST  = f\"{DATA_DIR}/oas1_test_colab_mapped.csv\"\n",
        "OUT_DIR   = \"/content/drive/MyDrive/CognitivaAI/ft_effb3_colab\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Hiperparámetros base\n",
        "IMG_SIZE   = 300            # recomendado para EfficientNet-B3\n",
        "BATCH_SIZE = 32             # T4 friendly (ajusta a 24-40 si falta memoria)\n",
        "NUM_WORKERS= 2\n",
        "EPOCHS     = 12             # entrenamiento corto con early stopping\n",
        "BASE_LR    = 3e-4\n",
        "WD         = 1e-4\n",
        "PATIENCE   = 4              # early stopping\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1: Dataset MRI slices y DataLoaders\n",
        "\n",
        "class MRISliceDataset(Dataset):\n",
        "    def __init__(self, csv_path, transform=None):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        assert {'png_path','target','patient_id','scan_id'}.issubset(df.columns), \"CSV con columnas requeridas\"\n",
        "        self.paths = df['png_path'].astype(str).tolist()\n",
        "        self.labels = df['target'].astype(int).to_numpy()\n",
        "        self.pids = df['patient_id'].astype(str).to_numpy()\n",
        "        self.sids = df['scan_id'].astype(str).to_numpy()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.paths[idx]\n",
        "        img = Image.open(path).convert('L')  # imágenes axiales en escala de grises\n",
        "        img = img.resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR)\n",
        "        img = np.array(img, dtype=np.float32) / 255.0\n",
        "        img = np.stack([img, img, img], axis=0)  # 1->3 canales\n",
        "        if self.transform:\n",
        "            # transform de torchvision espera PIL o tensor HWC; convertimos\n",
        "            img_t = transforms.functional.to_pil_image(img.transpose(1,2,0))\n",
        "            img_t = self.transform(img_t)\n",
        "        else:\n",
        "            img_t = torch.from_numpy(img)\n",
        "        y = self.labels[idx]\n",
        "        return img_t, y, self.pids[idx], self.sids[idx], path\n",
        "\n",
        "# Transforms\n",
        "mean_std = ([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.85,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=7),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(*mean_std),\n",
        "])\n",
        "eval_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(*mean_std),\n",
        "])\n",
        "\n",
        "ds_tr = MRISliceDataset(CSV_TRAIN, transform=train_tfms)\n",
        "ds_va = MRISliceDataset(CSV_VAL,   transform=eval_tfms)\n",
        "ds_te = MRISliceDataset(CSV_TEST,  transform=eval_tfms)\n",
        "\n",
        "# Sampler balanceado por clase (opcional pero útil)\n",
        "class_counts = np.bincount(ds_tr.labels, minlength=2)\n",
        "w_neg, w_pos = 1.0/class_counts[0], 1.0/class_counts[1]\n",
        "sample_weights = np.where(ds_tr.labels==1, w_pos, w_neg)\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, sampler=sampler,  num_workers=NUM_WORKERS, pin_memory=True)\n",
        "dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False,     num_workers=NUM_WORKERS, pin_memory=True)\n",
        "dl_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False,     num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(\"TRAIN slices:\", len(ds_tr), \"| VAL:\", len(ds_va), \"| TEST:\", len(ds_te))\n",
        "print(\"Class counts train:\", class_counts, \"→ pos_weight≈\", round(class_counts[0]/max(1,class_counts[1]),3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5shRNeJxQzhg",
        "outputId": "cd40cd44-3a0e-43ed-f12d-00a3202f9d54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN slices: 2820 | VAL: 940 | TEST: 940\n",
            "Class counts train: [1620 1200] → pos_weight≈ 1.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 2: Modelo EfficientNet-B3 con fine-tuning parcial\n",
        "\n",
        "BACKBONE = \"tf_efficientnet_b3_ns\"  # timm\n",
        "model = timm.create_model(BACKBONE, pretrained=True, num_classes=0, in_chans=3)\n",
        "feat_dim = model.num_features\n",
        "\n",
        "# Head ligera\n",
        "head = nn.Sequential(\n",
        "    nn.Dropout(p=0.3),\n",
        "    nn.Linear(feat_dim, 1)\n",
        ")\n",
        "\n",
        "net = nn.Sequential(model, head).to(DEVICE)\n",
        "\n",
        "# Congelar todo menos el último bloque del backbone + head\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Descongelar el último bloque de EfficientNet-B3\n",
        "# Identificamos módulos finales típicos en timm\n",
        "for name, module in model.named_modules():\n",
        "    last_block = ('blocks.6', 'blocks.7')  # por si la variante incluye más\n",
        "    if any(name.startswith(lb) for lb in last_block):\n",
        "        for p in module.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "# Head entrenable\n",
        "for p in head.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Optimizador\n",
        "trainable_params = [p for p in net.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.AdamW(trainable_params, lr=BASE_LR, weight_decay=WD)\n",
        "# Cosine schedule\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "# Pérdida con pos_weight\n",
        "pos_weight = torch.tensor([class_counts[0]/max(1,class_counts[1])], device=DEVICE, dtype=torch.float32)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "print(\"Trainable params:\", sum(p.numel() for p in net.parameters() if p.requires_grad))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "042af11c026944798753d51c2a2ebc24",
            "15d13208898e40f390c25b1359b1b4c9",
            "b590d03e7eaa4d609bd4b207d25a3598",
            "c5a5349006ca40178b9be075aa884457",
            "026031811f914c2ab79894ef45e786e3",
            "7d65086aa41b4a58807fbceeedd94943",
            "75c36a3c78054ac1a6ac50ffffebf623",
            "b7692e37471c49c5bbea1fbd7365fad2",
            "6f47b1b4a0274f6ebec7c055472b34e3",
            "2d4754483f5a4d6d97102cc3e67315e4",
            "ffd4b081456240e1bed56cb4ff2c1e1f"
          ]
        },
        "id": "BKjuSfKeRFGL",
        "outputId": "62cccef9-a9a2-430b-db01-1ccf71d67942"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b3_ns to current tf_efficientnet_b3.ns_jft_in1k.\n",
            "  model = create_fn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "042af11c026944798753d51c2a2ebc24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 3285755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 3: Entrenamiento\n",
        "\n",
        "def run_epoch(dataloader, train=True):\n",
        "    net.train(train)\n",
        "    total_loss = 0.0\n",
        "    logits_all, y_all = [], []\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "    for xb, yb, *_ in dataloader:\n",
        "        xb, yb = xb.to(DEVICE, non_blocking=True), yb.float().to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(True):\n",
        "            logits = net(xb).squeeze(1)\n",
        "            loss = criterion(logits, yb)\n",
        "        if train:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        logits_all.append(logits.detach().float().cpu().numpy())\n",
        "        y_all.append(yb.detach().float().cpu().numpy())\n",
        "    if not train:\n",
        "        with torch.no_grad():\n",
        "            pass\n",
        "    if train:\n",
        "        scheduler.step()\n",
        "    y_all = np.concatenate(y_all)\n",
        "    logits_all = np.concatenate(logits_all)\n",
        "    probs_all = 1/(1+np.exp(-logits_all))\n",
        "    # Métricas slice-level\n",
        "    auc = roc_auc_score(y_all, probs_all) if len(np.unique(y_all))>1 else np.nan\n",
        "    pr  = average_precision_score(y_all, probs_all) if len(np.unique(y_all))>1 else np.nan\n",
        "    # Thr 0.5\n",
        "    yhat = (probs_all >= 0.5).astype(int)\n",
        "    acc = accuracy_score(y_all, yhat)\n",
        "    pre = precision_score(y_all, yhat, zero_division=0)\n",
        "    rec = recall_score(y_all, yhat, zero_division=0)\n",
        "    brier = np.mean((probs_all - y_all)**2)\n",
        "    return {\n",
        "        \"loss\": total_loss/len(dataloader.dataset),\n",
        "        \"auc\": auc, \"pr\": pr, \"acc\": acc, \"pre\": pre, \"rec\": rec, \"brier\": brier,\n",
        "        \"probs\": probs_all, \"logits\": logits_all, \"y\": y_all\n",
        "    }\n",
        "\n",
        "best_val = -np.inf\n",
        "pat = 0\n",
        "hist = []\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr = run_epoch(dl_tr, train=True)\n",
        "    va = run_epoch(dl_va, train=False)\n",
        "    # criterio: PR-AUC slice en VAL (más sensible a clase positiva)\n",
        "    score = va[\"pr\"] if not np.isnan(va[\"pr\"]) else va[\"auc\"]\n",
        "    hist.append({\"epoch\":epoch, \"train\":tr, \"val\":va})\n",
        "    print(f\"[{epoch:02d}] TR loss={tr['loss']:.4f} | VAL AUC={va['auc']:.3f} PR-AUC={va['pr']:.3f} Brier={va['brier']:.3f}\")\n",
        "    if score > best_val:\n",
        "        best_val = score\n",
        "        pat = 0\n",
        "        torch.save(net.state_dict(), f\"{OUT_DIR}/best_ft_effb3.pth\")\n",
        "    else:\n",
        "        pat += 1\n",
        "        if pat >= PATIENCE:\n",
        "            print(\"→ Early stopping.\")\n",
        "            break\n",
        "\n",
        "# Guardar historial\n",
        "with open(f\"{OUT_DIR}/train_history.json\",\"w\") as f:\n",
        "    json.dump(hist, f)\n",
        "print(\"Entrenamiento finalizado. Mejor PR-AUC VAL:\", round(best_val,4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gdaAPRurRM-d",
        "outputId": "4a47e3f5-30d2-4f41-b98f-709daae5a256"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01] TR loss=0.7466 | VAL AUC=0.674 PR-AUC=0.558 Brier=0.237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02] TR loss=0.6407 | VAL AUC=0.652 PR-AUC=0.563 Brier=0.237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[03] TR loss=0.5669 | VAL AUC=0.666 PR-AUC=0.568 Brier=0.246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04] TR loss=0.5075 | VAL AUC=0.671 PR-AUC=0.583 Brier=0.242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05] TR loss=0.4643 | VAL AUC=0.665 PR-AUC=0.583 Brier=0.249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06] TR loss=0.4370 | VAL AUC=0.655 PR-AUC=0.577 Brier=0.257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07] TR loss=0.3698 | VAL AUC=0.667 PR-AUC=0.573 Brier=0.260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[08] TR loss=0.3477 | VAL AUC=0.666 PR-AUC=0.572 Brier=0.277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-3580555631.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-3580555631.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[09] TR loss=0.3292 | VAL AUC=0.661 PR-AUC=0.568 Brier=0.278\n",
            "→ Early stopping.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object of type float32 is not JSON serializable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3580555631.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Guardar historial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{OUT_DIR}/train_history.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Entrenamiento finalizado. Mejor PR-AUC VAL:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0m_floatstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type float32 is not JSON serializable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 4: Inferencia + pooling paciente (mean y attention)\n",
        "\n",
        "# Cargar mejor modelo\n",
        "net.load_state_dict(torch.load(f\"{OUT_DIR}/best_ft_effb3.pth\", map_location=DEVICE))\n",
        "net.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer(dataloader):\n",
        "    logits_all, y_all, pids_all = [], [], []\n",
        "    for xb, yb, pids, *_ in dataloader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        lg = net(xb).squeeze(1)\n",
        "        logits_all.append(lg.float().cpu().numpy())\n",
        "        y_all.append(yb.numpy())\n",
        "        pids_all += list(pids)\n",
        "    logits = np.concatenate(logits_all)\n",
        "    y = np.concatenate(y_all)\n",
        "    probs = 1/(1+np.exp(-logits))\n",
        "    return logits, probs, y, np.array(pids_all)\n",
        "\n",
        "log_tr, pr_tr, y_tr, pid_tr = infer(dl_tr)\n",
        "log_va, pr_va, y_va, pid_va = infer(dl_va)\n",
        "log_te, pr_te, y_te, pid_te = infer(dl_te)\n",
        "\n",
        "def patient_pool_mean(probs, labels, pids):\n",
        "    df = pd.DataFrame({\"pid\":pids, \"y\":labels, \"p\":probs})\n",
        "    g = df.groupby(\"pid\")\n",
        "    p_pool = g[\"p\"].mean().values\n",
        "    y_pool = g[\"y\"].mean().round().astype(int).values\n",
        "    return y_pool, p_pool, g.size().values\n",
        "\n",
        "def patient_pool_attention(logits, labels, pids, temp=1.0):\n",
        "    # Atención softmax sobre |logits| como importancias (simple, estable)\n",
        "    df = pd.DataFrame({\"pid\":pids, \"y\":labels, \"z\":logits})\n",
        "    outs = []\n",
        "    for pid, grp in df.groupby(\"pid\"):\n",
        "        z = grp[\"z\"].values\n",
        "        # pesos ~ softmax(|z|/T) para resaltar slices informativos\n",
        "        w = np.exp(np.abs(z)/temp); w = w / (w.sum()+1e-8)\n",
        "        p = 1/(1+np.exp(-z))\n",
        "        p_att = (w*p).sum()\n",
        "        y = int(round(grp[\"y\"].mean()))\n",
        "        outs.append((pid, y, p_att, len(grp)))\n",
        "    outs = pd.DataFrame(outs, columns=[\"pid\",\"y\",\"p\",\"n\"])\n",
        "    return outs[\"y\"].values, outs[\"p\"].values, outs[\"n\"].values\n",
        "\n",
        "def eval_patient(y, p, thr=0.5):\n",
        "    auc = roc_auc_score(y, p) if len(np.unique(y))>1 else np.nan\n",
        "    pr  = average_precision_score(y, p) if len(np.unique(y))>1 else np.nan\n",
        "    yhat = (p>=thr).astype(int)\n",
        "    acc = accuracy_score(y, yhat)\n",
        "    pre = precision_score(y, yhat, zero_division=0)\n",
        "    rec = recall_score(y, yhat, zero_division=0)\n",
        "    return {\"AUC\":auc,\"PR-AUC\":pr,\"Acc\":acc,\"P\":pre,\"R\":rec,\"thr\":thr,\"n\":len(y)}\n",
        "\n",
        "# Mean pooling\n",
        "yV_m, pV_m, _ = patient_pool_mean(pr_va, y_va, pid_va)\n",
        "yT_m, pT_m, _ = patient_pool_mean(pr_te, y_te, pid_te)\n",
        "\n",
        "# Attention pooling\n",
        "yV_a, pV_a, _ = patient_pool_attention(log_va, y_va, pid_va, temp=1.0)\n",
        "yT_a, pT_a, _ = patient_pool_attention(log_te, y_te, pid_te, temp=1.0)\n",
        "\n",
        "print(\"VAL (mean@0.5):\", eval_patient(yV_m, pV_m, 0.5))\n",
        "print(\"TEST(mean@0.5):\", eval_patient(yT_m, pT_m, 0.5))\n",
        "print(\"VAL (attn@0.5):\", eval_patient(yV_a, pV_a, 0.5))\n",
        "print(\"TEST(attn@0.5):\", eval_patient(yT_a, pT_a, 0.5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzEP2NS9ZiRP",
        "outputId": "8db0803a-dc57-4aa9-cf13-3055a195be2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL (mean@0.5): {'AUC': np.float64(0.7388888888888889), 'PR-AUC': np.float64(0.6587843825001534), 'Acc': 0.6382978723404256, 'P': 0.6, 'R': 0.45, 'thr': 0.5, 'n': 47}\n",
            "TEST(mean@0.5): {'AUC': np.float64(0.875925925925926), 'PR-AUC': np.float64(0.7626011139703089), 'Acc': 0.723404255319149, 'P': 0.7333333333333333, 'R': 0.55, 'thr': 0.5, 'n': 47}\n",
            "VAL (attn@0.5): {'AUC': np.float64(0.7611111111111111), 'PR-AUC': np.float64(0.6851116491294511), 'Acc': 0.6382978723404256, 'P': 0.6153846153846154, 'R': 0.4, 'thr': 0.5, 'n': 47}\n",
            "TEST(attn@0.5): {'AUC': np.float64(0.8722222222222222), 'PR-AUC': np.float64(0.764498046830885), 'Acc': 0.7659574468085106, 'P': 0.8461538461538461, 'R': 0.55, 'thr': 0.5, 'n': 47}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 5: Temperature scaling (ajuste en VAL) y evaluación paciente\n",
        "\n",
        "class TemperatureScaler(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.logT = nn.Parameter(torch.zeros(1))  # T = exp(logT) >= 1\n",
        "\n",
        "    def forward(self, logits):\n",
        "        T = torch.exp(self.logT) + 1e-6\n",
        "        return logits / T\n",
        "\n",
        "def fit_temperature(logits_val, y_val, max_iter=2000, lr=0.01):\n",
        "    y = torch.tensor(y_val, dtype=torch.float32, device=DEVICE)\n",
        "    z = torch.tensor(logits_val, dtype=torch.float32, device=DEVICE)\n",
        "    ts = TemperatureScaler().to(DEVICE)\n",
        "    opt = torch.optim.LBFGS(ts.parameters(), lr=lr, max_iter=50, line_search_fn=\"strong_wolfe\")\n",
        "\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "    def closure():\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        zT = ts(z)\n",
        "        loss = bce(zT, y)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    last = 1e9\n",
        "    for _ in range(30):\n",
        "        loss = opt.step(closure)\n",
        "        if abs(loss.item()-last) < 1e-7:\n",
        "            break\n",
        "        last = loss.item()\n",
        "    with torch.no_grad():\n",
        "        T = torch.exp(ts.logT).item() + 1e-6\n",
        "    return ts, T\n",
        "\n",
        "ts, T_val = fit_temperature(log_va, y_va)\n",
        "print(\"Temperatura ajustada (VAL):\", round(T_val,4))\n",
        "\n",
        "def apply_T(logits, T):\n",
        "    return logits / (T + 1e-6)\n",
        "\n",
        "# Aplicar T\n",
        "pV_m_T = 1/(1+np.exp(-apply_T(log_va, T_val)))\n",
        "pT_m_T = 1/(1+np.exp(-apply_T(log_te, T_val)))\n",
        "\n",
        "# Recalcular pooling\n",
        "yV_mean, pV_mean, _ = patient_pool_mean(pV_m_T, y_va, pid_va)\n",
        "yT_mean, pT_mean, _ = patient_pool_mean(pT_m_T, y_te, pid_te)\n",
        "\n",
        "yV_attn, pV_attn, _ = patient_pool_attention(apply_T(log_va, T_val), y_va, pid_va, temp=1.0)\n",
        "yT_attn, pT_attn, _ = patient_pool_attention(apply_T(log_te, T_val), y_te, pid_te, temp=1.0)\n",
        "\n",
        "print(\"VAL mean (temp):\", eval_patient(yV_mean, pV_mean, 0.5))\n",
        "print(\"TEST mean(temp):\", eval_patient(yT_mean, pT_mean, 0.5))\n",
        "print(\"VAL attn (temp):\", eval_patient(yV_attn, pV_attn, 0.5))\n",
        "print(\"TEST attn(temp):\", eval_patient(yT_attn, pT_attn, 0.5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDtnpj_4buea",
        "outputId": "dfcf87e4-1be0-4321-ec2f-0a497ccc98e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperatura ajustada (VAL): 2.6732\n",
            "VAL mean (temp): {'AUC': np.float64(0.7481481481481482), 'PR-AUC': np.float64(0.664989747813566), 'Acc': 0.6382978723404256, 'P': 0.6, 'R': 0.45, 'thr': 0.5, 'n': 47}\n",
            "TEST mean(temp): {'AUC': np.float64(0.8759259259259259), 'PR-AUC': np.float64(0.7620865452057403), 'Acc': 0.723404255319149, 'P': 0.7333333333333333, 'R': 0.55, 'thr': 0.5, 'n': 47}\n",
            "VAL attn (temp): {'AUC': np.float64(0.75), 'PR-AUC': np.float64(0.660088903151692), 'Acc': 0.6382978723404256, 'P': 0.6153846153846154, 'R': 0.4, 'thr': 0.5, 'n': 47}\n",
            "TEST attn(temp): {'AUC': np.float64(0.8777777777777778), 'PR-AUC': np.float64(0.7617757509275546), 'Acc': 0.723404255319149, 'P': 0.7333333333333333, 'R': 0.55, 'thr': 0.5, 'n': 47}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 6: Umbral clínico (VAL recall≥0.90) y evaluación en TEST\n",
        "\n",
        "def pick_threshold_for_recall(y, p, min_recall=0.90):\n",
        "    prec, rec, thr = precision_recall_curve(y, p)\n",
        "    # precision_recall_curve devuelve thr para todos menos el primer punto\n",
        "    thr = np.append(thr, 1.0)  # para igualar longitudes\n",
        "    # buscamos el primer punto con recall >= min_recall que maximice precisión\n",
        "    mask = (rec >= min_recall)\n",
        "    if mask.any():\n",
        "        idx = np.argmax(prec[mask])\n",
        "        thr_sel = thr[mask][idx]\n",
        "        return float(thr_sel), float(prec[mask][idx]), float(rec[mask][idx])\n",
        "    else:\n",
        "        # si no hay, devolvemos el que más recall tenga\n",
        "        idx = np.argmax(rec)\n",
        "        return float(thr[idx]), float(prec[idx]), float(rec[idx])\n",
        "\n",
        "# Elegimos el *mejor pooling* en VAL (entre mean y attn tras temperature scaling) por PR-AUC\n",
        "def pr_auc(y,p):\n",
        "    return average_precision_score(y,p) if len(np.unique(y))>1 else np.nan\n",
        "\n",
        "pr_val_mean = pr_auc(yV_mean, pV_mean)\n",
        "pr_val_attn = pr_auc(yV_attn, pV_attn)\n",
        "use_attn = (pr_val_attn > pr_val_mean)\n",
        "print(f\"Comparativa VAL PR-AUC: mean={pr_val_mean:.3f} | attn={pr_val_attn:.3f} → usar {'ATTN' if use_attn else 'MEAN'}\")\n",
        "\n",
        "if use_attn:\n",
        "    thr, prec, rec = pick_threshold_for_recall(yV_attn, pV_attn, min_recall=0.90)\n",
        "    val_metrics  = eval_patient(yV_attn, pV_attn, thr)\n",
        "    test_metrics = eval_patient(yT_attn, pT_attn, thr)\n",
        "else:\n",
        "    thr, prec, rec = pick_threshold_for_recall(yV_mean, pV_mean, min_recall=0.90)\n",
        "    val_metrics  = eval_patient(yV_mean, pV_mean, thr)\n",
        "    test_metrics = eval_patient(yT_mean, pT_mean, thr)\n",
        "\n",
        "print(f\"→ Umbral clínico (VAL recall≥0.90): thr={thr:.4f} | precision={prec:.3f} | recall={rec:.3f}\")\n",
        "print(\"[VAL-final]\", val_metrics)\n",
        "print(\"[TEST-final]\", test_metrics)\n",
        "\n",
        "# Guardar resumen\n",
        "res = {\n",
        "    \"pooling_used\": \"attention\" if use_attn else \"mean\",\n",
        "    \"temperature\": T_val,\n",
        "    \"threshold\": thr,\n",
        "    \"val_metrics\": val_metrics,\n",
        "    \"test_metrics\": test_metrics\n",
        "}\n",
        "with open(f\"{OUT_DIR}/ft_effb3_patient_eval.json\",\"w\") as f:\n",
        "    json.dump(res, f, indent=2)\n",
        "print(\"Resumen guardado en:\", f\"{OUT_DIR}/ft_effb3_patient_eval.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z45kRVu6bxKZ",
        "outputId": "c05ae2fe-174a-42e9-e825-e0c637994aa5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparativa VAL PR-AUC: mean=0.665 | attn=0.660 → usar MEAN\n",
            "→ Umbral clínico (VAL recall≥0.90): thr=0.3651 | precision=0.588 | recall=1.000\n",
            "[VAL-final] {'AUC': np.float64(0.7481481481481482), 'PR-AUC': np.float64(0.664989747813566), 'Acc': 0.7021276595744681, 'P': 0.5882352941176471, 'R': 1.0, 'thr': 0.3651449978351593, 'n': 47}\n",
            "[TEST-final] {'AUC': np.float64(0.8759259259259259), 'PR-AUC': np.float64(0.7620865452057403), 'Acc': 0.7446808510638298, 'P': 0.625, 'R': 1.0, 'thr': 0.3651449978351593, 'n': 47}\n",
            "Resumen guardado en: /content/drive/MyDrive/CognitivaAI/ft_effb3_colab/ft_effb3_patient_eval.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CoZ7vuhlb3BB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}