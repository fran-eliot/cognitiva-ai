{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Celda 0\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, sys, platform, torch, random, numpy as np\n",
        "print(f\"Python: {sys.version.split()[0]} | PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA disponible: {torch.cuda.is_available()} | device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
        "\n",
        "# Semillas\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR6yqeFJnQUt",
        "outputId": "1de65e0f-fe58-4b79-a6e5-7de76a1f212d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Python: 3.12.11 | PyTorch: 2.8.0+cu126\n",
            "CUDA disponible: True | device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1\n",
        "BASE_DIR      = \"/content/drive/MyDrive/CognitivaAI\"\n",
        "DATA_DIR      = os.path.join(BASE_DIR, \"oas1_data\")                   # contiene DATA/OAS1_PROCESSED/*.png + CSV originales\n",
        "ARTIFACTS_DIR = os.path.join(BASE_DIR, \"oas1_resnet18_linearprobe\")   # carpeta de trabajo/resultados\n",
        "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"BASE_DIR     :\", BASE_DIR)\n",
        "print(\"DATA_DIR     :\", DATA_DIR)\n",
        "print(\"ARTIFACTS_DIR:\", ARTIFACTS_DIR)\n",
        "\n",
        "def data_path(*x):       return os.path.join(DATA_DIR, *x)\n",
        "def artifacts_path(*x):  return os.path.join(ARTIFACTS_DIR, *x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHQTFbu8oTU3",
        "outputId": "19d9eb6c-8d5c-465f-d0fd-567f83ce81ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASE_DIR     : /content/drive/MyDrive/CognitivaAI\n",
            "DATA_DIR     : /content/drive/MyDrive/CognitivaAI/oas1_data\n",
            "ARTIFACTS_DIR: /content/drive/MyDrive/CognitivaAI/oas1_resnet18_linearprobe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 2\n",
        "expected = [\n",
        "    data_path(\"oas1_train.csv\"),\n",
        "    data_path(\"oas1_val.csv\"),\n",
        "    data_path(\"oas1_test.csv\"),\n",
        "]\n",
        "for p in expected:\n",
        "    print((\"OK\" if os.path.exists(p) else \"FALTA\"), \"→\", p)\n",
        "\n",
        "# Directorio con PNGs (antiguamente los CSV referencian \"DATA/OAS1_PROCESSED/...\")\n",
        "png_root_guess = data_path(\"DATA\", \"OAS1_PROCESSED\")\n",
        "print(\"Posible carpeta de PNGs:\", png_root_guess, \"| existe:\", os.path.exists(png_root_guess))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDQaYbaBoao4",
        "outputId": "181a4d85-2f7b-41c0-9bea-f6cbc5ac8f0b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK → /content/drive/MyDrive/CognitivaAI/oas1_data/oas1_train.csv\n",
            "OK → /content/drive/MyDrive/CognitivaAI/oas1_data/oas1_val.csv\n",
            "OK → /content/drive/MyDrive/CognitivaAI/oas1_data/oas1_test.csv\n",
            "Posible carpeta de PNGs: /content/drive/MyDrive/CognitivaAI/oas1_data/DATA/OAS1_PROCESSED | existe: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/CognitivaAI/oas1_data\")\n",
        "\n",
        "pngs_lower = list(DATA_DIR.rglob(\"*.png\"))\n",
        "pngs_upper = list(DATA_DIR.rglob(\"*.PNG\"))\n",
        "all_pngs  = pngs_lower + pngs_upper\n",
        "\n",
        "print(f\"PNG encontrados (recursivo): {len(all_pngs)}\")\n",
        "for p in all_pngs[:5]:\n",
        "    print(\"  •\", p)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vsDivOlpy6a",
        "outputId": "4d2e5599-d2f1-415d-f342-f5df9848ebf8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PNG encontrados (recursivo): 8320\n",
            "  • /content/drive/MyDrive/CognitivaAI/oas1_data/OAS1_0404_MR1_slice18.png\n",
            "  • /content/drive/MyDrive/CognitivaAI/oas1_data/OAS1_0404_MR1_slice13.png\n",
            "  • /content/drive/MyDrive/CognitivaAI/oas1_data/OAS1_0404_MR1_slice01.png\n",
            "  • /content/drive/MyDrive/CognitivaAI/oas1_data/OAS1_0404_MR1_slice19.png\n",
            "  • /content/drive/MyDrive/CognitivaAI/oas1_data/OAS1_0404_MR1_slice00.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "BASE_DIR      = Path(\"/content/drive/MyDrive/CognitivaAI\")\n",
        "DATA_DIR      = BASE_DIR/\"oas1_data\"\n",
        "CSV_TRAIN_SRC = DATA_DIR/\"oas1_train_colab.csv\"\n",
        "CSV_VAL_SRC   = DATA_DIR/\"oas1_val_colab.csv\"\n",
        "CSV_TEST_SRC  = DATA_DIR/\"oas1_test_colab.csv\"\n",
        "\n",
        "# 1) Indexar TODO lo que sea PNG (independiente del subdirectorio y del case)\n",
        "all_pngs = list(DATA_DIR.rglob(\"*.png\")) + list(DATA_DIR.rglob(\"*.PNG\"))\n",
        "print(f\"[Index] PNG totales encontrados: {len(all_pngs)}\")\n",
        "if len(all_pngs) == 0:\n",
        "    raise SystemExit(\"⛔ No se encontraron PNG en DATA_DIR. Revisa el montaje de Drive o la ubicación de 'oas1_data'.\")\n",
        "\n",
        "# 2) Mapa basename -> lista de paths completos (por si hay duplicados)\n",
        "index_by_base = defaultdict(list)\n",
        "for p in all_pngs:\n",
        "    index_by_base[p.name].append(p)\n",
        "\n",
        "# 3) Función de reparación: reemplaza la columna 'png_path' por el path real encontrado\n",
        "def repair_csv(csv_in: Path, csv_out: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(csv_in)\n",
        "    if 'png_path' not in df.columns:\n",
        "        raise ValueError(f\"{csv_in} no tiene columna 'png_path'\")\n",
        "\n",
        "    new_paths = []\n",
        "    missing   = []\n",
        "\n",
        "    for p_old in df['png_path'].astype(str).tolist():\n",
        "        base = Path(p_old).name  # nos quedamos con el basename\n",
        "        candidates = index_by_base.get(base, [])\n",
        "        if candidates:\n",
        "            # si hay varios con el mismo basename, elegimos el 1º (habitual: sólo 1)\n",
        "            new_paths.append(str(candidates[0]))\n",
        "        else:\n",
        "            new_paths.append(None)\n",
        "            missing.append(base)\n",
        "\n",
        "    df['png_path'] = new_paths\n",
        "    n_missing = sum(p is None for p in new_paths)\n",
        "    if n_missing > 0:\n",
        "        print(f\"⚠ {csv_in.name}: faltantes tras reparar = {n_missing}\")\n",
        "        print(\"   Ejemplos:\", missing[:5])\n",
        "    else:\n",
        "        print(f\"✅ {csv_in.name}: todas las rutas reparadas\")\n",
        "\n",
        "    df.to_csv(csv_out, index=False)\n",
        "    print(f\"   → Guardado: {csv_out}\")\n",
        "    return df\n",
        "\n",
        "CSV_TRAIN_FIXED = DATA_DIR/\"oas1_train_colab_mapped.csv\"\n",
        "CSV_VAL_FIXED   = DATA_DIR/\"oas1_val_colab_mapped.csv\"\n",
        "CSV_TEST_FIXED  = DATA_DIR/\"oas1_test_colab_mapped.csv\"\n",
        "\n",
        "df_tr = repair_csv(CSV_TRAIN_SRC, CSV_TRAIN_FIXED)\n",
        "df_va = repair_csv(CSV_VAL_SRC,   CSV_VAL_FIXED)\n",
        "df_te = repair_csv(CSV_TEST_SRC,  CSV_TEST_FIXED)\n",
        "\n",
        "print(\"\\nResumen shapes:\",\n",
        "      f\"train={df_tr.shape}, val={df_va.shape}, test={df_te.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsqIre_-p8_Q",
        "outputId": "cf2c965c-08ac-4965-d700-2dbe019b0151"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Index] PNG totales encontrados: 8320\n",
            "✅ oas1_train_colab.csv: todas las rutas reparadas\n",
            "   → Guardado: /content/drive/MyDrive/CognitivaAI/oas1_data/oas1_train_colab_mapped.csv\n",
            "✅ oas1_val_colab.csv: todas las rutas reparadas\n",
            "   → Guardado: /content/drive/MyDrive/CognitivaAI/oas1_data/oas1_val_colab_mapped.csv\n",
            "✅ oas1_test_colab.csv: todas las rutas reparadas\n",
            "   → Guardado: /content/drive/MyDrive/CognitivaAI/oas1_data/oas1_test_colab_mapped.csv\n",
            "\n",
            "Resumen shapes: train=(2820, 6), val=(940, 6), test=(940, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def count_missing(csv_path: Path) -> int:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    miss = 0\n",
        "    for p in df['png_path']:\n",
        "        if not Path(p).exists():\n",
        "            miss += 1\n",
        "    return miss\n",
        "\n",
        "for csv in [CSV_TRAIN_FIXED, CSV_VAL_FIXED, CSV_TEST_FIXED]:\n",
        "    miss = count_missing(csv)\n",
        "    print(f\"{csv.name} → faltantes: {miss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJHkM61TqBZW",
        "outputId": "ca243171-d4eb-4f4f-f46e-6198593db05c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oas1_train_colab_mapped.csv → faltantes: 0\n",
            "oas1_val_colab_mapped.csv → faltantes: 0\n",
            "oas1_test_colab_mapped.csv → faltantes: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 4) DATASETS & DATALOADERS (Colab) ===\n",
        "import pandas as pd, numpy as np, torch, os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "\n",
        "BASE_DIR      = Path(\"/content/drive/MyDrive/CognitivaAI\")\n",
        "DATA_DIR      = BASE_DIR/\"oas1_data\"\n",
        "ARTIFACTS_DIR = BASE_DIR/\"oas1_resnet18_linearprobe\"\n",
        "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CSV_TRAIN = str(DATA_DIR/\"oas1_train_colab_mapped.csv\")\n",
        "CSV_VAL   = str(DATA_DIR/\"oas1_val_colab_mapped.csv\")\n",
        "CSV_TEST  = str(DATA_DIR/\"oas1_test_colab_mapped.csv\")\n",
        "print(\"Usando CSV mapeados:\\n\", CSV_TRAIN, \"\\n\", CSV_VAL, \"\\n\", CSV_TEST)\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "train_tf = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomRotation(degrees=10),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "eval_tf = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "class PNGCsvDataset(Dataset):\n",
        "    def __init__(self, csv_path, transform):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        # Asegurar tipos\n",
        "        self.df[\"png_path\"] = self.df[\"png_path\"].astype(str)\n",
        "        self.df[\"target\"]   = self.df[\"target\"].astype(int)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        p   = Path(row[\"png_path\"])\n",
        "        if not p.exists():\n",
        "            raise FileNotFoundError(f\"No puedo leer {p}\")\n",
        "        img = Image.open(p).convert(\"RGB\")\n",
        "        x   = self.transform(img)\n",
        "        y   = int(row[\"target\"])\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "ds_tr = PNGCsvDataset(CSV_TRAIN, train_tf)\n",
        "ds_va = PNGCsvDataset(CSV_VAL,   eval_tf)\n",
        "ds_te = PNGCsvDataset(CSV_TEST,  eval_tf)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Workers/pin_memory para T4\n",
        "num_workers = 4 if device.type == \"cuda\" else 2\n",
        "dl_tr = DataLoader(ds_tr, batch_size=32, shuffle=True,  num_workers=num_workers, pin_memory=(device.type==\"cuda\"), persistent_workers=True)\n",
        "dl_va = DataLoader(ds_va, batch_size=64, shuffle=False, num_workers=num_workers, pin_memory=(device.type==\"cuda\"), persistent_workers=True)\n",
        "dl_te = DataLoader(ds_te, batch_size=64, shuffle=False, num_workers=num_workers, pin_memory=(device.type==\"cuda\"), persistent_workers=True)\n",
        "\n",
        "print(f\"[OK] Train={len(ds_tr)}  Val={len(ds_va)}  Test={len(ds_te)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfyzkJFArYWu",
        "outputId": "f295175e-e213-4a9b-ccf5-f0b205c58c53"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando CSV mapeados:\n",
            " /content/drive/MyDrive/CognitivaAI/oas1_data/oas1_train_colab_mapped.csv \n",
            " /content/drive/MyDrive/CognitivaAI/oas1_data/oas1_val_colab_mapped.csv \n",
            " /content/drive/MyDrive/CognitivaAI/oas1_data/oas1_test_colab_mapped.csv\n",
            "Device: cuda\n",
            "[OK] Train=2820  Val=940  Test=940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 5) SANITY CHECK ===\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# Modelo ligero (ResNet18)\n",
        "m = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "m.fc = nn.Linear(m.fc.in_features, 2)\n",
        "m = m.to(device)\n",
        "\n",
        "# Un batch\n",
        "xb, yb = next(iter(dl_tr))\n",
        "xb, yb = xb.to(device), yb.to(device)\n",
        "with torch.no_grad():\n",
        "    logits = m(xb)\n",
        "print(\"Sanity logits shape:\", logits.shape)  # (B,2) esperado\n",
        "\n",
        "# Mini-overfit a 1 batch (5-10 pasos) para comprobar que baja la loss\n",
        "opt = torch.optim.AdamW(m.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "crit = nn.CrossEntropyLoss()\n",
        "m.train()\n",
        "for step in range(10):\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    out = m(xb)\n",
        "    loss = crit(out, yb)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    print(f\"step={step:02d} loss={loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdH58gb-rgbC",
        "outputId": "507acbe7-f649-441c-9e95-fed0d9fd5489"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity logits shape: torch.Size([32, 2])\n",
            "step=00 loss=0.7878\n",
            "step=01 loss=0.1106\n",
            "step=02 loss=0.0202\n",
            "step=03 loss=0.0078\n",
            "step=04 loss=0.0038\n",
            "step=05 loss=0.0022\n",
            "step=06 loss=0.0014\n",
            "step=07 loss=0.0009\n",
            "step=08 loss=0.0006\n",
            "step=09 loss=0.0004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda A — Diagnóstico (muestra 10 rutas del loader actual y comprueba existencia)\n",
        "import os, itertools\n",
        "\n",
        "def peek_loader_paths(loader, n=10):\n",
        "    ds = getattr(loader, \"dataset\", None)\n",
        "    if ds is None or not hasattr(ds, \"df\"):\n",
        "        print(\"❌ Este loader no tiene .dataset.df; probablemente no está inicializado correctamente.\")\n",
        "        return\n",
        "    print(\"CSV usado por este dataset (primeras 3 filas):\")\n",
        "    print(ds.df.head(3))\n",
        "    paths = ds.df[\"png_path\"].tolist()[:n]\n",
        "    print(f\"\\nPrimeras {n} rutas:\")\n",
        "    for p in paths:\n",
        "        print(\" -\", p, \"| exists:\", os.path.exists(p))\n",
        "\n",
        "print(\"— TRAIN —\")\n",
        "try: peek_loader_paths(train_loader, n=10)\n",
        "except NameError: print(\"train_loader no existe en memoria.\")\n",
        "\n",
        "print(\"\\n— VAL —\")\n",
        "try: peek_loader_paths(val_loader, n=10)\n",
        "except NameError: print(\"val_loader no existe en memoria.\")\n",
        "\n",
        "print(\"\\n— TEST —\")\n",
        "try: peek_loader_paths(test_loader, n=10)\n",
        "except NameError: print(\"test_loader no existe en memoria.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOQDrDVA5Y_t",
        "outputId": "26be1177-ad46-4d73-c4aa-5dd3e12ef057"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "— TRAIN —\n",
            "train_loader no existe en memoria.\n",
            "\n",
            "— VAL —\n",
            "val_loader no existe en memoria.\n",
            "\n",
            "— TEST —\n",
            "test_loader no existe en memoria.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda B — Loaders robustos desde cero (Colab)\n",
        "import os, glob, json\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) Paths base (ajusta si cambiaste carpetas)\n",
        "BASE_DIR      = Path(\"/content/drive/MyDrive/CognitivaAI\")\n",
        "DATA_DIR      = BASE_DIR / \"oas1_data\"\n",
        "ARTIFACTS_DIR = BASE_DIR / \"oas1_resnet18_linearprobe\"\n",
        "\n",
        "CSV_TRAIN = DATA_DIR / \"oas1_train_colab_mapped.csv\"\n",
        "CSV_VAL   = DATA_DIR / \"oas1_val_colab_mapped.csv\"\n",
        "CSV_TEST  = DATA_DIR / \"oas1_test_colab_mapped.csv\"\n",
        "\n",
        "assert CSV_TRAIN.exists() and CSV_VAL.exists() and CSV_TEST.exists(), \"Faltan CSV *_colab_mapped.csv\"\n",
        "\n",
        "# 2) Índice global (filename -> ruta completa real)\n",
        "pngs = glob.glob(str(DATA_DIR / \"*.png\"))\n",
        "index_by_name = {Path(p).name: p for p in pngs}\n",
        "print(f\"[Index] PNG totales encontrados: {len(index_by_name)}\")\n",
        "\n",
        "# 3) Dataset robusto (normaliza y aplica fallbacks)\n",
        "from torchvision import transforms\n",
        "\n",
        "def default_transform():\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((224,224), interpolation=Image.BILINEAR),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485,0.456,0.406],\n",
        "                             std=[0.229,0.224,0.225]),\n",
        "    ])\n",
        "\n",
        "class DatasetRobusto(Dataset):\n",
        "    def __init__(self, csv_path, transform=None, index_by_name=None):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.df[\"png_path\"] = self.df[\"png_path\"].astype(str)\n",
        "        self.df[\"target\"] = self.df[\"target\"].astype(int)\n",
        "        self.transform = transform or default_transform()\n",
        "        self.index_by_name = index_by_name or {}\n",
        "        self.data_dir = Path(DATA_DIR)\n",
        "\n",
        "    def _resolve_path(self, p):\n",
        "        # 1) normalizar separadores\n",
        "        p = str(p).replace(\"\\\\\", \"/\")\n",
        "        # 2) si es relativo con prefijos viejos, quedarnos con solo el nombre\n",
        "        name = Path(p).name\n",
        "        # 3) si existe tal cual, bien\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "        # 4) si existe bajo DATA_DIR + nombre, usarlo\n",
        "        cand = str(self.data_dir / name)\n",
        "        if os.path.exists(cand):\n",
        "            return cand\n",
        "        # 5) buscar en índice por nombre\n",
        "        if name in self.index_by_name and os.path.exists(self.index_by_name[name]):\n",
        "            return self.index_by_name[name]\n",
        "        # 6) último intento: si la ruta está dentro de DATA_DIR con subcarpetas (no es tu caso)\n",
        "        #    podrías implementar una búsqueda recursiva, pero sería más lenta\n",
        "        raise FileNotFoundError(f\"No puedo resolver la ruta del PNG: {p}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        row = self.df.iloc[i]\n",
        "        p = self._resolve_path(row[\"png_path\"])\n",
        "        y = int(row[\"target\"])\n",
        "\n",
        "        with Image.open(p) as im:\n",
        "            im = im.convert(\"RGB\")\n",
        "        x = self.transform(im)\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# 4) Instancias de datasets y loaders\n",
        "train_ds = DatasetRobusto(CSV_TRAIN, index_by_name=index_by_name)\n",
        "val_ds   = DatasetRobusto(CSV_VAL,   index_by_name=index_by_name)\n",
        "test_ds  = DatasetRobusto(CSV_TEST,  index_by_name=index_by_name)\n",
        "\n",
        "BATCH_SIZE   = 32\n",
        "NUM_WORKERS  = 2 if torch.cuda.is_available() else 0\n",
        "PIN_MEMORY   = torch.cuda.is_available()\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=False)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=False)\n",
        "\n",
        "# 5) Smoke test: tomar 1 batch y verificar\n",
        "xb, yb = next(iter(train_loader))\n",
        "print(f\"[Smoke test] batch shapes: x={tuple(xb.shape)} y={tuple(yb.shape)}  (OK si no hubo excepciones)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTVwoJO56DCR",
        "outputId": "b57688e6-6918-4eb4-f442-7b075cc982bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Index] PNG totales encontrados: 8320\n",
            "[Smoke test] batch shapes: x=(32, 3, 224, 224) y=(32,)  (OK si no hubo excepciones)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda C — Lista 10 rutas resueltas y existencia real\n",
        "for i in range(10):\n",
        "    p = train_ds._resolve_path(train_ds.df.iloc[i][\"png_path\"])\n",
        "    print(i, \"→\", p, \"| exists:\", os.path.exists(p))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL84p6Ed6N3z",
        "outputId": "8d0e3a48-741d-4487-da6e-d4db287aa150"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 → /content/drive/MyDrive/CognitivaAI/oas1_data/OAS1_0001_MR1_slice00.png | exists: True\n",
            "1 → /content/drive/MyDrive/CognitivaAI/oas1_data/OAS1_0001_MR1_slice01.png | exists: True\n",
            "2 → /content/drive/MyDrive/CognitivaAI/oas1_data/OAS1_0001_MR1_slice02.png | exists: True\n",
            "3 → /content/drive/MyDrive/CognitivaAI/oas1_data/OAS1_0001_MR1_slice03.png | exists: True\n",
            "4 → /content/drive/MyDrive/CognitivaAI/oas1_data/OAS1_0001_MR1_slice04.png | exists: True\n",
            "5 → /content/drive/MyDrive/CognitivaAI/oas1_data/OAS1_0001_MR1_slice05.png | exists: True\n",
            "6 → /content/drive/MyDrive/CognitivaAI/oas1_data/OAS1_0001_MR1_slice06.png | exists: True\n",
            "7 → /content/drive/MyDrive/CognitivaAI/oas1_data/OAS1_0001_MR1_slice07.png | exists: True\n",
            "8 → /content/drive/MyDrive/CognitivaAI/oas1_data/OAS1_0001_MR1_slice08.png | exists: True\n",
            "9 → /content/drive/MyDrive/CognitivaAI/oas1_data/OAS1_0001_MR1_slice09.png | exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Celda 6 — Entrenamiento (Colab, T4)\n",
        "#  - Warm-up (congelar backbone) + fine-tuning\n",
        "#  - Early stopping por AUC de validación\n",
        "#  - Guardado \"solo pesos\" + metadatos JSON (evita UnpicklingError)\n",
        "#  - AMP (mixed precision) en GPU\n",
        "# Requisitos previos:\n",
        "#   • train_loader, val_loader, test_loader ya creados (celdas previas)\n",
        "#   • ARTIFACTS_DIR definido\n",
        "# ============================\n",
        "\n",
        "import os, json, math, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cuda\":\n",
        "    # En Ampere (T4) mejora algo el rendimiento numérico de matmul\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ---------- Paths de checkpoints (solo pesos + metadatos) ----------\n",
        "CKPT_DIR = Path(ARTIFACTS_DIR) / \"ckpts_colab\"\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "best_weights = CKPT_DIR / \"resnet18_best_val_auc_weights.pth\"\n",
        "best_meta    = CKPT_DIR / \"resnet18_best_val_auc_meta.json\"\n",
        "\n",
        "# ---------- Modelo (ResNet18 ligera para CPU/GPU) ----------\n",
        "# Nota: usamos pesos ImageNet para acelerar convergencia\n",
        "def create_model(num_classes=2, pretrained=True):\n",
        "    weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "    model = models.resnet18(weights=weights)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "model = create_model().to(device)\n",
        "\n",
        "# ---------- Pérdida ----------\n",
        "# Usamos CrossEntropy con un poco de label smoothing (mejor calibración/robustez)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
        "\n",
        "# ---------- Optimizador y Scheduler ----------\n",
        "LR = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# En PyTorch recientes, ReduceLROnPlateau acepta 'verbose', pero para compatibilidad lo omitimos\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"max\", factor=0.5, patience=2\n",
        ")\n",
        "\n",
        "# ---------- Utilidad de evaluación ----------\n",
        "@torch.no_grad()\n",
        "def evaluate(loader, model, device):\n",
        "    model.eval()\n",
        "    all_probs, all_labels, all_preds = [], [], []\n",
        "    running_loss, running_n = 0.0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        prob = torch.softmax(logits, dim=1)[:, 1]\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "\n",
        "        running_loss += loss.item() * y.size(0)\n",
        "        running_n    += y.size(0)\n",
        "\n",
        "        all_probs.append(prob.detach().cpu().numpy())\n",
        "        all_labels.append(y.detach().cpu().numpy())\n",
        "        all_preds.append(pred.detach().cpu().numpy())\n",
        "\n",
        "    if running_n == 0:\n",
        "        return {\"loss\": np.nan, \"acc\": np.nan, \"auc\": np.nan, \"prauc\": np.nan}\n",
        "\n",
        "    all_probs  = np.concatenate(all_probs)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    all_preds  = np.concatenate(all_preds)\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    try:\n",
        "        auc = roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else np.nan\n",
        "    except Exception:\n",
        "        auc = np.nan\n",
        "    try:\n",
        "        prauc = average_precision_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else np.nan\n",
        "    except Exception:\n",
        "        prauc = np.nan\n",
        "\n",
        "    loss = running_loss / running_n\n",
        "    return {\"loss\": loss, \"acc\": acc, \"auc\": auc, \"prauc\": prauc}\n",
        "\n",
        "# ---------- Config de entrenamiento ----------\n",
        "EPOCHS_TOTAL   = 12          # con T4 suele ir rápido; ajusta si lo ves estable\n",
        "EPOCHS_WARMUP  = 2           # congelamos backbone al principio\n",
        "PATIENCE_ES    = 4           # early stopping si no mejora el AUC de validación\n",
        "best_auc       = -np.inf\n",
        "patience_count = 0\n",
        "\n",
        "# ---------- Warm-up: congelar backbone (solo entrenar la FC) ----------\n",
        "for name, p in model.named_parameters():\n",
        "    if not name.startswith(\"fc.\"):\n",
        "        p.requires_grad = False\n",
        "\n",
        "print(\"Device:\", device.type)\n",
        "print(\"Warm-up congelado.\")\n",
        "\n",
        "# AMP: mixed precision para GPU; en CPU no aporta\n",
        "scaler = torch.amp.GradScaler(device.type, enabled=(device.type == \"cuda\"))\n",
        "\n",
        "# ---------- Loop de entrenamiento ----------\n",
        "for epoch in range(1, EPOCHS_TOTAL + 1):\n",
        "    t0 = time.time()\n",
        "    model.train()\n",
        "    running_loss, running_n = 0.0, 0\n",
        "\n",
        "    # Pasar a fine-tuning (descongelar) después del warm-up\n",
        "    if epoch == (EPOCHS_WARMUP + 1):\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = True\n",
        "        # Re-inicializamos optimizer (mismo LR) tras descongelar\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "        print(\"→ Fine-tuning activado (backbone descongelado)\")\n",
        "\n",
        "    # --------- Entrenamiento por batches ---------\n",
        "    for x, y in tqdm(train_loader, desc=f\"[Epoch {epoch}/{EPOCHS_TOTAL}] Train\", leave=False):\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # AMP autocast solo en GPU\n",
        "        with torch.autocast(device_type=device.type, dtype=torch.float16, enabled=(device.type == \"cuda\")):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * y.size(0)\n",
        "        running_n    += y.size(0)\n",
        "\n",
        "    train_loss = running_loss / max(1, running_n)\n",
        "\n",
        "    # --------- Evaluación (train y val) ---------\n",
        "    train_metrics = evaluate(train_loader, model, device)\n",
        "    val_metrics   = evaluate(val_loader,   model, device)\n",
        "\n",
        "    # Scheduler según AUC de validación\n",
        "    if not math.isnan(val_metrics[\"auc\"]):\n",
        "        scheduler.step(val_metrics[\"auc\"])\n",
        "\n",
        "    t1 = time.time()\n",
        "    print(f\"\"\"\n",
        "Epoch {epoch:02d}/{EPOCHS_TOTAL} | {t1 - t0:.1f}s\n",
        "  Train: loss={train_loss:.4f} | acc={train_metrics['acc']:.3f} | AUC={train_metrics['auc']:.3f} | PR-AUC={train_metrics['prauc']:.3f}\n",
        "  Val  : loss={val_metrics['loss']:.4f} | acc={val_metrics['acc']:.3f} | AUC={val_metrics['auc']:.3f} | PR-AUC={val_metrics['prauc']:.3f}\n",
        "\"\"\".strip())\n",
        "\n",
        "    # --------- Checkpoint si mejora AUC ---------\n",
        "    cur_auc = val_metrics[\"auc\"]\n",
        "    if not math.isnan(cur_auc) and cur_auc > best_auc:\n",
        "        best_auc = cur_auc\n",
        "        patience_count = 0\n",
        "\n",
        "        # Guardamos SOLO los pesos y un JSON con metadatos\n",
        "        torch.save(model.state_dict(), best_weights)\n",
        "        with open(best_meta, \"w\") as f:\n",
        "            json.dump({\"epoch\": int(epoch), \"best_auc\": float(best_auc)}, f, indent=2)\n",
        "\n",
        "        print(f\"\\n  ✅ Nuevo mejor AUC val = {best_auc:.4f} → {best_weights}\")\n",
        "    else:\n",
        "        patience_count += 1\n",
        "        print(f\"\\n  ↪ Sin mejora AUC (patience {patience_count}/{PATIENCE_ES})\")\n",
        "\n",
        "    # --------- Early stopping ---------\n",
        "    if patience_count >= PATIENCE_ES:\n",
        "        print(\"\\n🛑 Early stopping.\")\n",
        "        break\n",
        "\n",
        "# ---------- Cargar mejor checkpoint y evaluar en TEST ----------\n",
        "if best_weights.exists():\n",
        "    # Carga segura: \"solo pesos\" evita el UnpicklingError de PyTorch 2.6+\n",
        "    model.load_state_dict(torch.load(best_weights, map_location=device))\n",
        "    try:\n",
        "        meta = json.loads(best_meta.read_text())\n",
        "        print(f\"\\nCargado mejor checkpoint (AUC val={meta.get('best_auc', float('nan')):.4f})\")\n",
        "    except Exception:\n",
        "        print(\"\\nCargado mejor checkpoint (metadatos no disponibles)\")\n",
        "else:\n",
        "    print(\"\\n⚠ No se encontró checkpoint. Se evalúa el último estado del modelo.\")\n",
        "\n",
        "test_metrics = evaluate(test_loader, model, device)\n",
        "print(f\"\"\"\n",
        "=== TEST (por PNG) ===\n",
        "  Loss   : {test_metrics['loss']:.4f}\n",
        "  Acc    : {test_metrics['acc']:.3f}\n",
        "  ROC-AUC: {test_metrics['auc']:.3f}\n",
        "  PR-AUC : {test_metrics['prauc']:.3f}\n",
        "\"\"\".strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzQRBjLyyVIg",
        "outputId": "70835a7b-ad60-4fc3-e79c-d86f387e3c84"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Warm-up congelado.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/12 | 74.8s\n",
            "  Train: loss=0.7096 | acc=0.533 | AUC=0.542 | PR-AUC=0.465\n",
            "  Val  : loss=0.7132 | acc=0.516 | AUC=0.525 | PR-AUC=0.448\n",
            "\n",
            "  ✅ Nuevo mejor AUC val = 0.5246 → /content/drive/MyDrive/CognitivaAI/oas1_resnet18_linearprobe/ckpts_colab/resnet18_best_val_auc_weights.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02/12 | 38.7s\n",
            "  Train: loss=0.6923 | acc=0.575 | AUC=0.569 | PR-AUC=0.490\n",
            "  Val  : loss=0.6965 | acc=0.564 | AUC=0.531 | PR-AUC=0.449\n",
            "\n",
            "  ✅ Nuevo mejor AUC val = 0.5311 → /content/drive/MyDrive/CognitivaAI/oas1_resnet18_linearprobe/ckpts_colab/resnet18_best_val_auc_weights.pth\n",
            "→ Fine-tuning activado (backbone descongelado)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03/12 | 39.5s\n",
            "  Train: loss=0.5578 | acc=0.880 | AUC=0.965 | PR-AUC=0.956\n",
            "  Val  : loss=0.7455 | acc=0.616 | AUC=0.656 | PR-AUC=0.568\n",
            "\n",
            "  ✅ Nuevo mejor AUC val = 0.6562 → /content/drive/MyDrive/CognitivaAI/oas1_resnet18_linearprobe/ckpts_colab/resnet18_best_val_auc_weights.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04/12 | 40.8s\n",
            "  Train: loss=0.2848 | acc=0.971 | AUC=0.997 | PR-AUC=0.996\n",
            "  Val  : loss=0.8571 | acc=0.635 | AUC=0.636 | PR-AUC=0.574\n",
            "\n",
            "  ↪ Sin mejora AUC (patience 1/4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05/12 | 40.4s\n",
            "  Train: loss=0.1870 | acc=0.973 | AUC=1.000 | PR-AUC=1.000\n",
            "  Val  : loss=1.2197 | acc=0.609 | AUC=0.633 | PR-AUC=0.553\n",
            "\n",
            "  ↪ Sin mejora AUC (patience 2/4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06/12 | 38.8s\n",
            "  Train: loss=0.1619 | acc=0.995 | AUC=1.000 | PR-AUC=1.000\n",
            "  Val  : loss=1.0093 | acc=0.640 | AUC=0.642 | PR-AUC=0.584\n",
            "\n",
            "  ↪ Sin mejora AUC (patience 3/4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07/12 | 39.3s\n",
            "  Train: loss=0.1403 | acc=1.000 | AUC=1.000 | PR-AUC=1.000\n",
            "  Val  : loss=0.9037 | acc=0.619 | AUC=0.630 | PR-AUC=0.561\n",
            "\n",
            "  ↪ Sin mejora AUC (patience 4/4)\n",
            "\n",
            "🛑 Early stopping.\n",
            "\n",
            "Cargado mejor checkpoint (AUC val=0.6562)\n",
            "=== TEST (por PNG) ===\n",
            "  Loss   : 0.8172\n",
            "  Acc    : 0.621\n",
            "  ROC-AUC: 0.642\n",
            "  PR-AUC : 0.535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 7 — Eval a nivel paciente con el mejor checkpoint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
        "\n",
        "# reutiliza 'model', 'device', 'val_loader', 'test_loader', 'val_ds', 'test_ds'\n",
        "# y el path 'best_weights' que ya definiste en tu celda de entrenamiento\n",
        "\n",
        "# 1) Cargar los mejores pesos (por si el kernel se reinició)\n",
        "model.load_state_dict(torch.load(best_weights, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_png(loader, ds, device):\n",
        "    \"\"\"Devuelve un DF alineado con ds.df (assume shuffle=False).\"\"\"\n",
        "    probs, preds, labels = [], [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "        logits = model(xb)\n",
        "        prob1 = torch.softmax(logits, dim=1)[:, 1]\n",
        "        pred  = torch.argmax(logits, dim=1)\n",
        "        probs.append(prob1.cpu().numpy())\n",
        "        preds.append(pred.cpu().numpy())\n",
        "        labels.append(yb.cpu().numpy())\n",
        "    prob = np.concatenate(probs)\n",
        "    pred = np.concatenate(preds)\n",
        "    lab  = np.concatenate(labels)\n",
        "\n",
        "    df = ds.df.copy().reset_index(drop=True)\n",
        "    # si no existe 'scan_id' en el CSV, lo extraemos del filename\n",
        "    if \"scan_id\" not in df.columns:\n",
        "        df[\"scan_id\"] = df[\"png_path\"].apply(lambda p: Path(p).name.split(\"_slice\")[0])\n",
        "    df[\"prob1\"] = prob\n",
        "    df[\"pred\"]  = pred\n",
        "    df[\"target\"] = lab\n",
        "    return df\n",
        "\n",
        "def metrics_bin(y_true, score, hard=None):\n",
        "    acc = accuracy_score(y_true, (hard if hard is not None else (score>=0.5).astype(int)))\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, score) if len(np.unique(y_true))>1 else np.nan\n",
        "    except Exception:\n",
        "        auc = np.nan\n",
        "    try:\n",
        "        pr  = average_precision_score(y_true, score) if len(np.unique(y_true))>1 else np.nan\n",
        "    except Exception:\n",
        "        pr = np.nan\n",
        "    return acc, auc, pr\n",
        "\n",
        "def aggregate_patient(df_png):\n",
        "    \"\"\"Agrega por scan_id: prob_media. Toma 1er target (debería ser consistente).\"\"\"\n",
        "    g = df_png.groupby(\"scan_id\", as_index=False).agg(\n",
        "        prob_mean=(\"prob1\",\"mean\"),\n",
        "        target=(\"target\",\"first\")\n",
        "    )\n",
        "    g[\"pred\"] = (g[\"prob_mean\"]>=0.5).astype(int)\n",
        "    return g\n",
        "\n",
        "# 2) PNG-level\n",
        "val_png = predict_png(val_loader, val_ds, device)\n",
        "tst_png = predict_png(test_loader, test_ds, device)\n",
        "\n",
        "val_acc, val_auc, val_pr = metrics_bin(val_png[\"target\"].values, val_png[\"prob1\"].values, val_png[\"pred\"].values)\n",
        "tst_acc, tst_auc, tst_pr = metrics_bin(tst_png[\"target\"].values, tst_png[\"prob1\"].values, tst_png[\"pred\"].values)\n",
        "\n",
        "print(f\"[VAL-PNG ] Acc={val_acc:.3f} | AUC={val_auc:.3f} | PR-AUC={val_pr:.3f}\")\n",
        "print(f\"[TEST-PNG] Acc={tst_acc:.3f} | AUC={tst_auc:.3f} | PR-AUC={tst_pr:.3f}\")\n",
        "\n",
        "# 3) Paciente-level\n",
        "val_pat = aggregate_patient(val_png)\n",
        "tst_pat = aggregate_patient(tst_png)\n",
        "\n",
        "vA, vU, vP = metrics_bin(val_pat[\"target\"].values, val_pat[\"prob_mean\"].values, val_pat[\"pred\"].values)\n",
        "tA, tU, tP = metrics_bin(tst_pat[\"target\"].values, tst_pat[\"prob_mean\"].values, tst_pat[\"pred\"].values)\n",
        "\n",
        "print(f\"[VAL-PACIENTE ] n={len(val_pat)} | Acc={vA:.3f} | AUC={vU:.3f} | PR-AUC={vP:.3f}\")\n",
        "print(f\"[TEST-PACIENTE] n={len(tst_pat)} | Acc={tA:.3f} | AUC={tU:.3f} | PR-AUC={tP:.3f}\")\n",
        "\n",
        "# 4) Guardar resultados\n",
        "out_dir = Path(ARTIFACTS_DIR) / \"patient_eval_colab\"\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "val_png.to_csv(out_dir/\"val_png_preds.csv\", index=False)\n",
        "tst_png.to_csv(out_dir/\"test_png_preds.csv\", index=False)\n",
        "val_pat.to_csv(out_dir/\"val_patient_preds.csv\", index=False)\n",
        "tst_pat.to_csv(out_dir/\"test_patient_preds.csv\", index=False)\n",
        "print(\"CSV guardados en:\", out_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PN99SFRs8ayf",
        "outputId": "e0351d7d-e14d-44da-acc1-e0134cca9cdf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VAL-PNG ] Acc=0.616 | AUC=0.656 | PR-AUC=0.568\n",
            "[TEST-PNG] Acc=0.621 | AUC=0.642 | PR-AUC=0.535\n",
            "[VAL-PACIENTE ] n=47 | Acc=0.681 | AUC=0.704 | PR-AUC=0.627\n",
            "[TEST-PACIENTE] n=47 | Acc=0.723 | AUC=0.676 | PR-AUC=0.578\n",
            "CSV guardados en: /content/drive/MyDrive/CognitivaAI/oas1_resnet18_linearprobe/patient_eval_colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 8 — Extracción de embeddings (512-D) y guardado a .npz\n",
        "from torchvision import models\n",
        "\n",
        "# 1) reconstruir el mismo modelo y cargar pesos\n",
        "full = models.resnet18(weights=None)\n",
        "full.fc = nn.Linear(full.fc.in_features, 2)  # igual que en training\n",
        "full.load_state_dict(torch.load(best_weights, map_location=device))\n",
        "full.to(device).eval()\n",
        "\n",
        "# 2) backbone = todo menos la FC; salida [B,512,1,1] → flatten [B,512]\n",
        "backbone = nn.Sequential(*list(full.children())[:-1]).to(device).eval()\n",
        "for p in backbone.parameters(): p.requires_grad = False\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_embeddings(loader, ds):\n",
        "    feats, labs = [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        f = backbone(xb)            # [B,512,1,1]\n",
        "        f = torch.flatten(f, 1)     # [B,512]\n",
        "        feats.append(f.cpu().numpy())\n",
        "        labs.append(yb.numpy())\n",
        "    X = np.concatenate(feats, axis=0).astype(np.float32)\n",
        "    y = np.concatenate(labs,  axis=0).astype(np.int64)\n",
        "    return X, y\n",
        "\n",
        "# IMPORTANT: loaders con shuffle=False para alinear con .df si luego quieres mapear scan_id\n",
        "train_loader_eval = DataLoader(train_ds, batch_size=64, shuffle=False,\n",
        "                               num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "val_loader_eval   = DataLoader(val_ds,   batch_size=64, shuffle=False,\n",
        "                               num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "test_loader_eval  = DataLoader(test_ds,  batch_size=64, shuffle=False,\n",
        "                               num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "X_tr, y_tr = extract_embeddings(train_loader_eval, train_ds)\n",
        "X_va, y_va = extract_embeddings(val_loader_eval,   val_ds)\n",
        "X_te, y_te = extract_embeddings(test_loader_eval,  test_ds)\n",
        "\n",
        "emb_dir = Path(ARTIFACTS_DIR) / \"embeddings_npz_colab\"\n",
        "emb_dir.mkdir(parents=True, exist_ok=True)\n",
        "npz_path = emb_dir / \"oas1_resnet18_backbone_embeddings.npz\"\n",
        "np.savez_compressed(npz_path, X_train=X_tr, y_train=y_tr, X_val=X_va, y_val=y_va, X_test=X_te, y_test=y_te)\n",
        "\n",
        "print(\"Embeddings:\",\n",
        "      f\"train={X_tr.shape}, val={X_va.shape}, test={X_te.shape}\")\n",
        "print(\"Guardado en:\", npz_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoOYQECJ8f0U",
        "outputId": "d154870c-33f2-4e25-e30c-64c950a63417"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings: train=(2820, 512), val=(940, 512), test=(940, 512)\n",
            "Guardado en: /content/drive/MyDrive/CognitivaAI/oas1_resnet18_linearprobe/embeddings_npz_colab/oas1_resnet18_backbone_embeddings.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 9 — Linear probe (Logistic Regression) + métricas por PNG y por paciente\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 1) cargar embeddings (por si ejecutas en otra sesión)\n",
        "npz = np.load(npz_path)\n",
        "X_tr, y_tr = npz[\"X_train\"], npz[\"y_train\"]\n",
        "X_va, y_va = npz[\"X_val\"],   npz[\"y_val\"]\n",
        "X_te, y_te = npz[\"X_test\"],  npz[\"y_test\"]\n",
        "\n",
        "# 2) LR con escalado\n",
        "clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"lr\", LogisticRegression(max_iter=2000, solver=\"lbfgs\", n_jobs=1))\n",
        "]).fit(X_tr, y_tr)\n",
        "\n",
        "# 3) Probabilidades a nivel PNG\n",
        "p_va = clf.predict_proba(X_va)[:,1]\n",
        "p_te = clf.predict_proba(X_te)[:,1]\n",
        "\n",
        "pred_va = (p_va>=0.5).astype(int)\n",
        "pred_te = (p_te>=0.5).astype(int)\n",
        "\n",
        "def metrics(y, proba, pred):\n",
        "    acc = accuracy_score(y, pred)\n",
        "    try: auc = roc_auc_score(y, proba) if len(np.unique(y))>1 else np.nan\n",
        "    except: auc = np.nan\n",
        "    try: pr  = average_precision_score(y, proba) if len(np.unique(y))>1 else np.nan\n",
        "    except: pr = np.nan\n",
        "    return acc, auc, pr\n",
        "\n",
        "va_acc, va_auc, va_pr = metrics(y_va, p_va, pred_va)\n",
        "te_acc, te_auc, te_pr = metrics(y_te, p_te, pred_te)\n",
        "\n",
        "print(f\"[VAL-PNG]  Acc={va_acc:.3f} | AUC={va_auc:.3f} | PR-AUC={va_pr:.3f}\")\n",
        "print(f\"[TEST-PNG] Acc={te_acc:.3f} | AUC={te_auc:.3f} | PR-AUC={te_pr:.3f}\")\n",
        "\n",
        "# 4) Paciente-level (usamos el orden de val_ds/test_ds porque loaders_eval tenían shuffle=False)\n",
        "def patient_from_df(df):\n",
        "    if \"scan_id\" not in df.columns:\n",
        "        df = df.copy()\n",
        "        df[\"scan_id\"] = df[\"png_path\"].apply(lambda p: Path(p).name.split(\"_slice\")[0])\n",
        "    return df\n",
        "\n",
        "val_df = val_ds.df.copy().reset_index(drop=True)\n",
        "tst_df = test_ds.df.copy().reset_index(drop=True)\n",
        "val_df = patient_from_df(val_df);   val_df[\"prob1\"] = p_va; val_df[\"pred\"] = pred_va\n",
        "tst_df = patient_from_df(tst_df);   tst_df[\"prob1\"] = p_te; tst_df[\"pred\"] = pred_te\n",
        "\n",
        "def aggregate(df):\n",
        "    g = df.groupby(\"scan_id\", as_index=False).agg(\n",
        "        prob_mean=(\"prob1\",\"mean\"),\n",
        "        target   =(\"target\",\"first\")\n",
        "    )\n",
        "    g[\"pred\"] = (g[\"prob_mean\"]>=0.5).astype(int)\n",
        "    return g\n",
        "\n",
        "val_pat = aggregate(val_df)\n",
        "tst_pat = aggregate(tst_df)\n",
        "\n",
        "vA, vU, vP = metrics(val_pat[\"target\"].values, val_pat[\"prob_mean\"].values, val_pat[\"pred\"].values)\n",
        "tA, tU, tP = metrics(tst_pat[\"target\"].values, tst_pat[\"prob_mean\"].values, tst_pat[\"pred\"].values)\n",
        "\n",
        "print(f\"[VAL-PACIENTE]  n={len(val_pat)} | Acc={vA:.3f} | AUC={vU:.3f} | PR-AUC={vP:.3f}\")\n",
        "print(f\"[TEST-PACIENTE] n={len(tst_pat)} | Acc={tA:.3f} | AUC={tU:.3f} | PR-AUC={tP:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbdUqDp98zil",
        "outputId": "4eecd357-0832-4090-f7f4-ff28cd8a5ab6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VAL-PNG]  Acc=0.621 | AUC=0.624 | PR-AUC=0.531\n",
            "[TEST-PNG] Acc=0.627 | AUC=0.661 | PR-AUC=0.528\n",
            "[VAL-PACIENTE]  n=47 | Acc=0.553 | AUC=0.719 | PR-AUC=0.623\n",
            "[TEST-PACIENTE] n=47 | Acc=0.681 | AUC=0.715 | PR-AUC=0.605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones (hasta embeddings + LR):\n",
        "- El fine-tuning de ResNet18 en Colab (OASIS-1) alcanza un AUC paciente ~0.68.\n",
        "- La agregación a nivel paciente es fundamental: los resultados por PNG son más bajos.\n",
        "- El enfoque de embeddings + Linear Probe mejora la estabilidad y generaliza mejor:\n",
        "  • VAL-PACIENTE AUC ≈ 0.72\n",
        "  • TEST-PACIENTE AUC ≈ 0.72\n",
        "- Esto confirma que ResNet18 captura representaciones útiles, y que un clasificador lineal posterior\n",
        "  puede dar métricas similares o mejores con mucho menor coste de entrenamiento.\n"
      ],
      "metadata": {
        "id": "Ux37bB7L90Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Calibración (Colab) — LR vs LR + Isotónica\n",
        "#  - Carga embeddings (.npz)\n",
        "#  - Entrena LR base y Calibrated (isotónica)\n",
        "#  - Curvas de calibración y Brier score\n",
        "#  - Agregado a nivel paciente (umbral recall≥0.90 en VAL)\n",
        "#  - Guarda modelos, plots y CSVs\n",
        "# ============================================\n",
        "\n",
        "import os, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, average_precision_score,\n",
        "    brier_score_loss, precision_recall_curve, precision_score, recall_score\n",
        ")\n",
        "import joblib\n",
        "\n",
        "# ---------- Rutas base (Colab/Drive) ----------\n",
        "BASE_DIR      = Path(\"/content/drive/MyDrive/CognitivaAI\")\n",
        "DATA_DIR      = BASE_DIR / \"oas1_data\"\n",
        "ARTIFACTS_DIR = BASE_DIR / \"oas1_resnet18_linearprobe\"\n",
        "\n",
        "EMB_DIR   = ARTIFACTS_DIR / \"embeddings_npz_colab\"\n",
        "EMB_PATH  = EMB_DIR / \"oas1_resnet18_backbone_embeddings.npz\"\n",
        "\n",
        "CAL_DIR   = ARTIFACTS_DIR / \"calibration_colab\"\n",
        "CAL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# CSV de mapeo (orden por PNG para paciente-level)\n",
        "TRAIN_CSV = DATA_DIR / \"oas1_train_colab_mapped.csv\"\n",
        "VAL_CSV   = DATA_DIR / \"oas1_val_colab_mapped.csv\"\n",
        "TEST_CSV  = DATA_DIR / \"oas1_test_colab_mapped.csv\"\n",
        "\n",
        "assert EMB_PATH.exists(), f\"No existe el NPZ de embeddings: {EMB_PATH}\"\n",
        "assert TRAIN_CSV.exists() and VAL_CSV.exists() and TEST_CSV.exists(), \"Faltan CSV *_colab_mapped.csv\"\n",
        "\n",
        "# ---------- Carga robusta del NPZ ----------\n",
        "npz = np.load(EMB_PATH, allow_pickle=False)\n",
        "keys = sorted(list(npz.keys()))\n",
        "print(\"NPZ cargado:\", EMB_PATH)\n",
        "print(\"Claves:\", keys)\n",
        "\n",
        "def get_array(npz, *names):\n",
        "    for n in names:\n",
        "        if n in npz:\n",
        "            return npz[n]\n",
        "    return None\n",
        "\n",
        "X_tr = get_array(npz, \"X_train\", \"X_tr\", \"arr_0\")\n",
        "y_tr = get_array(npz, \"y_train\", \"y_tr\", \"arr_1\")\n",
        "X_va = get_array(npz, \"X_val\",   \"X_va\", \"arr_2\")\n",
        "y_va = get_array(npz, \"y_val\",   \"y_va\", \"arr_3\")\n",
        "X_te = get_array(npz, \"X_test\",  \"X_te\", \"arr_4\")\n",
        "y_te = get_array(npz, \"y_test\",  \"y_te\", \"arr_5\")\n",
        "\n",
        "for name, arr in [(\"X_tr\", X_tr), (\"y_tr\", y_tr), (\"X_va\", X_va), (\"y_va\", y_va), (\"X_te\", X_te), (\"y_te\", y_te)]:\n",
        "    if arr is None:\n",
        "        raise KeyError(f\"Falta {name} en el NPZ\")\n",
        "\n",
        "print(f\"Shapes: X_tr={X_tr.shape}, X_va={X_va.shape}, X_te={X_te.shape}\")\n",
        "\n",
        "# ---------- LR (sin calibrar) ----------\n",
        "lr_base = Pipeline([\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"lr\", LogisticRegression(max_iter=2000, solver=\"lbfgs\", n_jobs=1))\n",
        "])\n",
        "lr_base.fit(X_tr, y_tr)\n",
        "\n",
        "proba_va_base = lr_base.predict_proba(X_va)[:, 1]\n",
        "proba_te_base = lr_base.predict_proba(X_te)[:, 1]\n",
        "\n",
        "def safe_auc(y, p):\n",
        "    try:\n",
        "        return roc_auc_score(y, p) if len(np.unique(y)) > 1 else np.nan\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def safe_ap(y, p):\n",
        "    try:\n",
        "        return average_precision_score(y, p) if len(np.unique(y)) > 1 else np.nan\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def report_split(y_true, proba, name=\"SPLIT\"):\n",
        "    pred = (proba >= 0.5).astype(int)\n",
        "    acc   = accuracy_score(y_true, pred)\n",
        "    auc   = safe_auc(y_true, proba)\n",
        "    prauc = safe_ap(y_true, proba)\n",
        "    brier = brier_score_loss(y_true, proba)\n",
        "    print(f\"[{name}] Acc={acc:.3f} | AUC={auc:.3f} | PR-AUC={prauc:.3f} | Brier={brier:.4f}\")\n",
        "    return dict(acc=acc, auc=auc, prauc=prauc, brier=brier)\n",
        "\n",
        "print(\"\\n=== SIN CALIBRAR (LR) ===\")\n",
        "m_val_base = report_split(y_va, proba_va_base, \"VAL\")\n",
        "m_tst_base = report_split(y_te, proba_te_base, \"TEST\")\n",
        "\n",
        "# ---------- LR + Isotónica ----------\n",
        "# NOTA: usamos CalibratedClassifierCV con 'prefit=True' (calibra sobre VAL).\n",
        "# Entrenamos base en TRAIN y calibramos con VAL para emular pipeline \"train→val→test\" limpio.\n",
        "lr_iso = CalibratedClassifierCV(base_estimator=lr_base, method=\"isotonic\", cv=\"prefit\")\n",
        "lr_iso.fit(X_va, y_va)\n",
        "\n",
        "proba_va_iso = lr_iso.predict_proba(X_va)[:, 1]\n",
        "proba_te_iso = lr_iso.predict_proba(X_te)[:, 1]\n",
        "\n",
        "print(\"\\n=== CALIBRADO (LR + Isotónica) ===\")\n",
        "m_val_iso = report_split(y_va, proba_va_iso, \"VAL\")\n",
        "m_tst_iso = report_split(y_te, proba_te_iso, \"TEST\")\n",
        "\n",
        "# ---------- Curvas de calibración ----------\n",
        "def plot_calibration(ax, y_true, proba, label):\n",
        "    frac_pos, mean_pred = calibration_curve(y_true, proba, n_bins=10, strategy=\"uniform\")\n",
        "    ax.plot(mean_pred, frac_pos, marker=\"o\", linestyle=\"-\", label=label)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "# VAL\n",
        "axes[0].plot([0, 1], [0, 1], \"k--\", lw=1)\n",
        "plot_calibration(axes[0], y_va, proba_va_base, f\"LR (Brier={m_val_base['brier']:.3f})\")\n",
        "plot_calibration(axes[0], y_va, proba_va_iso,  f\"LR+Iso (Brier={m_val_iso['brier']:.3f})\")\n",
        "axes[0].set_title(\"Calibración (VAL)\")\n",
        "axes[0].set_xlabel(\"Prob. media predicha\")\n",
        "axes[0].set_ylabel(\"Frecuencia observada\")\n",
        "axes[0].legend()\n",
        "\n",
        "# TEST\n",
        "axes[1].plot([0, 1], [0, 1], \"k--\", lw=1)\n",
        "plot_calibration(axes[1], y_te, proba_te_base, f\"LR (Brier={m_tst_base['brier']:.3f})\")\n",
        "plot_calibration(axes[1], y_te, proba_te_iso,  f\"LR+Iso (Brier={m_tst_iso['brier']:.3f})\")\n",
        "axes[1].set_title(\"Calibración (TEST)\")\n",
        "axes[1].set_xlabel(\"Prob. media predicha\")\n",
        "axes[1].set_ylabel(\"Frecuencia observada\")\n",
        "axes[1].legend()\n",
        "\n",
        "fig.tight_layout()\n",
        "calib_png = CAL_DIR / \"calibration_curves_VAL_TEST.png\"\n",
        "fig.savefig(calib_png, dpi=150, bbox_inches=\"tight\")\n",
        "plt.close(fig)\n",
        "print(f\"\\nFiguras guardadas en: {calib_png}\")\n",
        "\n",
        "# ---------- Guardado de modelos y resumen ----------\n",
        "joblib.dump(lr_base, CAL_DIR / \"lr_uncalibrated.joblib\")\n",
        "joblib.dump(lr_iso,  CAL_DIR / \"lr_isotonic_calibrated.joblib\")\n",
        "\n",
        "summary = {\n",
        "    \"npz_path\": str(EMB_PATH),\n",
        "    \"val_uncalibrated\": m_val_base,\n",
        "    \"test_uncalibrated\": m_tst_base,\n",
        "    \"val_isotonic\": m_val_iso,\n",
        "    \"test_isotonic\": m_tst_iso,\n",
        "}\n",
        "with open(CAL_DIR / \"calibration_summary.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"Modelos y resumen guardados en:\", CAL_DIR)\n",
        "\n",
        "# ===========================================================\n",
        "# Agregado a nivel de PACIENTE (usando CSV *_colab_mapped.csv)\n",
        "#   - proba por PNG → media por paciente\n",
        "#   - selección de umbral en VAL (recall ≥ 0.90)\n",
        "#   - evaluación en TEST con ese umbral\n",
        "# ===========================================================\n",
        "\n",
        "# Cargamos CSVs y comprobamos longitud vs embeddings\n",
        "df_tr = pd.read_csv(TRAIN_CSV)\n",
        "df_va = pd.read_csv(VAL_CSV)\n",
        "df_te = pd.read_csv(TEST_CSV)\n",
        "\n",
        "# Intentar detectar columna de paciente\n",
        "def detect_patient_col(df):\n",
        "    for c in [\"patient_id_canon\", \"patient_id\", \"scan_id\", \"scan\", \"pid\"]:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    # si solo hay scan_id, usamos prefijo hasta _MR\n",
        "    if \"png_path\" in df.columns:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "def to_patient_id(df):\n",
        "    col = detect_patient_col(df)\n",
        "    if col is not None and col in df.columns and col != \"scan_id\":\n",
        "        return df[col].astype(str).tolist()\n",
        "    # fallback: derivar de scan_id si existe; si no, derivar del png_path\n",
        "    if \"scan_id\" in df.columns:\n",
        "        # ejemplo: OAS1_0001_MR1 → paciente OAS1_0001\n",
        "        return df[\"scan_id\"].astype(str).str.split(\"_MR\", n=1, expand=True)[0].tolist()\n",
        "    if \"png_path\" in df.columns:\n",
        "        return df[\"png_path\"].astype(str).apply(lambda p: Path(p).name.split(\"_MR\")[0]).tolist()\n",
        "    raise ValueError(\"No encuentro columnas para derivar patient_id.\")\n",
        "\n",
        "pid_tr = to_patient_id(df_tr)\n",
        "pid_va = to_patient_id(df_va)\n",
        "pid_te = to_patient_id(df_te)\n",
        "\n",
        "assert len(pid_tr) == len(y_tr), \"train CSV y embeddings no alinean\"\n",
        "assert len(pid_va) == len(y_va), \"val CSV y embeddings no alinean\"\n",
        "assert len(pid_te) == len(y_te), \"test CSV y embeddings no alinean\"\n",
        "\n",
        "# Usamos el clasificador calibrado (isotónica) para las probabilidades finales\n",
        "proba_va = proba_va_iso\n",
        "proba_te = proba_te_iso\n",
        "\n",
        "def aggregate_patient_probs(pid_list, probs, labels):\n",
        "    df = pd.DataFrame({\"patient_id\": pid_list, \"prob\": probs, \"label\": labels})\n",
        "    # media por paciente\n",
        "    g = df.groupby(\"patient_id\", as_index=False).agg(prob=(\"prob\", \"mean\"), label=(\"label\", \"first\"))\n",
        "    return g\n",
        "\n",
        "g_va = aggregate_patient_probs(pid_va, proba_va, y_va)\n",
        "g_te = aggregate_patient_probs(pid_te, proba_te, y_te)\n",
        "\n",
        "def patient_metrics(df, thr=0.5, name=\"SPLIT-PAC\"):\n",
        "    pred = (df[\"prob\"].values >= thr).astype(int)\n",
        "    y    = df[\"label\"].values.astype(int)\n",
        "    acc   = accuracy_score(y, pred)\n",
        "    auc   = safe_auc(y, df[\"prob\"].values)\n",
        "    prauc = safe_ap(y, df[\"prob\"].values)\n",
        "    pre   = precision_score(y, pred, zero_division=0)\n",
        "    rec   = recall_score(y, pred, zero_division=0)\n",
        "    print(f\"[{name}] n={len(df)} | Acc={acc:.3f} | AUC={auc:.3f} | PR-AUC={prauc:.3f} | P={pre:.3f} | R={rec:.3f}\")\n",
        "    return dict(acc=acc, auc=auc, prauc=prauc, precision=pre, recall=rec, n=len(df))\n",
        "\n",
        "print(\"\\n=== PACIENTE (prob media por paciente, thr=0.5) ===\")\n",
        "pm_va = patient_metrics(g_va, 0.5, \"VAL-PACIENTE\")\n",
        "pm_te = patient_metrics(g_te, 0.5, \"TEST-PACIENTE\")\n",
        "\n",
        "# Selección de umbral en VAL buscando recall ≥ 0.90 con mejor precision\n",
        "prec, rec, thr = precision_recall_curve(g_va[\"label\"].values, g_va[\"prob\"].values)\n",
        "# precision_recall_curve devuelve thresholds de tamaño len(prec)-1\n",
        "thr_candidates = np.concatenate([[0.0], thr, [1.0]])\n",
        "\n",
        "best_thr = 0.5\n",
        "best_prec = -1.0\n",
        "for t in thr_candidates:\n",
        "    pr = (g_va[\"prob\"].values >= t).astype(int)\n",
        "    r  = recall_score(g_va[\"label\"].values, pr, zero_division=0)\n",
        "    p  = precision_score(g_va[\"label\"].values, pr, zero_division=0)\n",
        "    if r >= 0.90 and p > best_prec:\n",
        "        best_prec = p\n",
        "        best_thr = t\n",
        "\n",
        "print(f\"\\n→ Umbral escogido (VAL, recall≥0.90): thr={best_thr:.4f} | precision={best_prec:.3f}\")\n",
        "\n",
        "pm_va_thr = patient_metrics(g_va, best_thr, \"VAL-PACIENTE(thr*)\")\n",
        "pm_te_thr = patient_metrics(g_te, best_thr, \"TEST-PACIENTE(thr*)\")\n",
        "\n",
        "# Guardar tablas por paciente y resumen JSON\n",
        "PAT_DIR = CAL_DIR / \"patient_level\"\n",
        "PAT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "g_va.to_csv(PAT_DIR / \"val_patient_probs.csv\", index=False)\n",
        "g_te.to_csv(PAT_DIR / \"test_patient_probs.csv\", index=False)\n",
        "\n",
        "patient_summary = {\n",
        "    \"val_patient_default_thr0.5\": pm_va,\n",
        "    \"test_patient_default_thr0.5\": pm_te,\n",
        "    \"chosen_threshold\": float(best_thr),\n",
        "    \"val_patient_thr_star\": pm_va_thr,\n",
        "    \"test_patient_thr_star\": pm_te_thr\n",
        "}\n",
        "with open(PAT_DIR / \"patient_summary.json\", \"w\") as f:\n",
        "    json.dump(patient_summary, f, indent=2)\n",
        "\n",
        "print(\"\\nCSVs por paciente y resumen guardados en:\", PAT_DIR)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB5ttiyg-tVb",
        "outputId": "3bcb4795-6372-461a-be19-899e9a70fb9e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NPZ cargado: /content/drive/MyDrive/CognitivaAI/oas1_resnet18_linearprobe/embeddings_npz_colab/oas1_resnet18_backbone_embeddings.npz\n",
            "Claves: ['X_test', 'X_train', 'X_val', 'y_test', 'y_train', 'y_val']\n",
            "Shapes: X_tr=(2820, 512), X_va=(940, 512), X_te=(940, 512)\n",
            "\n",
            "=== SIN CALIBRAR (LR) ===\n",
            "[VAL] Acc=0.621 | AUC=0.624 | PR-AUC=0.531 | Brier=0.3492\n",
            "[TEST] Acc=0.627 | AUC=0.661 | PR-AUC=0.528 | Brier=0.3347\n",
            "\n",
            "=== CALIBRADO (LR + Isotónica) ===\n",
            "[VAL] Acc=0.626 | AUC=0.639 | PR-AUC=0.539 | Brier=0.2295\n",
            "[TEST] Acc=0.629 | AUC=0.656 | PR-AUC=0.537 | Brier=0.2328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figuras guardadas en: /content/drive/MyDrive/CognitivaAI/oas1_resnet18_linearprobe/calibration_colab/calibration_curves_VAL_TEST.png\n",
            "Modelos y resumen guardados en: /content/drive/MyDrive/CognitivaAI/oas1_resnet18_linearprobe/calibration_colab\n",
            "\n",
            "=== PACIENTE (prob media por paciente, thr=0.5) ===\n",
            "[VAL-PACIENTE] n=47 | Acc=0.617 | AUC=0.730 | PR-AUC=0.641 | P=0.625 | R=0.250\n",
            "[TEST-PACIENTE] n=47 | Acc=0.638 | AUC=0.719 | PR-AUC=0.610 | P=0.615 | R=0.400\n",
            "\n",
            "→ Umbral escogido (VAL, recall≥0.90): thr=0.4037 | precision=0.643\n",
            "[VAL-PACIENTE(thr*)] n=47 | Acc=0.745 | AUC=0.730 | PR-AUC=0.641 | P=0.643 | R=0.900\n",
            "[TEST-PACIENTE(thr*)] n=47 | Acc=0.638 | AUC=0.719 | PR-AUC=0.610 | P=0.560 | R=0.700\n",
            "\n",
            "CSVs por paciente y resumen guardados en: /content/drive/MyDrive/CognitivaAI/oas1_resnet18_linearprobe/calibration_colab/patient_level\n"
          ]
        }
      ]
    }
  ]
}