{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e05a7c34623e47aa887dfc9305cd1aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_645a0ccce3694c17a6121115f3eaa31d",
              "IPY_MODEL_067aaab3cf1345d4b601d00c6354ae07",
              "IPY_MODEL_ad3a6f2234ff4f4199f686a6b16a3283"
            ],
            "layout": "IPY_MODEL_bb45967aa60e40e18249b9ae6105ac10"
          }
        },
        "645a0ccce3694c17a6121115f3eaa31d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4a2e14132ec4bf0b1f9af91be645874",
            "placeholder": "​",
            "style": "IPY_MODEL_7fd6d6921fad499fad9922b86e761c38",
            "value": "model.safetensors: 100%"
          }
        },
        "067aaab3cf1345d4b601d00c6354ae07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27c983ad5032439fbaa5639d6e13c059",
            "max": 49335454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cdc92b7ad8c44acc89399c74681e9581",
            "value": 49335454
          }
        },
        "ad3a6f2234ff4f4199f686a6b16a3283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66adf76c5b674eed9489c718a006bfc9",
            "placeholder": "​",
            "style": "IPY_MODEL_a61efa444c4e4805ad970aa27acbf58c",
            "value": " 49.3M/49.3M [00:00&lt;00:00, 198MB/s]"
          }
        },
        "bb45967aa60e40e18249b9ae6105ac10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4a2e14132ec4bf0b1f9af91be645874": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fd6d6921fad499fad9922b86e761c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27c983ad5032439fbaa5639d6e13c059": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdc92b7ad8c44acc89399c74681e9581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66adf76c5b674eed9489c718a006bfc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a61efa444c4e4805ad970aa27acbf58c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgl_cS5TQb4p",
        "outputId": "dda567a1-c782-4752-cc7a-ea981450d7ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m132.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Celda 0: Dependencias y configuración\n",
        "!pip -q install timm==1.0.9 --no-deps\n",
        "\n",
        "import os, math, time, json, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "\n",
        "import timm\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, accuracy_score, precision_score, recall_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# Rutas\n",
        "DATA_DIR = \"/content/drive/MyDrive/CognitivaAI/oas1_data\"\n",
        "CSV_TRAIN = f\"{DATA_DIR}/oas1_train_colab_mapped.csv\"\n",
        "CSV_VAL   = f\"{DATA_DIR}/oas1_val_colab_mapped.csv\"\n",
        "CSV_TEST  = f\"{DATA_DIR}/oas1_test_colab_mapped.csv\"\n",
        "OUT_DIR   = \"/content/drive/MyDrive/CognitivaAI/ft_effb3_colab\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Hiperparámetros base\n",
        "IMG_SIZE   = 300            # recomendado para EfficientNet-B3\n",
        "BATCH_SIZE = 32             # T4 friendly (ajusta a 24-40 si falta memoria)\n",
        "NUM_WORKERS= 2\n",
        "EPOCHS     = 12             # entrenamiento corto con early stopping\n",
        "BASE_LR    = 3e-4\n",
        "WD         = 1e-4\n",
        "PATIENCE   = 4              # early stopping\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1: Dataset MRI slices y DataLoaders\n",
        "\n",
        "class MRISliceDataset(Dataset):\n",
        "    def __init__(self, csv_path, transform=None):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        assert {'png_path','target','patient_id','scan_id'}.issubset(df.columns), \"CSV con columnas requeridas\"\n",
        "        self.paths = df['png_path'].astype(str).tolist()\n",
        "        self.labels = df['target'].astype(int).to_numpy()\n",
        "        self.pids = df['patient_id'].astype(str).to_numpy()\n",
        "        self.sids = df['scan_id'].astype(str).to_numpy()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.paths[idx]\n",
        "        img = Image.open(path).convert('L')  # imágenes axiales en escala de grises\n",
        "        img = img.resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR)\n",
        "        img = np.array(img, dtype=np.float32) / 255.0\n",
        "        img = np.stack([img, img, img], axis=0)  # 1->3 canales\n",
        "        if self.transform:\n",
        "            # transform de torchvision espera PIL o tensor HWC; convertimos\n",
        "            img_t = transforms.functional.to_pil_image(img.transpose(1,2,0))\n",
        "            img_t = self.transform(img_t)\n",
        "        else:\n",
        "            img_t = torch.from_numpy(img)\n",
        "        y = self.labels[idx]\n",
        "        return img_t, y, self.pids[idx], self.sids[idx], path\n",
        "\n",
        "# Transforms\n",
        "mean_std = ([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.85,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=7),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(*mean_std),\n",
        "])\n",
        "eval_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(*mean_std),\n",
        "])\n",
        "\n",
        "ds_tr = MRISliceDataset(CSV_TRAIN, transform=train_tfms)\n",
        "ds_va = MRISliceDataset(CSV_VAL,   transform=eval_tfms)\n",
        "ds_te = MRISliceDataset(CSV_TEST,  transform=eval_tfms)\n",
        "\n",
        "# Sampler balanceado por clase (opcional pero útil)\n",
        "class_counts = np.bincount(ds_tr.labels, minlength=2)\n",
        "w_neg, w_pos = 1.0/class_counts[0], 1.0/class_counts[1]\n",
        "sample_weights = np.where(ds_tr.labels==1, w_pos, w_neg)\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, sampler=sampler,  num_workers=NUM_WORKERS, pin_memory=True)\n",
        "dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False,     num_workers=NUM_WORKERS, pin_memory=True)\n",
        "dl_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False,     num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(\"TRAIN slices:\", len(ds_tr), \"| VAL:\", len(ds_va), \"| TEST:\", len(ds_te))\n",
        "print(\"Class counts train:\", class_counts, \"→ pos_weight≈\", round(class_counts[0]/max(1,class_counts[1]),3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5shRNeJxQzhg",
        "outputId": "47142603-2f92-452e-9ead-9cf933e72e7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN slices: 2820 | VAL: 940 | TEST: 940\n",
            "Class counts train: [1620 1200] → pos_weight≈ 1.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 2: Modelo EfficientNet-B3 con fine-tuning parcial\n",
        "\n",
        "BACKBONE = \"tf_efficientnet_b3_ns\"  # timm\n",
        "model = timm.create_model(BACKBONE, pretrained=True, num_classes=0, in_chans=3)\n",
        "feat_dim = model.num_features\n",
        "\n",
        "# Head ligera\n",
        "head = nn.Sequential(\n",
        "    nn.Dropout(p=0.3),\n",
        "    nn.Linear(feat_dim, 1)\n",
        ")\n",
        "\n",
        "net = nn.Sequential(model, head).to(DEVICE)\n",
        "\n",
        "# Congelar todo menos el último bloque del backbone + head\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Descongelar el último bloque de EfficientNet-B3\n",
        "# Identificamos módulos finales típicos en timm\n",
        "for name, module in model.named_modules():\n",
        "    last_block = ('blocks.6', 'blocks.7')  # por si la variante incluye más\n",
        "    if any(name.startswith(lb) for lb in last_block):\n",
        "        for p in module.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "# Head entrenable\n",
        "for p in head.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Optimizador\n",
        "trainable_params = [p for p in net.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.AdamW(trainable_params, lr=BASE_LR, weight_decay=WD)\n",
        "# Cosine schedule\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "# Pérdida con pos_weight\n",
        "pos_weight = torch.tensor([class_counts[0]/max(1,class_counts[1])], device=DEVICE, dtype=torch.float32)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "print(\"Trainable params:\", sum(p.numel() for p in net.parameters() if p.requires_grad))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "e05a7c34623e47aa887dfc9305cd1aee",
            "645a0ccce3694c17a6121115f3eaa31d",
            "067aaab3cf1345d4b601d00c6354ae07",
            "ad3a6f2234ff4f4199f686a6b16a3283",
            "bb45967aa60e40e18249b9ae6105ac10",
            "f4a2e14132ec4bf0b1f9af91be645874",
            "7fd6d6921fad499fad9922b86e761c38",
            "27c983ad5032439fbaa5639d6e13c059",
            "cdc92b7ad8c44acc89399c74681e9581",
            "66adf76c5b674eed9489c718a006bfc9",
            "a61efa444c4e4805ad970aa27acbf58c"
          ]
        },
        "id": "BKjuSfKeRFGL",
        "outputId": "2a36eaea-8f27-46ec-841e-6163887f024b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b3_ns to current tf_efficientnet_b3.ns_jft_in1k.\n",
            "  model = create_fn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e05a7c34623e47aa887dfc9305cd1aee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 3285755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 3: Entrenamiento\n",
        "\n",
        "def run_epoch(dataloader, train=True):\n",
        "    net.train(train)\n",
        "    total_loss = 0.0\n",
        "    logits_all, y_all = [], []\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "    for xb, yb, *_ in dataloader:\n",
        "        xb, yb = xb.to(DEVICE, non_blocking=True), yb.float().to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(True):\n",
        "            logits = net(xb).squeeze(1)\n",
        "            loss = criterion(logits, yb)\n",
        "        if train:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        logits_all.append(logits.detach().float().cpu().numpy())\n",
        "        y_all.append(yb.detach().float().cpu().numpy())\n",
        "    if not train:\n",
        "        with torch.no_grad():\n",
        "            pass\n",
        "    if train:\n",
        "        scheduler.step()\n",
        "    y_all = np.concatenate(y_all)\n",
        "    logits_all = np.concatenate(logits_all)\n",
        "    probs_all = 1/(1+np.exp(-logits_all))\n",
        "    # Métricas slice-level\n",
        "    auc = roc_auc_score(y_all, probs_all) if len(np.unique(y_all))>1 else np.nan\n",
        "    pr  = average_precision_score(y_all, probs_all) if len(np.unique(y_all))>1 else np.nan\n",
        "    # Thr 0.5\n",
        "    yhat = (probs_all >= 0.5).astype(int)\n",
        "    acc = accuracy_score(y_all, yhat)\n",
        "    pre = precision_score(y_all, yhat, zero_division=0)\n",
        "    rec = recall_score(y_all, yhat, zero_division=0)\n",
        "    brier = np.mean((probs_all - y_all)**2)\n",
        "    return {\n",
        "        \"loss\": float(total_loss/len(dataloader.dataset)),\n",
        "        \"auc\": float(auc) if not np.isnan(auc) else None,\n",
        "        \"pr\": float(pr) if not np.isnan(pr) else None,\n",
        "        \"acc\": float(acc),\n",
        "        \"pre\": float(pre),\n",
        "        \"rec\": float(rec),\n",
        "        \"brier\": float(brier),\n",
        "        \"probs\": probs_all.tolist(), # Convert numpy array to list\n",
        "        \"logits\": logits_all.tolist(), # Convert numpy array to list\n",
        "        \"y\": y_all.tolist() # Convert numpy array to list\n",
        "    }\n",
        "\n",
        "best_val = -np.inf\n",
        "pat = 0\n",
        "hist = []\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr = run_epoch(dl_tr, train=True)\n",
        "    va = run_epoch(dl_va, train=False)\n",
        "    # criterio: PR-AUC slice en VAL (más sensible a clase positiva)\n",
        "    score = va[\"pr\"] if va[\"pr\"] is not None else va[\"auc\"]\n",
        "    hist.append({\"epoch\":epoch, \"train\":tr, \"val\":va})\n",
        "    print(f\"[{epoch:02d}] TR loss={tr['loss']:.4f} | VAL AUC={va['auc']:.3f} PR-AUC={va['pr']:.3f} Brier={va['brier']:.3f}\")\n",
        "    if score is not None and score > best_val:\n",
        "        best_val = score\n",
        "        pat = 0\n",
        "        torch.save(net.state_dict(), f\"{OUT_DIR}/best_ft_effb3.pth\")\n",
        "    else:\n",
        "        pat += 1\n",
        "        if pat >= PATIENCE:\n",
        "            print(\"→ Early stopping.\")\n",
        "            break\n",
        "\n",
        "# Guardar historial\n",
        "with open(f\"{OUT_DIR}/train_history.json\",\"w\") as f:\n",
        "    json.dump(hist, f)\n",
        "print(\"Entrenamiento finalizado. Mejor PR-AUC VAL:\", round(best_val,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdaAPRurRM-d",
        "outputId": "6b97dff9-9566-417a-fa89-66fe3e09e7d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01] TR loss=0.7466 | VAL AUC=0.674 PR-AUC=0.558 Brier=0.237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02] TR loss=0.6407 | VAL AUC=0.652 PR-AUC=0.563 Brier=0.237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[03] TR loss=0.5669 | VAL AUC=0.666 PR-AUC=0.568 Brier=0.246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04] TR loss=0.5075 | VAL AUC=0.671 PR-AUC=0.583 Brier=0.242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05] TR loss=0.4643 | VAL AUC=0.665 PR-AUC=0.583 Brier=0.249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06] TR loss=0.4370 | VAL AUC=0.655 PR-AUC=0.577 Brier=0.257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07] TR loss=0.3698 | VAL AUC=0.667 PR-AUC=0.573 Brier=0.260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[08] TR loss=0.3477 | VAL AUC=0.666 PR-AUC=0.572 Brier=0.277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[09] TR loss=0.3292 | VAL AUC=0.661 PR-AUC=0.568 Brier=0.278\n",
            "→ Early stopping.\n",
            "Entrenamiento finalizado. Mejor PR-AUC VAL: 0.5833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 4: Inferencia + pooling paciente (mean y attention)\n",
        "\n",
        "# Cargar mejor modelo\n",
        "net.load_state_dict(torch.load(f\"{OUT_DIR}/best_ft_effb3.pth\", map_location=DEVICE))\n",
        "net.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer(dataloader):\n",
        "    logits_all, y_all, pids_all = [], [], []\n",
        "    for xb, yb, pids, *_ in dataloader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        lg = net(xb).squeeze(1)\n",
        "        logits_all.append(lg.float().cpu().numpy())\n",
        "        y_all.append(yb.numpy())\n",
        "        pids_all += list(pids)\n",
        "    logits = np.concatenate(logits_all)\n",
        "    y = np.concatenate(y_all)\n",
        "    probs = 1/(1+np.exp(-logits))\n",
        "    return logits, probs, y, np.array(pids_all)\n",
        "\n",
        "log_tr, pr_tr, y_tr, pid_tr = infer(dl_tr)\n",
        "log_va, pr_va, y_va, pid_va = infer(dl_va)\n",
        "log_te, pr_te, y_te, pid_te = infer(dl_te)\n",
        "\n",
        "def patient_pool_mean(probs, labels, pids):\n",
        "    df = pd.DataFrame({\"pid\":pids, \"y\":labels, \"p\":probs})\n",
        "    g = df.groupby(\"pid\")\n",
        "    p_pool = g[\"p\"].mean().values\n",
        "    y_pool = g[\"y\"].mean().round().astype(int).values\n",
        "    return y_pool, p_pool, g.size().values\n",
        "\n",
        "def patient_pool_attention(logits, labels, pids, temp=1.0):\n",
        "    # Atención softmax sobre |logits| como importancias (simple, estable)\n",
        "    df = pd.DataFrame({\"pid\":pids, \"y\":labels, \"z\":logits})\n",
        "    outs = []\n",
        "    for pid, grp in df.groupby(\"pid\"):\n",
        "        z = grp[\"z\"].values\n",
        "        # pesos ~ softmax(|z|/T) para resaltar slices informativos\n",
        "        w = np.exp(np.abs(z)/temp); w = w / (w.sum()+1e-8)\n",
        "        p = 1/(1+np.exp(-z))\n",
        "        p_att = (w*p).sum()\n",
        "        y = int(round(grp[\"y\"].mean()))\n",
        "        outs.append((pid, y, p_att, len(grp)))\n",
        "    outs = pd.DataFrame(outs, columns=[\"pid\",\"y\",\"p\",\"n\"])\n",
        "    return outs[\"y\"].values, outs[\"p\"].values, outs[\"n\"].values\n",
        "\n",
        "def eval_patient(y, p, thr=0.5):\n",
        "    auc = roc_auc_score(y, p) if len(np.unique(y))>1 else np.nan\n",
        "    pr  = average_precision_score(y, p) if len(np.unique(y))>1 else np.nan\n",
        "    yhat = (p>=thr).astype(int)\n",
        "    acc = accuracy_score(y, yhat)\n",
        "    pre = precision_score(y, yhat, zero_division=0)\n",
        "    rec = recall_score(y, yhat, zero_division=0)\n",
        "    return {\"AUC\":auc,\"PR-AUC\":pr,\"Acc\":acc,\"P\":pre,\"R\":rec,\"thr\":thr,\"n\":len(y)}\n",
        "\n",
        "# Mean pooling\n",
        "yV_m, pV_m, _ = patient_pool_mean(pr_va, y_va, pid_va)\n",
        "yT_m, pT_m, _ = patient_pool_mean(pr_te, y_te, pid_te)\n",
        "\n",
        "# Attention pooling\n",
        "yV_a, pV_a, _ = patient_pool_attention(log_va, y_va, pid_va, temp=1.0)\n",
        "yT_a, pT_a, _ = patient_pool_attention(log_te, y_te, pid_te, temp=1.0)\n",
        "\n",
        "print(\"VAL (mean@0.5):\", eval_patient(yV_m, pV_m, 0.5))\n",
        "print(\"TEST(mean@0.5):\", eval_patient(yT_m, pT_m, 0.5))\n",
        "print(\"VAL (attn@0.5):\", eval_patient(yV_a, pV_a, 0.5))\n",
        "print(\"TEST(attn@0.5):\", eval_patient(yT_a, pT_a, 0.5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzEP2NS9ZiRP",
        "outputId": "0e8ddd4a-f34a-487f-8418-066533f30ab8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL (mean@0.5): {'AUC': np.float64(0.7388888888888889), 'PR-AUC': np.float64(0.6587843825001534), 'Acc': 0.6382978723404256, 'P': 0.6, 'R': 0.45, 'thr': 0.5, 'n': 47}\n",
            "TEST(mean@0.5): {'AUC': np.float64(0.875925925925926), 'PR-AUC': np.float64(0.7626011139703089), 'Acc': 0.723404255319149, 'P': 0.7333333333333333, 'R': 0.55, 'thr': 0.5, 'n': 47}\n",
            "VAL (attn@0.5): {'AUC': np.float64(0.7611111111111111), 'PR-AUC': np.float64(0.6851116491294511), 'Acc': 0.6382978723404256, 'P': 0.6153846153846154, 'R': 0.4, 'thr': 0.5, 'n': 47}\n",
            "TEST(attn@0.5): {'AUC': np.float64(0.8722222222222222), 'PR-AUC': np.float64(0.764498046830885), 'Acc': 0.7659574468085106, 'P': 0.8461538461538461, 'R': 0.55, 'thr': 0.5, 'n': 47}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 5: Temperature scaling (ajuste en VAL) y evaluación paciente\n",
        "\n",
        "class TemperatureScaler(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.logT = nn.Parameter(torch.zeros(1))  # T = exp(logT) >= 1\n",
        "\n",
        "    def forward(self, logits):\n",
        "        T = torch.exp(self.logT) + 1e-6\n",
        "        return logits / T\n",
        "\n",
        "def fit_temperature(logits_val, y_val, max_iter=2000, lr=0.01):\n",
        "    y = torch.tensor(y_val, dtype=torch.float32, device=DEVICE)\n",
        "    z = torch.tensor(logits_val, dtype=torch.float32, device=DEVICE)\n",
        "    ts = TemperatureScaler().to(DEVICE)\n",
        "    opt = torch.optim.LBFGS(ts.parameters(), lr=lr, max_iter=50, line_search_fn=\"strong_wolfe\")\n",
        "\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "    def closure():\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        zT = ts(z)\n",
        "        loss = bce(zT, y)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    last = 1e9\n",
        "    for _ in range(30):\n",
        "        loss = opt.step(closure)\n",
        "        if abs(loss.item()-last) < 1e-7:\n",
        "            break\n",
        "        last = loss.item()\n",
        "    with torch.no_grad():\n",
        "        T = torch.exp(ts.logT).item() + 1e-6\n",
        "    return ts, T\n",
        "\n",
        "ts, T_val = fit_temperature(log_va, y_va)\n",
        "print(\"Temperatura ajustada (VAL):\", round(T_val,4))\n",
        "\n",
        "def apply_T(logits, T):\n",
        "    return logits / (T + 1e-6)\n",
        "\n",
        "# Aplicar T\n",
        "pV_m_T = 1/(1+np.exp(-apply_T(log_va, T_val)))\n",
        "pT_m_T = 1/(1+np.exp(-apply_T(log_te, T_val)))\n",
        "\n",
        "# Recalcular pooling\n",
        "yV_mean, pV_mean, _ = patient_pool_mean(pV_m_T, y_va, pid_va)\n",
        "yT_mean, pT_mean, _ = patient_pool_mean(pT_m_T, y_te, pid_te)\n",
        "\n",
        "yV_attn, pV_attn, _ = patient_pool_attention(apply_T(log_va, T_val), y_va, pid_va, temp=1.0)\n",
        "yT_attn, pT_attn, _ = patient_pool_attention(apply_T(log_te, T_val), y_te, pid_te, temp=1.0)\n",
        "\n",
        "print(\"VAL mean (temp):\", eval_patient(yV_mean, pV_mean, 0.5))\n",
        "print(\"TEST mean(temp):\", eval_patient(yT_mean, pT_mean, 0.5))\n",
        "print(\"VAL attn (temp):\", eval_patient(yV_attn, pV_attn, 0.5))\n",
        "print(\"TEST attn(temp):\", eval_patient(yT_attn, pT_attn, 0.5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDtnpj_4buea",
        "outputId": "9fb5e8f4-859f-40d6-f36c-af843325f40c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperatura ajustada (VAL): 2.6732\n",
            "VAL mean (temp): {'AUC': np.float64(0.7481481481481482), 'PR-AUC': np.float64(0.664989747813566), 'Acc': 0.6382978723404256, 'P': 0.6, 'R': 0.45, 'thr': 0.5, 'n': 47}\n",
            "TEST mean(temp): {'AUC': np.float64(0.8759259259259259), 'PR-AUC': np.float64(0.7620865452057403), 'Acc': 0.723404255319149, 'P': 0.7333333333333333, 'R': 0.55, 'thr': 0.5, 'n': 47}\n",
            "VAL attn (temp): {'AUC': np.float64(0.75), 'PR-AUC': np.float64(0.660088903151692), 'Acc': 0.6382978723404256, 'P': 0.6153846153846154, 'R': 0.4, 'thr': 0.5, 'n': 47}\n",
            "TEST attn(temp): {'AUC': np.float64(0.8777777777777778), 'PR-AUC': np.float64(0.7617757509275546), 'Acc': 0.723404255319149, 'P': 0.7333333333333333, 'R': 0.55, 'thr': 0.5, 'n': 47}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 6: Umbral clínico (VAL recall≥0.90) y evaluación en TEST\n",
        "\n",
        "def pick_threshold_for_recall(y, p, min_recall=0.90):\n",
        "    prec, rec, thr = precision_recall_curve(y, p)\n",
        "    # precision_recall_curve devuelve thr para todos menos el primer punto\n",
        "    thr = np.append(thr, 1.0)  # para igualar longitudes\n",
        "    # buscamos el primer punto con recall >= min_recall que maximice precisión\n",
        "    mask = (rec >= min_recall)\n",
        "    if mask.any():\n",
        "        idx = np.argmax(prec[mask])\n",
        "        thr_sel = thr[mask][idx]\n",
        "        return float(thr_sel), float(prec[mask][idx]), float(rec[mask][idx])\n",
        "    else:\n",
        "        # si no hay, devolvemos el que más recall tenga\n",
        "        idx = np.argmax(rec)\n",
        "        return float(thr[idx]), float(prec[idx]), float(rec[idx])\n",
        "\n",
        "# Elegimos el *mejor pooling* en VAL (entre mean y attn tras temperature scaling) por PR-AUC\n",
        "def pr_auc(y,p):\n",
        "    return average_precision_score(y,p) if len(np.unique(y))>1 else np.nan\n",
        "\n",
        "pr_val_mean = pr_auc(yV_mean, pV_mean)\n",
        "pr_val_attn = pr_auc(yV_attn, pV_attn)\n",
        "use_attn = (pr_val_attn > pr_val_mean)\n",
        "print(f\"Comparativa VAL PR-AUC: mean={pr_val_mean:.3f} | attn={pr_val_attn:.3f} → usar {'ATTN' if use_attn else 'MEAN'}\")\n",
        "\n",
        "if use_attn:\n",
        "    thr, prec, rec = pick_threshold_for_recall(yV_attn, pV_attn, min_recall=0.90)\n",
        "    val_metrics  = eval_patient(yV_attn, pV_attn, thr)\n",
        "    test_metrics = eval_patient(yT_attn, pT_attn, thr)\n",
        "else:\n",
        "    thr, prec, rec = pick_threshold_for_recall(yV_mean, pV_mean, min_recall=0.90)\n",
        "    val_metrics  = eval_patient(yV_mean, pV_mean, thr)\n",
        "    test_metrics = eval_patient(yT_mean, pT_mean, thr)\n",
        "\n",
        "print(f\"→ Umbral clínico (VAL recall≥0.90): thr={thr:.4f} | precision={prec:.3f} | recall={rec:.3f}\")\n",
        "print(\"[VAL-final]\", val_metrics)\n",
        "print(\"[TEST-final]\", test_metrics)\n",
        "\n",
        "# Guardar resumen\n",
        "res = {\n",
        "    \"pooling_used\": \"attention\" if use_attn else \"mean\",\n",
        "    \"temperature\": T_val,\n",
        "    \"threshold\": thr,\n",
        "    \"val_metrics\": val_metrics,\n",
        "    \"test_metrics\": test_metrics\n",
        "}\n",
        "with open(f\"{OUT_DIR}/ft_effb3_patient_eval.json\",\"w\") as f:\n",
        "    json.dump(res, f, indent=2)\n",
        "print(\"Resumen guardado en:\", f\"{OUT_DIR}/ft_effb3_patient_eval.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z45kRVu6bxKZ",
        "outputId": "8169de03-8040-4d43-dd48-806d3bd403bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparativa VAL PR-AUC: mean=0.665 | attn=0.660 → usar MEAN\n",
            "→ Umbral clínico (VAL recall≥0.90): thr=0.3651 | precision=0.588 | recall=1.000\n",
            "[VAL-final] {'AUC': np.float64(0.7481481481481482), 'PR-AUC': np.float64(0.664989747813566), 'Acc': 0.7021276595744681, 'P': 0.5882352941176471, 'R': 1.0, 'thr': 0.3651449978351593, 'n': 47}\n",
            "[TEST-final] {'AUC': np.float64(0.8759259259259259), 'PR-AUC': np.float64(0.7620865452057403), 'Acc': 0.7446808510638298, 'P': 0.625, 'R': 1.0, 'thr': 0.3651449978351593, 'n': 47}\n",
            "Resumen guardado en: /content/drive/MyDrive/CognitivaAI/ft_effb3_colab/ft_effb3_patient_eval.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# ✅ CELDA FINAL (sin predicciones): lee métricas del JSON → gráficos y reportes\n",
        "#    - Busca ft_effb3_patient_eval.json en todo el workspace.\n",
        "#    - Usa solo métricas agregadas (VAL/TEST) y umbral/temperature.\n",
        "#    - Reconstruye la matriz de confusión (TEST) a partir de Acc, P, R, n.\n",
        "#    - Genera: confusion.png, pr_point.png, bars_auc_prauc.png, metrics.txt, metrics.csv\n",
        "# ===============================================\n",
        "import os, json, glob, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- Localizar el JSON en cualquier ruta\n",
        "CANDIDATES = [\n",
        "    \"ft_effb3_colab/ft_effb3_patient_eval.json\",\n",
        "    \"ft_effb3_patient_eval.json\"\n",
        "]\n",
        "json_path = None\n",
        "for c in CANDIDATES:\n",
        "    if os.path.exists(c):\n",
        "        json_path = c\n",
        "        break\n",
        "if json_path is None:\n",
        "    hits = glob.glob(\"**/ft_effb3_patient_eval.json\", recursive=True)\n",
        "    if hits:\n",
        "        json_path = hits[0]\n",
        "\n",
        "assert json_path is not None, \"No se encontró 'ft_effb3_patient_eval.json' en el workspace.\"\n",
        "print(\"📄 Usando JSON:\", json_path)\n",
        "\n",
        "# ---- Cargar\n",
        "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    d = json.load(f)\n",
        "\n",
        "# ---- Extraer parámetros\n",
        "temperature = d.get(\"temperature\") or (d.get(\"calibration\", {}) if isinstance(d.get(\"calibration\"), dict) else {}).get(\"T\")\n",
        "threshold   = d.get(\"threshold\") or d.get(\"thr\") or d.get(\"best_thr\") or (d.get(\"val_metrics\", {}) or {}).get(\"thr\") or (d.get(\"test_metrics\", {}) or {}).get(\"thr\")\n",
        "\n",
        "val = d.get(\"val_metrics\", {})\n",
        "tes = d.get(\"test_metrics\", {})\n",
        "\n",
        "def _get(m, key, default=None):\n",
        "    return m.get(key, default)\n",
        "\n",
        "# ---- Construir DataFrame de métricas (VAL/TEST)\n",
        "rows = []\n",
        "if val:\n",
        "    rows.append({\"split\":\"VAL\",\n",
        "                 \"AUC\":_get(val,\"AUC\"),\n",
        "                 \"PR-AUC\":_get(val,\"PR-AUC\"),\n",
        "                 \"Acc\":_get(val,\"Acc\"),\n",
        "                 \"P\":_get(val,\"P\"),\n",
        "                 \"R\":_get(val,\"R\"),\n",
        "                 \"thr\":_get(val,\"thr\", threshold),\n",
        "                 \"n\":_get(val,\"n\")})\n",
        "if tes:\n",
        "    rows.append({\"split\":\"TEST\",\n",
        "                 \"AUC\":_get(tes,\"AUC\"),\n",
        "                 \"PR-AUC\":_get(tes,\"PR-AUC\"),\n",
        "                 \"Acc\":_get(tes,\"Acc\"),\n",
        "                 \"P\":_get(tes,\"P\"),\n",
        "                 \"R\":_get(tes,\"R\"),\n",
        "                 \"thr\":_get(tes,\"thr\", threshold),\n",
        "                 \"n\":_get(tes,\"n\")})\n",
        "\n",
        "metrics_df = pd.DataFrame(rows)\n",
        "OUTDIR = os.path.join(os.path.dirname(json_path) or \".\", \"graphs_from_metrics\")\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "metrics_csv = os.path.join(OUTDIR, \"ft_b3_summary_metrics.csv\")\n",
        "metrics_txt = os.path.join(OUTDIR, \"ft_b3_summary_metrics.txt\")\n",
        "metrics_df.to_csv(metrics_csv, index=False)\n",
        "\n",
        "# ---- Reconstruir matriz de confusión (TEST) con Acc, P, R, n\n",
        "# Fórmulas:\n",
        "#   Precision = TP / (TP + FP)  -> FP = TP * (1-P)/P\n",
        "#   Recall    = TP / (TP + FN)  -> FN = TP * (1-R)/R\n",
        "#   Accuracy  = (TP + TN) / n   -> TN = Acc*n - TP\n",
        "#   Suma      = TP + FP + TN + FN = n\n",
        "# Sustituyendo ->  TP * ((1-P)/P) + (Acc*n - TP) + TP * ((1-R)/R) + TP = n\n",
        "# Se despeja TP (real) y luego se redondea al entero más cercano y se ajusta TN = Acc*n - TP.\n",
        "cm_png = os.path.join(OUTDIR, \"ft_b3_patient_confusion_from_metrics.png\")\n",
        "pr_point_png = os.path.join(OUTDIR, \"ft_b3_pr_point.png\")\n",
        "bars_png = os.path.join(OUTDIR, \"ft_b3_bars_auc_prauc.png\")\n",
        "\n",
        "if tes and all(k in tes for k in [\"Acc\",\"P\",\"R\",\"n\"]):\n",
        "    Acc = float(tes[\"Acc\"])\n",
        "    P   = float(tes[\"P\"])\n",
        "    R   = float(tes[\"R\"])\n",
        "    n   = int(tes[\"n\"])\n",
        "\n",
        "    # Manejo de R=1 o P=1 para evitar divisiones por cero\n",
        "    eps = 1e-12\n",
        "    denom = ( (1.0 - P)/(P + eps) ) + ( (1.0 - R)/(R + eps) ) + 1.0  # coeficiente de TP tras agrupar términos\n",
        "    # Derivación compacta basada en: TP*((1-P)/P) + (Acc*n - TP) + TP*((1-R)/R) + TP = n  -> TP*denom + Acc*n - TP = n\n",
        "    #                                -> TP*(denom) = n*(1-Acc)\n",
        "    TP_real = n * (1.0 - Acc) / max(( (1.0 - P)/(P + eps) + (1.0 - R)/(R + eps) + 1.0 ), eps)\n",
        "\n",
        "    TP = int(round(TP_real))\n",
        "    # Recalcular con fórmulas discretas\n",
        "    FP = int(round(TP * (1.0 - P) / max(P, eps)))\n",
        "    FN = int(round(TP * (1.0 - R) / max(R, eps)))\n",
        "    TN = int(round(Acc * n)) - TP\n",
        "    # Ajuste simple por si el redondeo rompe la suma\n",
        "    delta = (TP + FP + TN + FN) - n\n",
        "    if delta != 0:\n",
        "        # Corrige en TN si hace falta\n",
        "        TN = TN - delta\n",
        "\n",
        "    print(f\"✅ Confusion TEST reconstruida: TP={TP}, FP={FP}, TN={TN}, FN={FN}  (n={n}, thr={threshold})\")\n",
        "\n",
        "    # --- Graficar matriz (sin estilos fijos)\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    cm = np.array([[TN, FP],\n",
        "                   [FN, TP]])\n",
        "    plt.figure()\n",
        "    plt.imshow(cm)\n",
        "    plt.xticks([0,1], [\"Pred 0\",\"Pred 1\"])\n",
        "    plt.yticks([0,1], [\"True 0\",\"True 1\"])\n",
        "    for (i,j),v in np.ndenumerate(cm):\n",
        "        plt.text(j, i, str(int(v)), ha=\"center\", va=\"center\")\n",
        "    plt.title(f\"Matriz de confusión (TEST) — thr={threshold}\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(cm_png, dpi=180); plt.close()\n",
        "else:\n",
        "    print(\"⚠️ No hay campos suficientes en TEST para reconstruir la matriz de confusión (se necesitan Acc,P,R,n).\")\n",
        "\n",
        "# ---- PR point (solo el punto, no la curva)\n",
        "if tes and (\"P\" in tes) and (\"R\" in tes):\n",
        "    Pp = float(tes[\"P\"]); Rr = float(tes[\"R\"])\n",
        "    plt.figure()\n",
        "    plt.plot([0,1],[Pp,Pp], linestyle=\"--\")  # línea horizontal para referencia\n",
        "    plt.plot([Rr], [Pp], marker=\"o\")\n",
        "    plt.xlim(0,1); plt.ylim(0,1)\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(\"Punto Precision–Recall (TEST)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(pr_point_png, dpi=180); plt.close()\n",
        "\n",
        "# ---- Barras de AUC y PR-AUC (VAL/TEST)\n",
        "labels = []\n",
        "auc_vals = []\n",
        "prauc_vals = []\n",
        "if val:\n",
        "    labels.append(\"VAL\"); auc_vals.append(float(val.get(\"AUC\", np.nan))); prauc_vals.append(float(val.get(\"PR-AUC\", np.nan)))\n",
        "if tes:\n",
        "    labels.append(\"TEST\"); auc_vals.append(float(tes.get(\"AUC\", np.nan))); prauc_vals.append(float(tes.get(\"PR-AUC\", np.nan)))\n",
        "\n",
        "if labels:\n",
        "    # AUC\n",
        "    plt.figure()\n",
        "    plt.bar(labels, auc_vals)\n",
        "    plt.ylim(0,1)\n",
        "    plt.ylabel(\"AUC\")\n",
        "    plt.title(\"AUC (VAL/TEST)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(bars_png.replace(\"auc_prauc\",\"auc\"), dpi=180); plt.close()\n",
        "\n",
        "    # PR-AUC\n",
        "    plt.figure()\n",
        "    plt.bar(labels, prauc_vals)\n",
        "    plt.ylim(0,1)\n",
        "    plt.ylabel(\"PR-AUC\")\n",
        "    plt.title(\"PR-AUC (VAL/TEST)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(bars_png.replace(\"auc_prauc\",\"prauc\"), dpi=180); plt.close()\n",
        "\n",
        "# ---- Guardar resumen en .txt\n",
        "with open(metrics_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(f\"Archivo: {json_path}\\n\")\n",
        "    if temperature is not None:\n",
        "        f.write(f\"Temperature: {temperature}\\n\")\n",
        "    if threshold is not None:\n",
        "        f.write(f\"Threshold: {threshold}\\n\")\n",
        "    f.write(\"\\n== VAL ==\\n\")\n",
        "    for k in [\"AUC\",\"PR-AUC\",\"Acc\",\"P\",\"R\",\"thr\",\"n\"]:\n",
        "        if k in val: f.write(f\"{k}: {val[k]}\\n\")\n",
        "    f.write(\"\\n== TEST ==\\n\")\n",
        "    for k in [\"AUC\",\"PR-AUC\",\"Acc\",\"P\",\"R\",\"thr\",\"n\"]:\n",
        "        if k in tes: f.write(f\"{k}: {tes[k]}\\n\")\n",
        "    if tes and all(k in tes for k in [\"Acc\",\"P\",\"R\",\"n\"]):\n",
        "        f.write(\"\\nMatriz de confusión (TEST) reconstruida a partir de Acc,P,R,n:\\n\")\n",
        "        f.write(f\"TP={TP} FP={FP} TN={TN} FN={FN}\\n\")\n",
        "\n",
        "print(\"📁 Resultados guardados en:\", OUTDIR)\n",
        "\n",
        "# ---- Descarga directa (si estás en Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    for fn in [metrics_csv, metrics_txt, cm_png, pr_point_png,\n",
        "               bars_png.replace(\"auc_prauc\",\"auc\"),\n",
        "               bars_png.replace(\"auc_prauc\",\"prauc\")]:\n",
        "        if os.path.exists(fn):\n",
        "            files.download(fn)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "r5Z5hPidqYwz",
        "outputId": "9b16ec9a-ac5b-45f0-dbf4-5c8b4f83ec17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Usando JSON: drive/MyDrive/CognitivaAI/ft_effb3_colab/ft_effb3_patient_eval.json\n",
            "✅ Confusion TEST reconstruida: TP=8, FP=5, TN=34, FN=0  (n=47, thr=0.3651449978351593)\n",
            "📁 Resultados guardados en: drive/MyDrive/CognitivaAI/ft_effb3_colab/graphs_from_metrics\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fecd007a-513d-4ddc-a0a5-a4c8e9f6252b\", \"ft_b3_summary_metrics.csv\", 230)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e4374011-f292-4305-94cd-f9c593bd289e\", \"ft_b3_summary_metrics.txt\", 502)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b52e7763-b791-41dd-861e-28aa12f56e31\", \"ft_b3_patient_confusion_from_metrics.png\", 29153)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_de897930-97b8-4f34-ab84-07a8d79a5de7\", \"ft_b3_pr_point.png\", 29525)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_09900ac7-3514-4898-bc5a-e1cf1c2ce3c4\", \"ft_b3_bars_auc.png\", 21496)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6fbb9118-c301-4257-96c7-d811f5065b59\", \"ft_b3_bars_prauc.png\", 23091)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MONTAJE ROBUSTO DEL DRIVE ---\n",
        "from google.colab import drive\n",
        "import os, shutil\n",
        "\n",
        "MOUNT = '/content/drive'\n",
        "\n",
        "def safe_mount():\n",
        "    try:\n",
        "        # Si la carpeta existe y tiene contenido, borrar solo enlaces residuales del colab\n",
        "        if os.path.islink(MOUNT):\n",
        "            os.unlink(MOUNT)\n",
        "        if os.path.isdir(MOUNT) and os.listdir(MOUNT):\n",
        "            # Si falla por \"ya contiene archivos\", forzamos remount\n",
        "            drive.mount(MOUNT, force_remount=True)\n",
        "        else:\n",
        "            drive.mount(MOUNT)\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Problema montando Drive. Fuerzo remount…\", e)\n",
        "        drive.mount(MOUNT, force_remount=True)\n",
        "\n",
        "safe_mount()\n",
        "!ls -lah /content/drive/MyDrive | sed -n '1,120p'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eXDVZA3aANb",
        "outputId": "13d53561-95af-447f-bac2-6d38c6805591"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "total 301M\n",
            "-rw------- 1 root root  189 Dec  9  2023 1354098917.gdoc\n",
            "-rw------- 1 root root 4.6M Dec  4  2023 1354098917.pdf\n",
            "-rw------- 1 root root  189 Aug 27  2023 202307010CV-FRM-TIC-EN.gdoc\n",
            "-rw------- 1 root root  189 Oct 15  2023 20230710CV-FRM-TIC.gdoc\n",
            "-rw------- 1 root root  189 Sep 14  2023 20230914CV-FRM-ADECCO.gdoc\n",
            "-rw------- 1 root root 220K May 21  2024 20240521DARDE.pdf\n",
            "-rw------- 1 root root 181K Dec 18  2024 24-165+REMISION+COPIAS1.pdf\n",
            "-rw------- 1 root root 181K Dec 18  2024 24-165+REMISION+COPIAS2.pdf\n",
            "-rw------- 1 root root 181K Dec 18  2024 24-165+REMISION+COPIAS3.pdf\n",
            "-rw------- 1 root root 181K Dec 18  2024 24-165+REMISION+COPIAS4.pdf\n",
            "-rw------- 1 root root 181K Dec 18  2024 24-165+REMISION+COPIAS5.pdf\n",
            "-rw------- 1 root root 181K Dec 18  2024 24-165+REMISION+COPIAS6.pdf\n",
            "-rw------- 1 root root 181K Dec 18  2024 24-165+REMISION+COPIAS7.pdf\n",
            "-rw------- 1 root root 181K Dec 18  2024 24-165+REMISION+COPIAS8.pdf\n",
            "-rw------- 1 root root  40K Apr  7  2017 AGENDA INSTITUCIONAL.doc\n",
            "-rw------- 1 root root  189 Aug 30  2018 Anexo III.A.gdoc\n",
            "-rw------- 1 root root  189 Sep  1  2018 Anexo III.E.gdoc\n",
            "-rw------- 1 root root 726K Sep 25  2016 Anexos FSE corregidos.xlsx\n",
            "-rw------- 1 root root  189 Sep 25  2016 Anexos FSE corregidos.xlsx.gsheet\n",
            "drwx------ 2 root root 4.0K May 28  2016 APUNTES\n",
            "-rw------- 1 root root 1.4K Jan  4  2017 APUNTES ASESORIA EXTERNA 2015.txt\n",
            "-rw------- 1 root root 1.3K Oct 13  2015 APUNTES INMIGRACION 2016.txt\n",
            "-rw------- 1 root root 7.0M Dec 18  2024 ARCHIVO+3+301-400.pdf\n",
            "-rw------- 1 root root 1.1M Aug 26  2022 Cancelación Registral.pdf\n",
            "-rw------- 1 root root 6.5K Apr  7  2020 Carta Nelson Gandulla.docx\n",
            "-rw------- 1 root root  13K Apr 15  2016 carta-recomendación.doc\n",
            "-rw------- 1 root root 342K May 25  2024 CasoPracticoFinal.png\n",
            "-rw------- 1 root root 6.7K May 25  2024 CasoPracticoFinal.py\n",
            "drwx------ 2 root root 4.0K Aug 22 18:29 CognitivaAI\n",
            "drwx------ 2 root root 4.0K Dec 29  2023 Colab Notebooks\n",
            "drwx------ 2 root root 4.0K Oct  8  2015 COMUNICACIÓN\n",
            "drwx------ 2 root root 4.0K Dec 16  2019 CONTRATOS\n",
            "-rw------- 1 root root 273K Sep 25  2016 Copia de ANEXOS JUSTIFICACIÓN INTERMEDIA FAMI 2015.xls\n",
            "-rw------- 1 root root  189 Sep 25  2016 Copia de ANEXOS JUSTIFICACIÓN INTERMEDIA FAMI 2015.xls.gsheet\n",
            "-rw------- 1 root root  189 Feb 20  2020 Copia de labnol's Gmail Unsubscriber\n",
            ".gsheet\n",
            "-rw------- 1 root root  189 May 19  2023 Copia de Novedades React 18.gslides\n",
            "-rw------- 1 root root 2.9M Jul 11  2024 Copia de Plantilla CV - Harvard\n",
            ".docx\n",
            "-rw------- 1 root root  272 Oct 16  2017 CUENTAS BANCARIAS COLEGA CONFEDERACION.txt\n",
            "-rw------- 1 root root 3.4K May  8 12:01 diagrama (1).txt\n",
            "-rw------- 1 root root 1.3M May  8 12:01 DiagramaArquitectura (1).png\n",
            "-rw------- 1 root root 1.3M May 27 13:24 DiagramaArquitectura.png\n",
            "-rw------- 1 root root 1.8M May 27 13:24 DiagramaArquitecturaSimple.png\n",
            "-rw------- 1 root root 3.4K May 27 13:24 diagrama.txt\n",
            "-rw------- 1 root root 3.7M May  8 12:01 Diseño de Arquitectura en la Nube para GreenFit (1).pptx\n",
            "-rw------- 1 root root  41M May  8 12:01 Diseño de Arquitectura en la Nube para GreenFit.pdf\n",
            "-rw------- 1 root root 3.7M May 27 13:24 Diseño de Arquitectura en la Nube para GreenFit.pptx\n",
            "-rw------- 1 root root  39M May 27 13:24 Diseño de Arquitectura en la Nube para GreenFitv2.pdf\n",
            "drwx------ 2 root root 4.0K Feb 29  2016 Docs Legales\n",
            "-rw------- 1 root root  189 Aug 27  2023 Documento sin título (1).gdoc\n",
            "-rw------- 1 root root  189 Aug 18  2023 Documento sin título (2).gdoc\n",
            "-rw------- 1 root root  189 Jul 20  2023 Documento sin título (3).gdoc\n",
            "-rw------- 1 root root  189 Oct 18  2024 Documento sin título.gdoc\n",
            "-rw------- 1 root root  83K Apr  8  2018 Dossier Servicio Diag Precoz.doc\n",
            "drwx------ 2 root root 4.0K Dec 17  2015 Economia y gestión\n",
            "-rw------- 1 root root  17K May 27 13:24 Ejercicios Tema 10.docx\n",
            "-rw------- 1 root root 9.0K Feb 24 16:59 Ejercicios Tema 8.docx\n",
            "drwx------ 2 root root 4.0K Jun  5  2016 Elecciones\n",
            "drwx------ 2 root root 4.0K Feb 21  2016 Encuestas\n",
            "-rw------- 1 root root  189 Jan 22  2024 Example.Learning Log: Think about data in daily life (1).gdoc\n",
            "-rw------- 1 root root  189 Jan 22  2024 Example.Learning Log: Think about data in daily life.gdoc\n",
            "drwx------ 2 root root 4.0K Dec 14  2015 Exposiciones\n",
            "drwx------ 2 root root 4.0K Dec 18  2015 Facturas\n",
            "drwx------ 2 root root 4.0K Dec 11  2017 FICHAS\n",
            "drwx------ 2 root root 4.0K Sep 17  2016 Formularios\n",
            "-rw------- 1 root root 115K May 15  2018 Foto de FRAN ELIOT RAMIREZ\n",
            "drwx------ 2 root root 4.0K Jul  7  2012 GLBT\n",
            "drwx------ 2 root root 4.0K Mar 22  2016 HACIENDA\n",
            "-rw------- 1 root root  189 May  4  2017 Hepatis A.gdoc\n",
            "-rw------- 1 root root 278K Feb 14  2018 IMG-20180214-WA0002.jpg\n",
            "-rw------- 1 root root 176K Feb 14  2018 IMG-20180214-WA0003.jpg\n",
            "-rw------- 1 root root 273K Feb 14  2018 IMG-20180214-WA0005.jpg\n",
            "-rw------- 1 root root 157K Feb 14  2018 IMG-20180214-WA0006.jpg\n",
            "-rw------- 1 root root 227K Feb 14  2018 IMG-20180214-WA0007.jpg\n",
            "-rw------- 1 root root 269K Feb 14  2018 IMG-20180214-WA0008.jpg\n",
            "-rw------- 1 root root 303K Feb 14  2018 IMG-20180214-WA0009.jpg\n",
            "-rw------- 1 root root 323K Feb 14  2018 IMG-20180214-WA0010.jpg\n",
            "-rw------- 1 root root 243K Feb 14  2018 IMG-20180214-WA0011.jpg\n",
            "-rw------- 1 root root 275K Feb 14  2018 IMG-20180214-WA0012.jpg\n",
            "-rw------- 1 root root 2.3M Apr 26  2018 IMG_20180426_193838.jpg\n",
            "-rw------- 1 root root 2.5M Apr 26  2018 IMG_20180426_202733.jpg\n",
            "-rw------- 1 root root 126K Apr 26  2018 IMG-20180426-WA0002.jpg\n",
            "-rw------- 1 root root 128K Apr 26  2018 IMG-20180426-WA0003.jpg\n",
            "-rw------- 1 root root  98K Apr 26  2018 IMG-20180426-WA0004.jpg\n",
            "-rw------- 1 root root  98K Apr 26  2018 IMG-20180426-WA0005.jpg\n",
            "-rw------- 1 root root 138K Apr 26  2018 IMG-20180426-WA0006.jpg\n",
            "-rw------- 1 root root 119K Apr 26  2018 IMG-20180426-WA0007.jpg\n",
            "-rw------- 1 root root 124K Apr 26  2018 IMG-20180426-WA0008.jpg\n",
            "-rw------- 1 root root 115K Apr 26  2018 IMG-20180426-WA0009.jpg\n",
            "-rw------- 1 root root 113K Apr 26  2018 IMG-20180426-WA0010.jpg\n",
            "-rw------- 1 root root 116K Apr 26  2018 IMG-20180426-WA0011.jpg\n",
            "-rw------- 1 root root 149K Apr 26  2018 IMG-20180426-WA0016.jpg\n",
            "-rw------- 1 root root 129K Apr 26  2018 IMG-20180426-WA0017.jpg\n",
            "-rw------- 1 root root 134K Apr 26  2018 IMG-20180426-WA0018.jpg\n",
            "-rw------- 1 root root 112K Apr 26  2018 IMG-20180426-WA0019.jpg\n",
            "-rw------- 1 root root 3.9M Feb 24 16:59 IPE - EJERCICIOS TEMA 8.pdf\n",
            "-rw------- 1 root root 8.0M Feb 24 16:59 IPE - T8 .pdf\n",
            "-rw------- 1 root root  189 Oct 29  2024 IPE - TEMA 2 EJERCICIOS 1 SOLUCIONES.gdoc\n",
            "-rw------- 1 root root  189 Oct 29  2024 IPE - TEMA 2 EJERCICIOS 2 SOLUCIONES.gdoc\n",
            "-rw------- 1 root root  189 Oct 29  2024 IPE - TEMA 2 RETO PROFESIONAL 1 SOLUCIONES.gdoc\n",
            "drwx------ 2 root root 4.0K Mar 14  2016 Justificaciones\n",
            "-rw------- 1 root root  189 Jan 22  2024 Lp5OjOQsRSeeTozkLIUnPA_77035b6752ba45fe987c1f2ef234d9f1_DAC1M1L1R2ATTACHMENT_SPA.gdoc\n",
            "-rw------- 1 root root 219K Aug  8  2019 MANDATO POLIZA BIDT024307.pdf\n",
            "-rw------- 1 root root  189 May 18  2023 MAPEO DW+TI.gsheet\n",
            "drwx------ 2 root root 4.0K Dec 17  2015 Nóminas\n",
            "drwx------ 2 root root 4.0K Sep 16  2022 PERSONAL \n",
            "-rw------- 1 root root  189 Jan 23  2014 plantilla_solicitud_2013_2014.gsheet\n",
            "drwx------ 2 root root 4.0K Feb 21  2016 PRENSA\n",
            "-rw------- 1 root root 229K Feb  1  2019 Programme-de-formation-RAVAD-3-volets-1.pdf\n",
            "-rw------- 1 root root  68K Jul 19  2019 protocolo_casetas.pdf\n",
            "-rw------- 1 root root  189 Aug  7  2019 PROTOCOLO CASETAS TIKETS.gdoc\n",
            "-rw------- 1 root root  24K May  8 12:01 PROYECTO FINAL (1).docx\n",
            "-rw------- 1 root root  24K May 27 13:24 PROYECTO FINAL.docx\n",
            "-rw------- 1 root root  89K Aug 23  2023 RECETARIO ELIOT.docx\n",
            "-rw------- 1 root root  95K Jul 19  2019 recursos_violacion.pdf\n",
            "-rw------- 1 root root 1.5M Oct 20  2017 Respeto a la diversidad.pdf\n",
            "-rw------- 1 root root 8.0M Oct 20  2017 Respeto a la diversidad.ppt\n",
            "drwx------ 2 root root 4.0K Mar 22  2016 SEGURIDAD SOCIAL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LOCALIZADOR DE RUTAS: muestra dónde están tus ficheros ---\n",
        "import glob, os\n",
        "from pathlib import Path\n",
        "\n",
        "def find(patterns, roots=('/content', '/content/drive/MyDrive')):\n",
        "    hits = []\n",
        "    for root in roots:\n",
        "        for pat in patterns:\n",
        "            hits.extend(glob.glob(os.path.join(root, '**', pat), recursive=True))\n",
        "    return sorted(set(hits))\n",
        "\n",
        "candidates = {\n",
        "    \"checkpoint (best_ft_effb3.pth)\": find(['best_ft_effb3.pth','effb3_finetuned.pth','best_model.pth']),\n",
        "    \"eval json (ft_effb3_patient_eval.json)\": find(['ft_effb3_patient_eval.json']),\n",
        "    # CSVs que comentaste:\n",
        "    \"oas1_val_colab_mapped.csv\": find(['oas1_val_colab_mapped.csv']),\n",
        "    \"oas1_test_colab_mapped.csv\": find(['oas1_test_colab_mapped.csv']),\n",
        "    # por si están con otros nombres:\n",
        "    \"oas1_val.csv\": find(['oas1_val.csv']),\n",
        "    \"oas1_test.csv\": find(['oas1_test.csv']),\n",
        "    \"val_mapped.csv\": find(['val_mapped.csv','val.csv']),\n",
        "    \"test_mapped.csv\": find(['test_mapped.csv','test.csv']),\n",
        "}\n",
        "\n",
        "for k,v in candidates.items():\n",
        "    print(f\"\\n{k}:\")\n",
        "    if v:\n",
        "        for p in v[:6]:\n",
        "            print(\"  -\", p)\n",
        "        if len(v) > 6:\n",
        "            print(f\"  (+{len(v)-6} más)\")\n",
        "    else:\n",
        "        print(\"  (no encontrado)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXLSZ66uk1sK",
        "outputId": "47a926c4-69b7-4549-adee-cdbed8db0c95"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "checkpoint (best_ft_effb3.pth):\n",
            "  - /content/drive/MyDrive/CognitivaAI/ft_effb3_colab/best_ft_effb3.pth\n",
            "\n",
            "eval json (ft_effb3_patient_eval.json):\n",
            "  - /content/drive/MyDrive/CognitivaAI/ft_effb3_colab/ft_effb3_patient_eval.json\n",
            "\n",
            "oas1_val_colab_mapped.csv:\n",
            "  - /content/drive/MyDrive/CognitivaAI/oas1_data/oas1_val_colab_mapped.csv\n",
            "\n",
            "oas1_test_colab_mapped.csv:\n",
            "  - /content/drive/MyDrive/CognitivaAI/oas1_data/oas1_test_colab_mapped.csv\n",
            "\n",
            "oas1_val.csv:\n",
            "  - /content/drive/MyDrive/CognitivaAI/oas1_data/oas1_val.csv\n",
            "\n",
            "oas1_test.csv:\n",
            "  - /content/drive/MyDrive/CognitivaAI/oas1_data/oas1_test.csv\n",
            "\n",
            "val_mapped.csv:\n",
            "  (no encontrado)\n",
            "\n",
            "test_mapped.csv:\n",
            "  (no encontrado)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "def sanity_check_dataset(df, sample=256):\n",
        "    n = len(df)\n",
        "    print(f\"Slices totales: {n}\")\n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "    from tqdm import trange\n",
        "\n",
        "    t0 = time.time()\n",
        "    for i in trange(min(sample, n)):\n",
        "        _ = Image.open(df[\"img_path\"].iloc[i]).convert(\"RGB\")\n",
        "    dt = time.time() - t0\n",
        "    ips = min(sample,n)/max(dt,1e-6)\n",
        "    print(f\"≈ {ips:.1f} imágenes/s solo lectura Drive → ETA lectura para {n} = {n/ips/60:.1f} min (aprox)\")\n",
        "\n",
        "# Llama a sanity_check_dataset(df_val); sanity_check_dataset(df_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "MaCZ_c_xoWlc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sanity_check_dataset(df_val)\n",
        "sanity_check_dataset(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JWACpJXEeqN",
        "outputId": "2b711af3-1904-472f-ffe5-acf723415412"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slices totales: 940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 256/256 [00:00<00:00, 276.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "≈ 275.5 imágenes/s solo lectura Drive → ETA lectura para 940 = 0.1 min (aprox)\n",
            "Slices totales: 940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 256/256 [00:57<00:00,  4.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "≈ 4.5 imágenes/s solo lectura Drive → ETA lectura para 940 = 3.5 min (aprox)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cacheo sólido a SSD local + DataLoader rápido (Colab) ===\n",
        "import os, shutil, hashlib, io, time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "LOCAL_ROOT = Path(\"/content/mri_cache\"); LOCAL_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _fingerprint(p: Path):\n",
        "    try:\n",
        "        st = p.stat()\n",
        "        return f\"{st.st_size}-{int(st.st_mtime)}\"\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _dst_for(src: Path):\n",
        "    # nombre estable por hash para evitar colisiones si hay mismos nombres en carpetas distintas\n",
        "    h = hashlib.md5(str(src).encode(\"utf-8\")).hexdigest()[:10]\n",
        "    return LOCAL_ROOT / f\"{h}__{src.name}\"\n",
        "\n",
        "def _copy_file(src: Path, dst: Path, bufsize=1024*1024):\n",
        "    # copia con buffer grande (más rápido que shutil.copy2 en Drive)\n",
        "    with open(src, \"rb\") as fi, open(dst, \"wb\") as fo:\n",
        "        shutil.copyfileobj(fi, fo, length=bufsize)\n",
        "    try:\n",
        "        os.utime(dst, (src.stat().st_atime, src.stat().st_mtime))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def cache_to_local(df, path_col=\"img_path\", max_workers=8):\n",
        "    paths = [Path(p) for p in df[path_col].tolist()]\n",
        "    todo = []\n",
        "    for src in paths:\n",
        "        dst = _dst_for(src)\n",
        "        if not dst.exists():\n",
        "            todo.append((src, dst))\n",
        "    print(f\"→ Preparando copia: {len(todo)} ficheros nuevos de {len(paths)} (cache en {LOCAL_ROOT})\")\n",
        "    t0 = time.time()\n",
        "    errors = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        futs = {ex.submit(_copy_file, s, d): (s, d) for s, d in todo}\n",
        "        for fut in as_completed(futs):\n",
        "            s, d = futs[fut]\n",
        "            try:\n",
        "                fut.result()\n",
        "            except Exception as e:\n",
        "                errors.append((str(s), repr(e)))\n",
        "    dt = time.time() - t0\n",
        "    if todo:\n",
        "        print(f\"✅ Copiados {len(todo)-len(errors)} / {len(todo)} en {dt:.1f}s | { (len(todo)-len(errors))/max(dt,1):.1f} f/s\")\n",
        "    if errors:\n",
        "        print(\"⚠️ Errores de copia (mostrando 5):\")\n",
        "        for e in errors[:5]: print(\"   \", e)\n",
        "    # reescribir columna\n",
        "    df = df.copy()\n",
        "    df[path_col] = [str(_dst_for(Path(p))) for p in df[path_col]]\n",
        "    return df\n",
        "\n",
        "# USO: tras construir df_val y df_test (con tus rutas reales)\n",
        "df_val  = cache_to_local(df_val)     # reescribe a /content/mri_cache/...\n",
        "df_test = cache_to_local(df_test)\n",
        "\n",
        "# Opcional: pequeño “termómetro” post‑cache\n",
        "def quick_probe(df, sample=256):\n",
        "    from PIL import Image\n",
        "    import numpy as np, time\n",
        "    n = min(sample, len(df))\n",
        "    t0 = time.time()\n",
        "    for i in range(n):\n",
        "        _ = Image.open(df[\"img_path\"].iloc[i]).convert(\"RGB\")\n",
        "    dt = time.time() - t0\n",
        "    print(f\"SSD local: {n/max(dt,1e-6):.1f} imágenes/s (muestra {n})\")\n",
        "quick_probe(df_val, 256)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUKGrwpcF27M",
        "outputId": "701b93be-3c6a-4ab1-d943-3dc58ee155a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Preparando copia: 940 ficheros nuevos de 940 (cache en /content/mri_cache)\n",
            "✅ Copiados 940 / 940 en 17.6s | 53.5 f/s\n",
            "→ Preparando copia: 940 ficheros nuevos de 940 (cache en /content/mri_cache)\n",
            "✅ Copiados 940 / 940 en 18.3s | 51.3 f/s\n",
            "SSD local: 695.3 imágenes/s (muestra 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === DataLoader estable para Colab (2 workers) + inferencia rápida ===\n",
        "import torch\n",
        "torch.backends.cudnn.benchmark = True\n",
        "try:\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Asumiendo que ya tienes df_val, df_test y la clase SliceDS definida\n",
        "val_ds  = SliceDS(df_val)\n",
        "test_ds = SliceDS(df_test)\n",
        "\n",
        "# Ajustes conservadores y estables para Colab\n",
        "val_dl  = DataLoader(\n",
        "    val_ds, batch_size=128, shuffle=False,\n",
        "    num_workers=2, pin_memory=True, prefetch_factor=2,\n",
        "    persistent_workers=True  # si diera problemas, ponlo en False\n",
        ")\n",
        "test_dl = DataLoader(\n",
        "    test_ds, batch_size=128, shuffle=False,\n",
        "    num_workers=2, pin_memory=True, prefetch_factor=2,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "model = model.to(device).eval().to(memory_format=torch.channels_last)\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_fast(dl):\n",
        "    import numpy as np, time\n",
        "    from tqdm import tqdm\n",
        "    probs, ys, pids = [], [], []\n",
        "    t0 = time.time()\n",
        "    for x, y, pid in tqdm(dl, total=len(dl)):\n",
        "        x = x.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "        with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n",
        "          logits = model(x).squeeze(1)\n",
        "        if T_temperature is not None:\n",
        "            logits = logits / float(T_temperature)\n",
        "        p = torch.sigmoid(logits).float().cpu().numpy()\n",
        "        probs.append(p); ys.append(y.numpy()); pids += list(pid)\n",
        "    dt = time.time() - t0\n",
        "    nimgs = len(dl.dataset)\n",
        "    print(f\"Throughput: {nimgs/dt:.1f} img/s  (~{dt/60:.1f} min para {nimgs} imgs)\")\n",
        "    return np.concatenate(probs), np.concatenate(ys), np.array(pids)\n",
        "\n",
        "# Ejecuta:\n",
        "# p_val, y_val, pid_val   = infer_fast(val_dl)\n",
        "# p_test, y_test, pid_test = infer_fast(test_dl)\n"
      ],
      "metadata": {
        "id": "67gxP2bCGHkO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_val, y_val, pid_val   = infer_fast(val_dl)\n",
        "p_test, y_test, pid_test = infer_fast(test_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GOZXdT-HPlG",
        "outputId": "686f35d3-2dc9-4fee-b3f9-14a0925d5565"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:04<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Throughput: 198.5 img/s  (~0.1 min para 940 imgs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:06<00:00,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Throughput: 150.4 img/s  (~0.1 min para 940 imgs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Celda final: reproducibilidad + inferencia + guardado métrico/plots ====\n",
        "import os, json, time, random, math, pathlib\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, roc_curve, confusion_matrix\n",
        "\n",
        "# --- 0) Rutas (ajústalas si fuese necesario) ---\n",
        "ROOT = Path(\"/content/drive/MyDrive/CognitivaAI\")\n",
        "OUT  = ROOT / \"ft_effb3_colab\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "GRAPHS = OUT / \"graphs_from_metrics\"\n",
        "GRAPHS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Si existe el JSON de evaluación anterior (temperatura/umbral), lo reaprovechamos\n",
        "EVAL_JSON = OUT / \"ft_effb3_patient_eval.json\"\n",
        "\n",
        "# DataFrames VAL/TEST ya usados en este notebook (ajusta nombres si difieren)\n",
        "# Se espera que tengas DataLoaders `val_dl` y `test_dl` y que entreguen (x, y, pid)\n",
        "# y también DataFrames df_val_map / df_test_map con columnas: patient_id, y_true, ...\n",
        "# Si no tienes esos DF, no pasa nada: reconstruimos por paciente con los pids de los DL.\n",
        "\n",
        "# --- 1) Semillas + backend ---\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cudnn.benchmark = True\n",
        "try: torch.set_float32_matmul_precision(\"high\")\n",
        "except: pass\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval(); model.to(device).to(memory_format=torch.channels_last)\n",
        "\n",
        "def run_infer(dl, T=None):\n",
        "    \"\"\"Inferencia rápida con medición de throughput y retorno de arrays (y_true, y_score, patient_id).\"\"\"\n",
        "    ys, ps, pids = [], [], []\n",
        "    # warm-up: 1 batch para estabilizar\n",
        "    with torch.no_grad():\n",
        "        for i, (x, y, pid) in enumerate(dl):\n",
        "            x = x.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n",
        "                logits = model(x).squeeze(1)\n",
        "            if T is not None:\n",
        "                logits = logits / float(T)\n",
        "            p = torch.sigmoid(logits).float().cpu().numpy()\n",
        "            ys.append(y.numpy()); ps.append(p); pids += list(pid)\n",
        "            break\n",
        "    # medición real\n",
        "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
        "    t0 = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (x, y, pid) in enumerate(dl):\n",
        "            x = x.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n",
        "                logits = model(x).squeeze(1)\n",
        "            if T is not None:\n",
        "                logits = logits / float(T)\n",
        "            p = torch.sigmoid(logits).float().cpu().numpy()\n",
        "            ys.append(y.numpy()); ps.append(p); pids += list(pid)\n",
        "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
        "    dt = time.time() - t0\n",
        "    n_imgs = sum(len(b) for b in ys)  # nº total de muestras\n",
        "    thrpt = n_imgs / max(dt, 1e-9)\n",
        "    return np.concatenate(ys), np.concatenate(ps), np.array(pids), thrpt\n",
        "\n",
        "def aggregate_patient(pids, ys, ps, pooling=\"mean\"):\n",
        "    \"\"\"Agrupa por patient_id con media (o max) de probabilidades y mayoritario en y_true.\"\"\"\n",
        "    df = pd.DataFrame({\"patient_id\": pids, \"y_true\": ys, \"y_score\": ps})\n",
        "    agg = (df.groupby(\"patient_id\")\n",
        "             .agg(y_true=(\"y_true\", lambda v: int(np.round(np.mean(v)))) ,\n",
        "                  y_score=(\"y_score\", np.mean if pooling==\"mean\" else np.max))\n",
        "             .reset_index())\n",
        "    return agg\n",
        "\n",
        "def eval_patient(df_pat, thr=None):\n",
        "    y = df_pat[\"y_true\"].values.astype(int)\n",
        "    s = df_pat[\"y_score\"].values.astype(float)\n",
        "    metrics = {\n",
        "        \"AUC\": float(roc_auc_score(y, s)) if len(np.unique(y))>1 else np.nan,\n",
        "        \"PR-AUC\": float(average_precision_score(y, s)),\n",
        "    }\n",
        "    if thr is None:\n",
        "        # umbral por Youden en ROC\n",
        "        fpr, tpr, t = roc_curve(y, s)\n",
        "        j = tpr - fpr\n",
        "        thr = float(t[np.argmax(j)])\n",
        "    yhat = (s >= thr).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y, yhat, labels=[0,1]).ravel()\n",
        "    P = float(tp / max(tp+fp, 1)); R = float(tp / max(tp+fn, 1))\n",
        "    acc = float((tp+tn)/max(len(y),1))\n",
        "    metrics.update({\"Acc\":acc,\"P\":P,\"R\":R,\"thr\":thr,\"n\":int(len(y))})\n",
        "    return metrics, thr\n",
        "\n",
        "# --- 2) Recuperar T y umbral previos si existen ---\n",
        "T_prev, thr_prev = None, None\n",
        "if EVAL_JSON.exists():\n",
        "    with open(EVAL_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "        j = json.load(f)\n",
        "    T_prev = float(j.get(\"temperature\")) if \"temperature\" in j else None\n",
        "    thr_prev = float(j.get(\"test_metrics\",{}).get(\"thr\") or j.get(\"val_metrics\",{}).get(\"thr\") or j.get(\"threshold\", np.nan))\n",
        "    if math.isnan(thr_prev): thr_prev=None\n",
        "    print(f\"🧪 Reutilizando T={T_prev} y thr≈{thr_prev} de {EVAL_JSON.name}\")\n",
        "\n",
        "# --- 3) Inferencia en VAL/TEST con medición de throughput ---\n",
        "# Asumimos que tienes val_dl y test_dl en memoria:\n",
        "ys_v, ps_v, pids_v, thrpt_v = run_infer(val_dl, T=T_prev)\n",
        "print(f\"[VAL] Throughput: {thrpt_v:.1f} img/s\")\n",
        "ys_t, ps_t, pids_t, thrpt_t = run_infer(test_dl, T=T_prev)\n",
        "print(f\"[TEST] Throughput: {thrpt_t:.1f} img/s\")\n",
        "\n",
        "# --- 4) Guardar CSV por slice y por paciente ---\n",
        "val_slice = pd.DataFrame({\"patient_id\":pids_v, \"y_true\":ys_v, \"y_score\":ps_v})\n",
        "test_slice= pd.DataFrame({\"patient_id\":pids_t, \"y_true\":ys_t, \"y_score\":ps_t})\n",
        "val_slice.to_csv(OUT/\"val_slice_preds.csv\", index=False)\n",
        "test_slice.to_csv(OUT/\"test_slice_preds.csv\", index=False)\n",
        "\n",
        "val_pat = aggregate_patient(pids_v, ys_v, ps_v, pooling=\"mean\")\n",
        "test_pat= aggregate_patient(pids_t, ys_t, ps_t, pooling=\"mean\")\n",
        "val_pat.to_csv(OUT/\"val_patient_preds.csv\", index=False)\n",
        "test_pat.to_csv(OUT/\"test_patient_preds.csv\", index=False)\n",
        "print(\"✅ CSV guardados en\", OUT)\n",
        "\n",
        "# --- 5) Métricas a nivel paciente (respetando umbral previo si lo había) ---\n",
        "val_m, thr_v = eval_patient(val_pat, thr=thr_prev)\n",
        "test_m, thr_t = eval_patient(test_pat, thr=thr_prev)\n",
        "print(\"VAL:\", val_m); print(\"TEST:\", test_m)\n",
        "\n",
        "# Persistimos JSON de evaluación (temperatura reutilizada si había)\n",
        "EVAL = {\n",
        "    \"pooling_used\": \"mean\",\n",
        "    \"temperature\": T_prev,\n",
        "    \"threshold\": thr_t,\n",
        "    \"val_metrics\": val_m,\n",
        "    \"test_metrics\": test_m\n",
        "}\n",
        "with open(OUT/\"ft_effb3_patient_eval.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(EVAL, f, indent=2)\n",
        "print(\"📝 Eval JSON actualizado →\", OUT/\"ft_effb3_patient_eval.json\")\n",
        "\n",
        "# --- 6) Gráficos (ROC/PR, hist, confusión) ---\n",
        "def plot_and_save_curves(df_pat, split):\n",
        "    y = df_pat[\"y_true\"].values.astype(int)\n",
        "    s = df_pat[\"y_score\"].values.astype(float)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y, s)\n",
        "    prec, rec, _ = precision_recall_curve(y, s)\n",
        "\n",
        "    plt.figure(); plt.plot(fpr, tpr); plt.plot([0,1],[0,1],'--')\n",
        "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC – {split}\")\n",
        "    plt.grid(True, alpha=.3)\n",
        "    plt.savefig(GRAPHS/f\"roc_{split.lower()}.png\", dpi=150, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "    plt.figure(); plt.plot(rec, prec)\n",
        "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR – {split}\")\n",
        "    plt.grid(True, alpha=.3)\n",
        "    plt.savefig(GRAPHS/f\"pr_{split.lower()}.png\", dpi=150, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "    plt.figure(); plt.hist(s[y==0], bins=25, alpha=.7, label=\"y=0\"); plt.hist(s[y==1], bins=25, alpha=.7, label=\"y=1\")\n",
        "    plt.legend(); plt.title(f\"Distribución probas – {split}\")\n",
        "    plt.savefig(GRAPHS/f\"histo_{split.lower()}.png\", dpi=150, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "    # Matriz de confusión con umbral óptimo del propio split\n",
        "    m, thr = eval_patient(df_pat, thr=None)\n",
        "    yhat = (s>=m[\"thr\"]).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y, yhat, labels=[0,1]).ravel()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(np.array([[tn, fp],[fn, tp]]), cmap=\"Blues\")\n",
        "    for (i,j),v in np.ndenumerate(np.array([[tn, fp],[fn, tp]])):\n",
        "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
        "    plt.xticks([0,1], [\"Pred 0\",\"Pred 1\"]); plt.yticks([0,1], [\"True 0\",\"True 1\"])\n",
        "    plt.title(f\"Confusión – {split} (thr={m['thr']:.3f})\")\n",
        "    plt.savefig(GRAPHS/f\"cm_{split.lower()}.png\", dpi=150, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "plot_and_save_curves(val_pat, \"VAL\")\n",
        "plot_and_save_curves(test_pat,\"TEST\")\n",
        "print(\"📈 Gráficos guardados en:\", GRAPHS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A83kbCt_J_RV",
        "outputId": "8e32c10f-371a-4d9a-c91c-470d14e19cd1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Reutilizando T=2.6731656060943605 y thr≈0.3651449978351593 de ft_effb3_patient_eval.json\n",
            "[VAL] Throughput: 176.7 img/s\n",
            "[TEST] Throughput: 140.0 img/s\n",
            "✅ CSV guardados en /content/drive/MyDrive/CognitivaAI/ft_effb3_colab\n",
            "VAL: {'AUC': 0.4398148148148148, 'PR-AUC': 0.39758997019708764, 'Acc': 0.425531914893617, 'P': 0.425531914893617, 'R': 1.0, 'thr': 0.3651449978351593, 'n': 47}\n",
            "TEST: {'AUC': 0.5601851851851852, 'PR-AUC': 0.45730928905033735, 'Acc': 0.425531914893617, 'P': 0.425531914893617, 'R': 1.0, 'thr': 0.3651449978351593, 'n': 47}\n",
            "📝 Eval JSON actualizado → /content/drive/MyDrive/CognitivaAI/ft_effb3_colab/ft_effb3_patient_eval.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2428139138.py:72: FutureWarning: The provided callable <function mean at 0x78c334f45c60> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
            "  .agg(y_true=(\"y_true\", lambda v: int(np.round(np.mean(v)))) ,\n",
            "/tmp/ipython-input-2428139138.py:72: FutureWarning: The provided callable <function mean at 0x78c334f45c60> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
            "  .agg(y_true=(\"y_true\", lambda v: int(np.round(np.mean(v)))) ,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📈 Gráficos guardados en: /content/drive/MyDrive/CognitivaAI/ft_effb3_colab/graphs_from_metrics\n"
          ]
        }
      ]
    }
  ]
}