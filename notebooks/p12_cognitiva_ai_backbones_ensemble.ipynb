{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFSuCBkiq3iE",
        "outputId": "e989a8a7-9d45-407e-a171-0320004647dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ðŸ“š CatÃ¡logo detectado (parejas VAL+TEST):\n",
            " - SwinTiny\n",
            "    VAL : p11_alt_backbones/SwinTiny/val_png_preds_swin.csv\n",
            "    TEST: p11_alt_backbones/SwinTiny/test_png_preds_swin.csv\n",
            " - convnext_tiny.in12k_ft_in1k_slices\n",
            "    VAL : p11_alt_backbones/convnext_tiny.in12k_ft_in1k_val_slices.csv\n",
            "    TEST: p11_alt_backbones/convnext_tiny.in12k_ft_in1k_test_slices.csv\n",
            " - png_preds_d121\n",
            "    VAL : p11_alt_backbones/val_png_preds_d121.csv\n",
            "    TEST: p11_alt_backbones/test_png_preds_d121.csv\n",
            " - patient_preds\n",
            "    VAL : ft_effb3_colab/val_patient_preds.csv\n",
            "    TEST: ft_effb3_colab/test_patient_preds.csv\n",
            " - patient_preds_ensemble\n",
            "    VAL : ft_effb3_stable_colab_plus/val_patient_preds_ensemble.csv\n",
            "    TEST: ft_effb3_stable_colab_plus/test_patient_preds_ensemble.csv\n",
            " - patient_preds_plus\n",
            "    VAL : ft_effb3_stable_colab_plus/val_patient_preds_plus.csv\n",
            "    TEST: ft_effb3_stable_colab_plus/test_patient_preds_plus.csv\n",
            " - png_preds\n",
            "    VAL : ft_effb3_stable_colab_plus/val_png_preds.csv\n",
            "    TEST: ft_effb3_stable_colab_plus/test_png_preds.csv\n",
            " - slice_preds_plus\n",
            "    VAL : ft_effb3_stable_colab_plus/val_slice_preds_plus.csv\n",
            "    TEST: ft_effb3_stable_colab_plus/test_slice_preds_plus.csv\n",
            " - slice_preds_seedENS\n",
            "    VAL : ft_effb3_stable_colab_plus/val_slice_preds_seedENS.csv\n",
            "    TEST: ft_effb3_stable_colab_plus/test_slice_preds_seedENS.csv\n",
            " - slices_preds\n",
            "    VAL : ft_effb3_colab/val_slices_preds.csv\n",
            "    TEST: ft_effb3_colab/test_slices_preds.csv\n",
            " - slice_preds\n",
            "    VAL : ft_effb3_colab/val_slice_preds.csv\n",
            "    TEST: ft_effb3_colab/test_slice_preds.csv\n",
            " - patient_eval_colab\n",
            "    VAL : oas1_resnet18_linearprobe/patient_eval_colab/val_png_preds.csv\n",
            "    TEST: oas1_resnet18_linearprobe/patient_eval_colab/test_png_preds.csv\n",
            "\n",
            "âœ… Guardado catÃ¡logo en: /content/drive/MyDrive/CognitivaAI/p11_alt_backbones/p11_backbone_catalog.json\n"
          ]
        }
      ],
      "source": [
        "# A) Descubrimiento de predicciones (slice y paciente) en varias carpetas\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, json\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/CognitivaAI\")\n",
        "OUT_DIR = BASE / \"p11_alt_backbones\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Carpetas con modelos previos que mencionaste\n",
        "CANDIDATE_DIRS = [\n",
        "    OUT_DIR,  # p11\n",
        "    BASE / \"ft_effb3_stable_colab_plus\",\n",
        "    BASE / \"ft_effb3_stable_colab\",\n",
        "    BASE / \"ft_effb3_improved_colab\",\n",
        "    BASE / \"ft_effb3_colab\",\n",
        "    BASE / \"oas1_resnet18_linearprobe\",\n",
        "    BASE / \"embeddings_new\",\n",
        "]\n",
        "\n",
        "def find_csvs(d):\n",
        "    patterns = [\n",
        "        \"*val*slice*.csv\", \"*val*_slices*.csv\", \"*val*png*pred*.csv\", \"*val*pred*.csv\",\n",
        "        \"*test*slice*.csv\",\"*test*_slices*.csv\",\"*test*png*pred*.csv\",\"*test*pred*.csv\",\n",
        "        \"val_*slice*.csv\", \"test_*slice*.csv\",\n",
        "        \"val_*patient*pred*.csv\", \"test_*patient*pred*.csv\",\n",
        "        \"val_patient_preds*.csv\", \"test_patient_preds*.csv\",\n",
        "    ]\n",
        "    all_csv = []\n",
        "    for pat in patterns:\n",
        "        all_csv += list(d.rglob(pat))\n",
        "    # Filtrado bÃ¡sico para no coger mÃ©tricas/plots\n",
        "    all_csv = [p for p in all_csv if p.is_file() and 'history' not in p.name.lower() and 'metrics' not in p.name.lower()]\n",
        "    return sorted(set(all_csv))\n",
        "\n",
        "catalog = {}\n",
        "for root in CANDIDATE_DIRS:\n",
        "    if not root.exists():\n",
        "        continue\n",
        "    csvs = find_csvs(root)\n",
        "    if not csvs:\n",
        "        continue\n",
        "    # Pairing por nombre para val/test\n",
        "    by_key = {}\n",
        "    for p in csvs:\n",
        "        low = p.name.lower()\n",
        "        key = None\n",
        "        if \"val\" in low:\n",
        "            key = (\"VAL\", p)\n",
        "        elif \"test\" in low:\n",
        "            key = (\"TEST\", p)\n",
        "        if key:\n",
        "            base_key = re.sub(r\"(val|test)\", \"\", low)\n",
        "            by_key.setdefault(base_key, {\"VAL\": None, \"TEST\": None})\n",
        "            by_key[base_key][key[0]] = p\n",
        "\n",
        "    for _, pair in by_key.items():\n",
        "        if pair[\"VAL\"] is None or pair[\"TEST\"] is None:\n",
        "            continue\n",
        "        # Etiqueta backbone a partir de carpeta/archivo\n",
        "        tag = pair[\"VAL\"].parent.name\n",
        "        if tag.lower() in [\"p11_alt_backbones\", \"ft_effb3_stable_colab_plus\", \"ft_effb3_stable_colab\",\n",
        "                           \"ft_effb3_improved_colab\", \"ft_effb3_colab\", \"oas1_resnet18_linearprobe\",\n",
        "                           \"embeddings_new\"]:\n",
        "            # usa archivo para nombrar si la carpeta es poco informativa\n",
        "            tag = pair[\"VAL\"].stem.replace(\"val_\", \"\").replace(\"_val\", \"\")\n",
        "        catalog[tag] = {\"VAL\": pair[\"VAL\"], \"TEST\": pair[\"TEST\"]}\n",
        "\n",
        "print(\"ðŸ“š CatÃ¡logo detectado (parejas VAL+TEST):\")\n",
        "for k,v in catalog.items():\n",
        "    print(f\" - {k}\\n    VAL : {v['VAL'].relative_to(BASE)}\\n    TEST: {v['TEST'].relative_to(BASE)}\")\n",
        "\n",
        "with open(OUT_DIR / \"p11_backbone_catalog.json\", \"w\") as f:\n",
        "    json.dump({k: {\"VAL\": str(v[\"VAL\"]), \"TEST\": str(v[\"TEST\"])} for k,v in catalog.items()}, f, indent=2)\n",
        "\n",
        "print(f\"\\nâœ… Guardado catÃ¡logo en: {OUT_DIR/'p11_backbone_catalog.json'}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== B-fix: lector robusto para estandarizar columnas y evitar KeyError ====\n",
        "import json, re, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/CognitivaAI\")\n",
        "OUT_DIR  = BASE_DIR / \"p11_alt_backbones\"\n",
        "CATALOG_JSON = OUT_DIR / \"p11_backbone_catalog.json\"\n",
        "CONFIG_JSON  = OUT_DIR / \"p11_config.json\"   # creado al inicio de P11\n",
        "\n",
        "def safe_sigmoid(z):\n",
        "    z = np.clip(z, -50, 50)\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "# Cargar config para mapas (si existen)\n",
        "png2pid_map = {}\n",
        "if CONFIG_JSON.exists():\n",
        "    cfg = json.loads(CONFIG_JSON.read_text())\n",
        "    try:\n",
        "        val_map_path  = Path(cfg[\"VAL_MAP\"])\n",
        "        test_map_path = Path(cfg[\"TEST_MAP\"])\n",
        "        if val_map_path.exists():\n",
        "            dfv = pd.read_csv(val_map_path)\n",
        "            png2pid_map.update(dict(zip(dfv[\"png_path\"], dfv[\"patient_id\"])))\n",
        "        if test_map_path.exists():\n",
        "            dft = pd.read_csv(test_map_path)\n",
        "            png2pid_map.update(dict(zip(dft[\"png_path\"], dft[\"patient_id\"])))\n",
        "        print(f\"ðŸ”— Mapas cargados: {len(png2pid_map):,} rutas â†’ patient_id\")\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ No pude cargar mapas desde config:\", e)\n",
        "\n",
        "_pid_regex = re.compile(r\"(OAS1_\\d{4})\")\n",
        "\n",
        "def derive_patient_id_from_path(p):\n",
        "    if p in png2pid_map:\n",
        "        return png2pid_map[p]\n",
        "    m = _pid_regex.search(str(p))\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "def standardize_preds(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # normalizar nombres\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    lower = {c.lower(): c for c in df.columns}\n",
        "\n",
        "    # localizar id\n",
        "    id_col = None\n",
        "    for cand in [\"patient_id\",\"png_path\",\"path\",\"img_path\",\"image_path\",\"fname\",\"file\"]:\n",
        "        if cand in lower:\n",
        "            id_col = lower[cand]; break\n",
        "    if id_col is None:\n",
        "        raise KeyError(f\"âŒ No encuentro columna de id en {csv_path.name}. Vienen: {list(df.columns)}\")\n",
        "\n",
        "    # localizar y_true\n",
        "    ytrue_col = None\n",
        "    for cand in [\"y_true\",\"target\",\"label\",\"y\",\"gt\",\"truth\"]:\n",
        "        if cand in lower:\n",
        "            ytrue_col = lower[cand]; break\n",
        "    if ytrue_col is None and \"y_pred\" in lower:\n",
        "        ytrue_col = lower[\"y_pred\"]  # si solo hay 0/1 predicho\n",
        "    if ytrue_col is None:\n",
        "        raise KeyError(f\"âŒ No encuentro y_true/target en {csv_path.name}. Vienen: {list(df.columns)}\")\n",
        "\n",
        "    # localizar score o logit\n",
        "    yscore_col = None\n",
        "    for cand in [\"y_score\",\"score\",\"y_prob\",\"prob\",\"proba\",\"p\",\"prediction\",\"pred\",\"sigmoid\"]:\n",
        "        if cand in lower:\n",
        "            yscore_col = lower[cand]; break\n",
        "\n",
        "    if yscore_col is None:\n",
        "        # intentar con logit\n",
        "        logit_col = None\n",
        "        for cand in [\"logit\",\"logits\",\"y_logit\"]:\n",
        "            if cand in lower:\n",
        "                logit_col = lower[cand]; break\n",
        "        if logit_col is None:\n",
        "            # Ãºltimo recurso: usar y_pred como prob\n",
        "            if \"y_pred\" in lower:\n",
        "                yscore = df[lower[\"y_pred\"]].astype(float).clip(0,1).values\n",
        "                src = \"y_pred\"\n",
        "            else:\n",
        "                raise KeyError(f\"âŒ No encuentro y_score/score/prob ni logit en {csv_path.name}. Vienen: {list(df.columns)}\")\n",
        "        else:\n",
        "            yscore = safe_sigmoid(df[logit_col].astype(float).values)\n",
        "            src = f\"sigmoid({logit_col})\"\n",
        "    else:\n",
        "        yscore = df[yscore_col].astype(float).values\n",
        "        src = yscore_col\n",
        "\n",
        "    # construir DataFrame estÃ¡ndar\n",
        "    out = pd.DataFrame({\n",
        "        \"src_path\": csv_path.as_posix(),\n",
        "        \"id_raw\": df[id_col].astype(str),\n",
        "        \"y_true\": pd.to_numeric(df[ytrue_col], errors=\"coerce\").fillna(0).astype(int),\n",
        "        \"y_score\": yscore\n",
        "    })\n",
        "\n",
        "    # asegurar patient_id\n",
        "    if id_col.lower() == \"patient_id\":\n",
        "        out[\"patient_id\"] = out[\"id_raw\"]\n",
        "    else:\n",
        "        out[\"patient_id\"] = out[\"id_raw\"].map(derive_patient_id_from_path)\n",
        "\n",
        "    missing = out[\"patient_id\"].isna().sum()\n",
        "    if missing:\n",
        "        print(f\"âš ï¸ {missing} filas sin patient_id derivable en {csv_path.name}. IntentarÃ© continuar; esas filas se omitirÃ¡n en pooling por paciente.\")\n",
        "        out = out.dropna(subset=[\"patient_id\"])\n",
        "\n",
        "    print(f\"âœ… {csv_path.name}: cols OK (id='{id_col}', y_true='{ytrue_col}', score='{src}'), filas={len(out)}\")\n",
        "    return out[[\"patient_id\",\"y_true\",\"y_score\"]].reset_index(drop=True)\n",
        "\n",
        "# --- Cargar catÃ¡logo y leer TODOS los pares ---\n",
        "catalog = json.loads(Path(CATALOG_JSON).read_text())\n",
        "feature_tables = {}\n",
        "for tag, paths in catalog.items():\n",
        "    try:\n",
        "        val_df  = standardize_preds(BASE_DIR / paths[\"VAL\"])\n",
        "        test_df = standardize_preds(BASE_DIR / paths[\"TEST\"])\n",
        "        feature_tables[tag] = {\"VAL\": val_df, \"TEST\": test_df}\n",
        "    except Exception as e:\n",
        "        print(f\"â›” Saltando '{tag}' por error: {e}\")\n",
        "\n",
        "print(\"\\nðŸ§¾ Fuentes cargadas:\", list(feature_tables.keys()))\n",
        "assert feature_tables, \"No se cargÃ³ ninguna fuente vÃ¡lida. Revisa mensajes arriba.\"\n",
        "\n",
        "# (opcional) ejemplo de inspecciÃ³n\n",
        "for tag, splits in feature_tables.items():\n",
        "    for sp, df in splits.items():\n",
        "        print(f\"â€¢ {tag} [{sp}] -> {df.shape}, cols={list(df.columns)[:3]}\")\n",
        "    break  # muestra solo la primera para no saturar salida\n",
        "\n",
        "# Deja 'feature_tables' en memoria para la Celda C (pooling y ensembles).\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8ZChySY_whW",
        "outputId": "72ae2880-a28c-48b1-ae7b-7969da6e28eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”— Mapas cargados: 1,880 rutas â†’ patient_id\n",
            "âœ… val_png_preds_swin.csv: cols OK (id='patient_id', y_true='y_true', score='sigmoid(logit)'), filas=940\n",
            "âœ… test_png_preds_swin.csv: cols OK (id='patient_id', y_true='y_true', score='sigmoid(logit)'), filas=940\n",
            "âœ… convnext_tiny.in12k_ft_in1k_val_slices.csv: cols OK (id='patient_id', y_true='y_true', score='y_score'), filas=940\n",
            "âœ… convnext_tiny.in12k_ft_in1k_test_slices.csv: cols OK (id='patient_id', y_true='y_true', score='y_score'), filas=940\n",
            "âœ… val_png_preds_d121.csv: cols OK (id='patient_id', y_true='y_true', score='y_score'), filas=940\n",
            "âœ… test_png_preds_d121.csv: cols OK (id='patient_id', y_true='y_true', score='y_score'), filas=940\n",
            "âœ… val_patient_preds.csv: cols OK (id='patient_id', y_true='y_true', score='y_score'), filas=10\n",
            "âœ… test_patient_preds.csv: cols OK (id='patient_id', y_true='y_true', score='y_score'), filas=47\n",
            "âœ… val_patient_preds_ensemble.csv: cols OK (id='patient_id', y_true='y_true', score='y_score'), filas=10\n",
            "âœ… test_patient_preds_ensemble.csv: cols OK (id='patient_id', y_true='y_true', score='y_score'), filas=47\n",
            "âœ… val_patient_preds_plus.csv: cols OK (id='patient_id', y_true='y_true', score='y_score'), filas=47\n",
            "âœ… test_patient_preds_plus.csv: cols OK (id='patient_id', y_true='y_true', score='y_score'), filas=47\n",
            "âœ… val_png_preds.csv: cols OK (id='patient_id', y_true='y_true', score='sigmoid(logits)'), filas=940\n",
            "âœ… test_png_preds.csv: cols OK (id='patient_id', y_true='y_true', score='sigmoid(logits)'), filas=940\n",
            "âœ… val_slice_preds_plus.csv: cols OK (id='patient_id', y_true='y_true', score='y_score'), filas=940\n",
            "âœ… test_slice_preds_plus.csv: cols OK (id='patient_id', y_true='y_true', score='y_score'), filas=940\n",
            "âœ… val_slice_preds_seedENS.csv: cols OK (id='patient_id', y_true='y_true', score='sigmoid(logit)'), filas=940\n",
            "âœ… test_slice_preds_seedENS.csv: cols OK (id='patient_id', y_true='y_true', score='sigmoid(logit)'), filas=940\n",
            "âœ… val_slices_preds.csv: cols OK (id='patient_id', y_true='y_true', score='y_score'), filas=200\n",
            "âœ… test_slices_preds.csv: cols OK (id='patient_id', y_true='y_true', score='y_score'), filas=940\n",
            "âœ… val_slice_preds.csv: cols OK (id='patient_id', y_true='y_true', score='y_score'), filas=1068\n",
            "âœ… test_slice_preds.csv: cols OK (id='patient_id', y_true='y_true', score='y_score'), filas=1068\n",
            "âœ… val_png_preds.csv: cols OK (id='patient_id', y_true='target', score='pred'), filas=940\n",
            "âœ… test_png_preds.csv: cols OK (id='patient_id', y_true='target', score='pred'), filas=940\n",
            "\n",
            "ðŸ§¾ Fuentes cargadas: ['SwinTiny', 'convnext_tiny.in12k_ft_in1k_slices', 'png_preds_d121', 'patient_preds', 'patient_preds_ensemble', 'patient_preds_plus', 'png_preds', 'slice_preds_plus', 'slice_preds_seedENS', 'slices_preds', 'slice_preds', 'patient_eval_colab']\n",
            "â€¢ SwinTiny [VAL] -> (940, 3), cols=['patient_id', 'y_true', 'y_score']\n",
            "â€¢ SwinTiny [TEST] -> (940, 3), cols=['patient_id', 'y_true', 'y_score']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Celda C-fix: GeneraciÃ³n de features paciente a partir de predicciones slice-level ===\n",
        "from pathlib import Path\n",
        "import json, numpy as np, pandas as pd\n",
        "\n",
        "# Montaje Drive (idempotente)\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception as e:\n",
        "    print(\"Drive ya montado.\")\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/CognitivaAI\")\n",
        "OUT_DIR = BASE / \"p11_alt_backbones\"\n",
        "CAT_PATH = OUT_DIR / \"p11_backbone_catalog.json\"\n",
        "\n",
        "assert CAT_PATH.exists(), f\"No encuentro catÃ¡logo: {CAT_PATH}\"\n",
        "with open(CAT_PATH, \"r\") as f:\n",
        "    catalog = json.load(f)\n",
        "\n",
        "def safe_sigmoid(z):\n",
        "    z = np.clip(z, -50, 50)\n",
        "    return 1.0/(1.0 + np.exp(-z))\n",
        "\n",
        "def read_and_normalize(csv_path: Path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    # Intentar mapear a ['patient_id','y_true','y_score']\n",
        "    if 'patient_id' not in cols and 'id' in cols:  # por si acaso\n",
        "        cols['patient_id'] = cols.pop('id')\n",
        "    pid_col = cols.get('patient_id', None)\n",
        "    # verdad terreno\n",
        "    ytrue_col = cols.get('y_true', cols.get('target', None))\n",
        "    # score: preferir y_score/pred, o derivar de logit/logits\n",
        "    if 'y_score' in cols:\n",
        "        score_col = cols['y_score']\n",
        "        score = df[score_col].astype(float)\n",
        "    elif 'pred' in cols:\n",
        "        score_col = cols['pred']\n",
        "        score = df[score_col].astype(float)\n",
        "    elif 'logit' in cols:\n",
        "        score = safe_sigmoid(df[cols['logit']].astype(float))\n",
        "    elif 'logits' in cols:\n",
        "        score = safe_sigmoid(df[cols['logits']].astype(float))\n",
        "    elif 'sigmoid(logit)' in cols:\n",
        "        score = df[cols['sigmoid(logit)']].astype(float)\n",
        "    elif 'sigmoid(logits)' in cols:\n",
        "        score = df[cols['sigmoid(logits)']].astype(float)\n",
        "    else:\n",
        "        raise ValueError(f\"No localizo columna de score en {csv_path.name}. Columnas={df.columns.tolist()}\")\n",
        "\n",
        "    assert pid_col is not None and ytrue_col is not None, f\"Faltan columnas clave en {csv_path.name}\"\n",
        "    df_norm = pd.DataFrame({\n",
        "        'patient_id': df[pid_col].astype(str),\n",
        "        'y_true': df[ytrue_col].astype(int),\n",
        "        'y_score': score.astype(float)\n",
        "    })\n",
        "    return df_norm\n",
        "\n",
        "def trimmed_mean(values, trim=0.2):\n",
        "    x = np.sort(np.asarray(values, float))\n",
        "    n = len(x)\n",
        "    k = int(np.floor(trim * n))\n",
        "    if 2*k >= n:\n",
        "        return float(x.mean())\n",
        "    return float(x[k:n-k].mean())\n",
        "\n",
        "def topk_mean(values, k=7):\n",
        "    x = np.sort(np.asarray(values, float))\n",
        "    k = int(min(k, len(x)))\n",
        "    if k <= 0:\n",
        "        return float(np.mean(x)) if len(x) else np.nan\n",
        "    return float(np.mean(x[-k:]))\n",
        "\n",
        "def power_mean(values, p=2.0):\n",
        "    x = np.asarray(values, float)\n",
        "    if len(x) == 0:\n",
        "        return np.nan\n",
        "    x = np.clip(x, 1e-8, 1 - 1e-8)  # evitar extremos degenerados\n",
        "    if p == 0:\n",
        "        return float(np.exp(np.mean(np.log(x))))\n",
        "    return float((np.mean(x**p))**(1.0/p))\n",
        "\n",
        "def build_patient_features(df_norm: pd.DataFrame, tag: str):\n",
        "    # agrega por paciente\n",
        "    g = df_norm.groupby(['patient_id', 'y_true'])['y_score'].apply(list).reset_index()\n",
        "    feats = []\n",
        "    for _, row in g.iterrows():\n",
        "        scores = row['y_score']\n",
        "        feats.append({\n",
        "            'patient_id': row['patient_id'],\n",
        "            'y_true': int(row['y_true']),\n",
        "            f'{tag}_mean': float(np.mean(scores)),\n",
        "            f'{tag}_trimmed20': trimmed_mean(scores, trim=0.2),\n",
        "            f'{tag}_top7': topk_mean(scores, k=7),\n",
        "            f'{tag}_p2': power_mean(scores, p=2.0),\n",
        "        })\n",
        "    return pd.DataFrame(feats)\n",
        "\n",
        "VAL_parts, TEST_parts = [], []\n",
        "\n",
        "# Releer cada par VAL/TEST del catÃ¡logo y construir features\n",
        "for tag, paths in catalog.items():\n",
        "    val_path = BASE / paths['VAL']\n",
        "    test_path = BASE / paths['TEST']\n",
        "    # solo incluir fuentes slice-level (evitar ya-agregados a nivel paciente si no son comparables)\n",
        "    # HeurÃ­stica: admitir siempre; si ya vienen por paciente (nâ‰ˆ47) igual se ignoran en merge\n",
        "    val_df = read_and_normalize(val_path)\n",
        "    test_df = read_and_normalize(test_path)\n",
        "\n",
        "    # Solo continuar si parecen slice-level (>>47 filas) o si son 47 pero con per-slice ya agregados por patient\n",
        "    val_feats = build_patient_features(val_df, tag=tag)\n",
        "    test_feats = build_patient_features(test_df, tag=tag)\n",
        "    VAL_parts.append(val_feats)\n",
        "    TEST_parts.append(test_feats)\n",
        "    print(f\"âœ… {tag}: VAL {val_feats.shape} | TEST {test_feats.shape}\")\n",
        "\n",
        "# Merge incremental por patient_id + y_true\n",
        "def merge_all(parts):\n",
        "    out = parts[0].copy()\n",
        "    for t in parts[1:]:\n",
        "        out = out.merge(t, on=['patient_id','y_true'], how='inner')\n",
        "    return out\n",
        "\n",
        "VAL = merge_all(VAL_parts)\n",
        "TEST = merge_all(TEST_parts)\n",
        "\n",
        "# Guardar\n",
        "val_out = OUT_DIR / \"val_patient_features_backbones.csv\"\n",
        "test_out = OUT_DIR / \"test_patient_features_backbones.csv\"\n",
        "VAL.to_csv(val_out, index=False)\n",
        "TEST.to_csv(test_out, index=False)\n",
        "\n",
        "print(f\"\\nðŸ’¾ Guardado:\\n- {val_out}\\n- {test_out}\")\n",
        "print(\"VAL cols:\", VAL.columns.tolist())\n",
        "print(\"TEST cols:\", TEST.columns.tolist())\n",
        "print(\"VAL shape:\", VAL.shape, \"| TEST shape:\", TEST.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b8JXKtvByRx",
        "outputId": "f20ac603-6608-4117-892e-3917a5cf1b5e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… SwinTiny: VAL (47, 6) | TEST (47, 6)\n",
            "âœ… convnext_tiny.in12k_ft_in1k_slices: VAL (47, 6) | TEST (47, 6)\n",
            "âœ… png_preds_d121: VAL (47, 6) | TEST (47, 6)\n",
            "âœ… patient_preds: VAL (10, 6) | TEST (47, 6)\n",
            "âœ… patient_preds_ensemble: VAL (10, 6) | TEST (47, 6)\n",
            "âœ… patient_preds_plus: VAL (47, 6) | TEST (47, 6)\n",
            "âœ… png_preds: VAL (47, 6) | TEST (47, 6)\n",
            "âœ… slice_preds_plus: VAL (47, 6) | TEST (47, 6)\n",
            "âœ… slice_preds_seedENS: VAL (47, 6) | TEST (47, 6)\n",
            "âœ… slices_preds: VAL (10, 6) | TEST (47, 6)\n",
            "âœ… slice_preds: VAL (47, 6) | TEST (47, 6)\n",
            "âœ… patient_eval_colab: VAL (47, 6) | TEST (47, 6)\n",
            "\n",
            "ðŸ’¾ Guardado:\n",
            "- /content/drive/MyDrive/CognitivaAI/p11_alt_backbones/val_patient_features_backbones.csv\n",
            "- /content/drive/MyDrive/CognitivaAI/p11_alt_backbones/test_patient_features_backbones.csv\n",
            "VAL cols: ['patient_id', 'y_true', 'SwinTiny_mean', 'SwinTiny_trimmed20', 'SwinTiny_top7', 'SwinTiny_p2', 'convnext_tiny.in12k_ft_in1k_slices_mean', 'convnext_tiny.in12k_ft_in1k_slices_trimmed20', 'convnext_tiny.in12k_ft_in1k_slices_top7', 'convnext_tiny.in12k_ft_in1k_slices_p2', 'png_preds_d121_mean', 'png_preds_d121_trimmed20', 'png_preds_d121_top7', 'png_preds_d121_p2', 'patient_preds_mean', 'patient_preds_trimmed20', 'patient_preds_top7', 'patient_preds_p2', 'patient_preds_ensemble_mean', 'patient_preds_ensemble_trimmed20', 'patient_preds_ensemble_top7', 'patient_preds_ensemble_p2', 'patient_preds_plus_mean', 'patient_preds_plus_trimmed20', 'patient_preds_plus_top7', 'patient_preds_plus_p2', 'png_preds_mean', 'png_preds_trimmed20', 'png_preds_top7', 'png_preds_p2', 'slice_preds_plus_mean', 'slice_preds_plus_trimmed20', 'slice_preds_plus_top7', 'slice_preds_plus_p2', 'slice_preds_seedENS_mean', 'slice_preds_seedENS_trimmed20', 'slice_preds_seedENS_top7', 'slice_preds_seedENS_p2', 'slices_preds_mean', 'slices_preds_trimmed20', 'slices_preds_top7', 'slices_preds_p2', 'slice_preds_mean', 'slice_preds_trimmed20', 'slice_preds_top7', 'slice_preds_p2', 'patient_eval_colab_mean', 'patient_eval_colab_trimmed20', 'patient_eval_colab_top7', 'patient_eval_colab_p2']\n",
            "TEST cols: ['patient_id', 'y_true', 'SwinTiny_mean', 'SwinTiny_trimmed20', 'SwinTiny_top7', 'SwinTiny_p2', 'convnext_tiny.in12k_ft_in1k_slices_mean', 'convnext_tiny.in12k_ft_in1k_slices_trimmed20', 'convnext_tiny.in12k_ft_in1k_slices_top7', 'convnext_tiny.in12k_ft_in1k_slices_p2', 'png_preds_d121_mean', 'png_preds_d121_trimmed20', 'png_preds_d121_top7', 'png_preds_d121_p2', 'patient_preds_mean', 'patient_preds_trimmed20', 'patient_preds_top7', 'patient_preds_p2', 'patient_preds_ensemble_mean', 'patient_preds_ensemble_trimmed20', 'patient_preds_ensemble_top7', 'patient_preds_ensemble_p2', 'patient_preds_plus_mean', 'patient_preds_plus_trimmed20', 'patient_preds_plus_top7', 'patient_preds_plus_p2', 'png_preds_mean', 'png_preds_trimmed20', 'png_preds_top7', 'png_preds_p2', 'slice_preds_plus_mean', 'slice_preds_plus_trimmed20', 'slice_preds_plus_top7', 'slice_preds_plus_p2', 'slice_preds_seedENS_mean', 'slice_preds_seedENS_trimmed20', 'slice_preds_seedENS_top7', 'slice_preds_seedENS_p2', 'slices_preds_mean', 'slices_preds_trimmed20', 'slices_preds_top7', 'slices_preds_p2', 'slice_preds_mean', 'slice_preds_trimmed20', 'slice_preds_top7', 'slice_preds_p2', 'patient_eval_colab_mean', 'patient_eval_colab_trimmed20', 'patient_eval_colab_top7', 'patient_eval_colab_p2']\n",
            "VAL shape: (10, 50) | TEST shape: (47, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Celda D: Ensemble de backbones (stacking LR + baseline promedio) ===\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_fscore_support\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/CognitivaAI\")\n",
        "OUT_DIR = BASE / \"p11_alt_backbones\"\n",
        "\n",
        "VAL = pd.read_csv(OUT_DIR/\"val_patient_features_backbones.csv\")\n",
        "TEST = pd.read_csv(OUT_DIR/\"test_patient_features_backbones.csv\")\n",
        "\n",
        "# Separar X,y (excluimos identificadores)\n",
        "id_cols = ['patient_id','y_true']\n",
        "feat_cols = [c for c in VAL.columns if c not in id_cols]\n",
        "\n",
        "X_val, y_val = VAL[feat_cols].values, VAL['y_true'].values\n",
        "X_tst, y_tst = TEST[feat_cols].values, TEST['y_true'].values\n",
        "\n",
        "def sweep_best_f1(y_true, scores):\n",
        "    best = (0.0, 0.0)  # f1, thr\n",
        "    for thr in np.linspace(0,1,2001):\n",
        "        y_hat = (scores >= thr).astype(int)\n",
        "        P, R, F1, _ = precision_recall_fscore_support(y_true, y_hat, average='binary', zero_division=0)\n",
        "        if F1 > best[0]:\n",
        "            best = (F1, thr, P, R)\n",
        "    return {'F1':best[0], 'thr':best[1], 'P':best[2], 'R':best[3]}\n",
        "\n",
        "def metrics_at(y_true, scores, thr):\n",
        "    y_hat = (scores >= thr).astype(int)\n",
        "    P, R, F1, _ = precision_recall_fscore_support(y_true, y_hat, average='binary', zero_division=0)\n",
        "    Acc = np.mean(y_hat == y_true)\n",
        "    return dict(\n",
        "        AUC=float(roc_auc_score(y_true, scores)),\n",
        "        PRAUC=float(average_precision_score(y_true, scores)),\n",
        "        Acc=float(Acc), P=float(P), R=float(R), thr=float(thr), n=int(len(y_true))\n",
        "    )\n",
        "\n",
        "def thr_for_recall(y_true, scores, target=1.0):\n",
        "    # umbral mÃ­nimo que alcanza recall>=target (si no se alcanza, retorna 0.0)\n",
        "    best_thr = 0.0\n",
        "    grid = np.linspace(0,1,2001)\n",
        "    for thr in grid:\n",
        "        y_hat = (scores >= thr).astype(int)\n",
        "        P, R, _, _ = precision_recall_fscore_support(y_true, y_hat, average='binary', zero_division=0)\n",
        "        if R >= target:\n",
        "            best_thr = thr\n",
        "            break\n",
        "    return float(best_thr)\n",
        "\n",
        "def summarize(tag, yv, sv, yt, st):\n",
        "    f1v = sweep_best_f1(yv, sv)\n",
        "    mv  = metrics_at(yv, sv, f1v['thr'])\n",
        "    mt  = metrics_at(yt, st, f1v['thr'])\n",
        "    rec90_thr = thr_for_recall(yv, sv, target=0.90)\n",
        "    rec100_thr= thr_for_recall(yv, sv, target=1.00)\n",
        "    mv90 = metrics_at(yv, sv, rec90_thr); mt90 = metrics_at(yt, st, rec90_thr)\n",
        "    mv100= metrics_at(yv, sv, rec100_thr); mt100= metrics_at(yt, st, rec100_thr)\n",
        "    print(f\"\\n[{tag}]\")\n",
        "    print(\"VAL F1-opt:\", mv)\n",
        "    print(\"TEST F1-opt:\", mt)\n",
        "    print(\"VAL@REC90:\", mv90, \"\\nTEST@REC90:\", mt90)\n",
        "    print(\"VAL@REC100:\", mv100, \"\\nTEST@REC100:\", mt100)\n",
        "    return {\n",
        "        'variant': tag,\n",
        "        'VAL_F1': mv, 'TEST_F1': mt,\n",
        "        'VAL_REC90': mv90, 'TEST_REC90': mt90,\n",
        "        'VAL_REC100': mv100, 'TEST_REC100': mt100\n",
        "    }\n",
        "\n",
        "# --- Baseline: promedio uniforme de backbones usando solo las columnas *_mean\n",
        "mean_cols = [c for c in feat_cols if c.endswith('_mean')]\n",
        "if len(mean_cols) >= 2:\n",
        "    s_val_mean = VAL[mean_cols].mean(axis=1).values\n",
        "    s_tst_mean = TEST[mean_cols].mean(axis=1).values\n",
        "    res_avg = summarize(f'AVG[{\",\".join(mean_cols)}]', y_val, s_val_mean, y_tst, s_tst_mean)\n",
        "else:\n",
        "    print(\"âš ï¸ No hay suficientes columnas *_mean para baseline promedio.\")\n",
        "\n",
        "# --- Stacking con LogisticRegression sobre todas las features construidas\n",
        "clf = LogisticRegression(max_iter=2000, class_weight='balanced', solver='liblinear')\n",
        "clf.fit(X_val, y_val)\n",
        "s_val = clf.predict_proba(X_val)[:,1]\n",
        "s_tst = clf.predict_proba(X_tst)[:,1]\n",
        "res_stack = summarize('STACK_LR(all_features)', y_val, s_val, y_tst, s_tst)\n",
        "\n",
        "# Guardar resumen en OUT_DIR\n",
        "summary_path = OUT_DIR / \"backbone_ensemble_summary.json\"\n",
        "import json\n",
        "with open(summary_path, \"w\") as f:\n",
        "    json.dump({'features': feat_cols,\n",
        "               'stack_coef': getattr(clf, 'coef_', None).tolist(),\n",
        "               'stack_intercept': getattr(clf, 'intercept_', None).tolist(),\n",
        "               'results': {'avg': res_avg if len(mean_cols)>=2 else None,\n",
        "                           'stack': res_stack}}, f, indent=2)\n",
        "print(f\"\\nðŸ’¾ Resumen guardado en: {summary_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOY1-4yyDOm7",
        "outputId": "3939a944-196e-4e6c-fa0b-d68f24252611"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[AVG[SwinTiny_mean,convnext_tiny.in12k_ft_in1k_slices_mean,png_preds_d121_mean,patient_preds_mean,patient_preds_ensemble_mean,patient_preds_plus_mean,png_preds_mean,slice_preds_plus_mean,slice_preds_seedENS_mean,slices_preds_mean,slice_preds_mean,patient_eval_colab_mean]]\n",
            "VAL F1-opt: {'AUC': 0.4761904761904762, 'PRAUC': 0.38888888888888884, 'Acc': 0.4, 'P': 0.3333333333333333, 'R': 1.0, 'thr': 0.3525, 'n': 10}\n",
            "TEST F1-opt: {'AUC': 0.712962962962963, 'PRAUC': 0.7242325642885107, 'Acc': 0.425531914893617, 'P': 0.425531914893617, 'R': 1.0, 'thr': 0.3525, 'n': 47}\n",
            "VAL@REC90: {'AUC': 0.4761904761904762, 'PRAUC': 0.38888888888888884, 'Acc': 0.3, 'P': 0.3, 'R': 1.0, 'thr': 0.0, 'n': 10} \n",
            "TEST@REC90: {'AUC': 0.712962962962963, 'PRAUC': 0.7242325642885107, 'Acc': 0.425531914893617, 'P': 0.425531914893617, 'R': 1.0, 'thr': 0.0, 'n': 47}\n",
            "VAL@REC100: {'AUC': 0.4761904761904762, 'PRAUC': 0.38888888888888884, 'Acc': 0.3, 'P': 0.3, 'R': 1.0, 'thr': 0.0, 'n': 10} \n",
            "TEST@REC100: {'AUC': 0.712962962962963, 'PRAUC': 0.7242325642885107, 'Acc': 0.425531914893617, 'P': 0.425531914893617, 'R': 1.0, 'thr': 0.0, 'n': 47}\n",
            "\n",
            "[STACK_LR(all_features)]\n",
            "VAL F1-opt: {'AUC': 0.8095238095238095, 'PRAUC': 0.7, 'Acc': 0.8, 'P': 0.6, 'R': 1.0, 'thr': 0.487, 'n': 10}\n",
            "TEST F1-opt: {'AUC': 0.29814814814814816, 'PRAUC': 0.3966513840022451, 'Acc': 0.3829787234042553, 'P': 0.30434782608695654, 'R': 0.35, 'thr': 0.487, 'n': 47}\n",
            "VAL@REC90: {'AUC': 0.8095238095238095, 'PRAUC': 0.7, 'Acc': 0.3, 'P': 0.3, 'R': 1.0, 'thr': 0.0, 'n': 10} \n",
            "TEST@REC90: {'AUC': 0.29814814814814816, 'PRAUC': 0.3966513840022451, 'Acc': 0.425531914893617, 'P': 0.425531914893617, 'R': 1.0, 'thr': 0.0, 'n': 47}\n",
            "VAL@REC100: {'AUC': 0.8095238095238095, 'PRAUC': 0.7, 'Acc': 0.3, 'P': 0.3, 'R': 1.0, 'thr': 0.0, 'n': 10} \n",
            "TEST@REC100: {'AUC': 0.29814814814814816, 'PRAUC': 0.3966513840022451, 'Acc': 0.425531914893617, 'P': 0.425531914893617, 'R': 1.0, 'thr': 0.0, 'n': 47}\n",
            "\n",
            "ðŸ’¾ Resumen guardado en: /content/drive/MyDrive/CognitivaAI/p11_alt_backbones/backbone_ensemble_summary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Celda E1: features con cobertura completa ---\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/CognitivaAI\")\n",
        "OUT  = BASE/\"p11_alt_backbones\"\n",
        "\n",
        "VAL = pd.read_csv(OUT/\"val_patient_features_backbones.csv\")\n",
        "TEST= pd.read_csv(OUT/\"test_patient_features_backbones.csv\")\n",
        "\n",
        "def full47(df):\n",
        "    # columnas por fuente terminan en _mean/_trimmed20/_top7/_p2\n",
        "    # detectamos fuentes por el prefijo antes del sufijo\n",
        "    suf = [\"_mean\",\"_trimmed20\",\"_top7\",\"_p2\"]\n",
        "    feats = [c for c in df.columns if any(c.endswith(s) for s in suf)]\n",
        "    # mapeo fuente -> sus columnas\n",
        "    src2cols = {}\n",
        "    for c in feats:\n",
        "        src = c.rsplit(\"_\",1)[0]  # quita Ãºltimo sufijo\n",
        "        src2cols.setdefault(src, []).append(c)\n",
        "    # nos quedamos con fuentes que tengan sus 4 columnas y sin NaN, y con 47 filas en VAL\n",
        "    keep_src = []\n",
        "    for src, cols in src2cols.items():\n",
        "        if len(cols) < 3:  # al menos mean/top7/trimmed o similar\n",
        "            continue\n",
        "        if VAL[cols].shape[0]==47 and not VAL[cols].isna().any().any():\n",
        "            keep_src.append(src)\n",
        "\n",
        "    print(\"Fuentes con cobertura completa (47 VAL):\", keep_src)\n",
        "    # Construimos nuevos DataFrames minimalistas\n",
        "    base_cols = [\"patient_id\",\"y_true\"]\n",
        "    VAL_full  = VAL[base_cols + sum([src2cols[s] for s in keep_src], [])].copy()\n",
        "    TEST_full = TEST[base_cols + sum([src2cols[s] for s in keep_src], [])].copy()\n",
        "    print(\"VAL_full:\", VAL_full.shape, \"| TEST_full:\", TEST_full.shape)\n",
        "\n",
        "    VAL_full.to_csv(OUT/\"val_features_full47.csv\", index=False)\n",
        "    TEST_full.to_csv(OUT/\"test_features_full47.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "eRkLN7GLEwAK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === E2-prep: alias de archivos esperados por E2 ===\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "OUT = Path(\"/content/drive/MyDrive/CognitivaAI/p11_alt_backbones\")\n",
        "src_val  = OUT / \"val_patient_features_backbones.csv\"\n",
        "src_test = OUT / \"test_patient_features_backbones.csv\"\n",
        "dst_val  = OUT / \"val_features_full47.csv\"\n",
        "dst_test = OUT / \"test_features_full47.csv\"\n",
        "\n",
        "assert src_val.exists(),  f\"No existe {src_val}\"\n",
        "assert src_test.exists(), f\"No existe {src_test}\"\n",
        "\n",
        "VAL  = pd.read_csv(src_val)\n",
        "TEST = pd.read_csv(src_test)\n",
        "\n",
        "# Sanidad rÃ¡pida\n",
        "print(\"VAL shape:\", VAL.shape, \"| TEST shape:\", TEST.shape)\n",
        "assert \"patient_id\" in VAL.columns and \"y_true\" in VAL.columns, \"Faltan columnas clave en VAL\"\n",
        "assert \"patient_id\" in TEST.columns and \"y_true\" in TEST.columns, \"Faltan columnas clave en TEST\"\n",
        "assert len(VAL) in (47, 10),  \"VAL deberÃ­a tener 47 (o 10 si subset); revisa el origen\"\n",
        "assert len(TEST) == 47,       \"TEST deberÃ­a tener 47 pacientes\"\n",
        "\n",
        "# Guardar con los nombres que E2 espera\n",
        "VAL.to_csv(dst_val, index=False)\n",
        "TEST.to_csv(dst_test, index=False)\n",
        "print(\"âœ… Archivos preparados para E2:\")\n",
        "print(\" -\", dst_val)\n",
        "print(\" -\", dst_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RcF3OedF4RB",
        "outputId": "cf7c50a9-3c38-4c79-c86d-bf18ae5317d9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL shape: (10, 50) | TEST shape: (47, 50)\n",
            "âœ… Archivos preparados para E2:\n",
            " - /content/drive/MyDrive/CognitivaAI/p11_alt_backbones/val_features_full47.csv\n",
            " - /content/drive/MyDrive/CognitivaAI/p11_alt_backbones/test_features_full47.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === E2: Random search de ensembles de BACKBONES (fix comparaciones) ===\n",
        "import json, math, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score,\n",
        "    precision_recall_curve, roc_curve, accuracy_score,\n",
        "    precision_score, recall_score, f1_score\n",
        ")\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/CognitivaAI\")\n",
        "OUT  = BASE / \"p11_alt_backbones\"\n",
        "\n",
        "# --- Carga de features de pacientes (creados en CFix/D) ---\n",
        "VAL  = pd.read_csv(OUT / \"val_patient_features_backbones.csv\")\n",
        "TEST = pd.read_csv(OUT / \"test_patient_features_backbones.csv\")\n",
        "\n",
        "assert set([\"patient_id\",\"y_true\"]).issubset(VAL.columns), \"VAL sin columnas base\"\n",
        "assert set([\"patient_id\",\"y_true\"]).issubset(TEST.columns), \"TEST sin columnas base\"\n",
        "\n",
        "# --- SelecciÃ³n de columnas de BACKBONES (solo variantes mean) ---\n",
        "# Puedes ampliarlo si quieres incluir trimmed/top7/p2\n",
        "FEATURES = [\n",
        "    \"SwinTiny_mean\",\n",
        "    \"convnext_tiny.in12k_ft_in1k_slices_mean\",\n",
        "    \"png_preds_d121_mean\",\n",
        "]\n",
        "for c in FEATURES:\n",
        "    if c not in VAL.columns:\n",
        "        raise KeyError(f\"Columna {c} no encontrada en VAL. Columnas disponibles: {list(VAL.columns)[:10]} ...\")\n",
        "    if c not in TEST.columns:\n",
        "        raise KeyError(f\"Columna {c} no encontrada en TEST. Columnas disponibles: {list(TEST.columns)[:10]} ...\")\n",
        "\n",
        "Xv = VAL[FEATURES].values.astype(float)\n",
        "Xt = TEST[FEATURES].values.astype(float)\n",
        "yv = VAL[\"y_true\"].values.astype(int)\n",
        "yt = TEST[\"y_true\"].values.astype(int)\n",
        "\n",
        "# --- Helpers de evaluaciÃ³n ---\n",
        "def eval_at_threshold(y_true, scores, thr):\n",
        "    y_pred = (scores >= thr).astype(int)\n",
        "    return {\n",
        "        \"Acc\": float(accuracy_score(y_true, y_pred)),\n",
        "        \"P\":   float(precision_score(y_true, y_pred, zero_division=0)),\n",
        "        \"R\":   float(recall_score(y_true, y_pred)),\n",
        "        \"thr\": float(thr),\n",
        "        \"n\":   int(len(y_true)),\n",
        "    }\n",
        "\n",
        "def eval_pack(y_true, scores):\n",
        "    # AUC / PR-AUC\n",
        "    try:\n",
        "        auc  = float(roc_auc_score(y_true, scores))\n",
        "    except Exception:\n",
        "        auc = float(\"nan\")\n",
        "    try:\n",
        "        prauc = float(average_precision_score(y_true, scores))\n",
        "    except Exception:\n",
        "        prauc = float(\"nan\")\n",
        "\n",
        "    # F1-Ã³ptimo sobre rejilla + puntos reales de PR\n",
        "    precision, recall, thr_pr = precision_recall_curve(y_true, scores)  # thr_pr shape = n-1\n",
        "    # Evitar problemas si list estÃ¡ vacÃ­o\n",
        "    grid = np.linspace(0, 1, 501)\n",
        "    thr_all = np.unique(np.concatenate([grid, thr_pr[:-1] if len(thr_pr) > 1 else np.array([0.5])]))\n",
        "    best_f1, best_thr = -1.0, 0.0\n",
        "    for thr in thr_all:\n",
        "        m = eval_at_threshold(y_true, scores, thr)\n",
        "        f1 = 0.0\n",
        "        if (m[\"P\"] + m[\"R\"]) > 0:\n",
        "            f1 = 2 * m[\"P\"] * m[\"R\"] / (m[\"P\"] + m[\"R\"])\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_thr = float(f1), float(thr)\n",
        "    mF1 = eval_at_threshold(y_true, scores, best_thr)\n",
        "    mF1[\"AUC\"] = auc\n",
        "    mF1[\"PRAUC\"] = prauc\n",
        "\n",
        "    # Youden (max TPR - FPR)\n",
        "    fpr, tpr, thr_roc = roc_curve(y_true, scores)\n",
        "    youden_idx = np.argmax(tpr - fpr)\n",
        "    thr_youden = float(thr_roc[youden_idx])\n",
        "    mY = eval_at_threshold(y_true, scores, thr_youden)\n",
        "    mY[\"AUC\"] = auc\n",
        "    mY[\"PRAUC\"] = prauc\n",
        "\n",
        "    # Recall-target (90% y 100%) -> seguimos tu convenciÃ³n: thr=0 fuerza Râ‰ˆ1.0 con scores âˆˆ [0,1]\n",
        "    mR90 = eval_at_threshold(y_true, scores, 0.0); mR90[\"AUC\"] = auc; mR90[\"PRAUC\"] = prauc\n",
        "    mR100 = eval_at_threshold(y_true, scores, 0.0); mR100[\"AUC\"] = auc; mR100[\"PRAUC\"] = prauc\n",
        "\n",
        "    return {\"F1\": mF1, \"Youden\": mY, \"REC90\": mR90, \"REC100\": mR100}\n",
        "\n",
        "def weighted_scores(X, w):\n",
        "    w = np.asarray(w, dtype=float)\n",
        "    w = w / (w.sum() + 1e-12)\n",
        "    return (X * w).sum(axis=1)\n",
        "\n",
        "# --- Random search de pesos (Dirichlet) ---\n",
        "N_SAMPLES = 800   # puedes subir/bajar\n",
        "SEED      = 42\n",
        "rng = np.random.default_rng(SEED)\n",
        "\n",
        "best = None  # guardarÃ¡ tupla SOLO con escalares: (PRAUC_VAL, AUC_VAL, w_tuple, metrics_dict)\n",
        "topk = []    # almacenar top-10 por PRAUC_VAL\n",
        "\n",
        "for i in range(N_SAMPLES):\n",
        "    # Dirichlet genera pesos no-negativos que suman 1\n",
        "    w = rng.dirichlet(alpha=np.ones(len(FEATURES)))\n",
        "    pv = weighted_scores(Xv, w)\n",
        "    mv = eval_pack(yv, pv)   # dict con F1/Youden/etc\n",
        "\n",
        "    # --- Tupla de comparaciÃ³n SOLO con escalares para evitar el error ---\n",
        "    cand = (float(mv[\"F1\"][\"PRAUC\"]), float(mv[\"F1\"][\"AUC\"]), tuple(map(float, w)), mv)\n",
        "\n",
        "    if (best is None) or (cand > best):\n",
        "        best = cand\n",
        "\n",
        "    # mantener top-10\n",
        "    topk.append(cand)\n",
        "    if len(topk) > 10:\n",
        "        topk = sorted(topk, reverse=True)[:10]\n",
        "\n",
        "# --- Mejor configuraciÃ³n en VAL ---\n",
        "best_prauc, best_auc, best_w_tuple, best_metrics_val = best\n",
        "best_w = np.array(best_w_tuple, dtype=float)\n",
        "pt = weighted_scores(Xt, best_w)\n",
        "mt = eval_pack(yt, pt)\n",
        "\n",
        "summary = {\n",
        "    \"variant\": \"BackboneEnsemble_DIRICHLET_means\",\n",
        "    \"FEATURES\": FEATURES,\n",
        "    \"N_SAMPLES\": N_SAMPLES,\n",
        "    \"SEED\": SEED,\n",
        "    \"weights\": {FEATURES[i]: float(best_w[i]) for i in range(len(FEATURES))},\n",
        "    \"VAL_F1\": best_metrics_val[\"F1\"],\n",
        "    \"TEST_F1\": mt[\"F1\"],\n",
        "    \"VAL_Youden\": best_metrics_val[\"Youden\"],\n",
        "    \"TEST_Youden\": mt[\"Youden\"],\n",
        "    \"VAL_REC90\": best_metrics_val[\"REC90\"],\n",
        "    \"TEST_REC90\": mt[\"REC90\"],\n",
        "    \"VAL_REC100\": best_metrics_val[\"REC100\"],\n",
        "    \"TEST_REC100\": mt[\"REC100\"],\n",
        "    \"top10_VAL_F1\": [\n",
        "        {\n",
        "            \"F1_VAL\": float(t[3][\"F1\"][\"P\"] * t[3][\"F1\"][\"R\"] * 2 / (t[3][\"F1\"][\"P\"] + t[3][\"F1\"][\"R\"]) if (t[3][\"F1\"][\"P\"]+t[3][\"F1\"][\"R\"])>0 else 0.0),\n",
        "            \"weights\": {FEATURES[j]: float(t[2][j]) for j in range(len(FEATURES))}\n",
        "        }\n",
        "        for t in sorted(topk, reverse=True)\n",
        "    ]\n",
        "}\n",
        "\n",
        "# --- Salida ---\n",
        "print(json.dumps(summary, indent=2))\n",
        "\n",
        "# Guardados\n",
        "with open(OUT / \"backbone_randomsearch_dirichlet_means.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "row = {\n",
        "    \"variant\": \"BKB-ENS(Dirichlet,means)\",\n",
        "    \"VAL_AUC\": summary[\"VAL_F1\"][\"AUC\"],\n",
        "    \"VAL_PRAUC\": summary[\"VAL_F1\"][\"PRAUC\"],\n",
        "    \"TEST_AUC\": summary[\"TEST_F1\"][\"AUC\"],\n",
        "    \"TEST_PRAUC\": summary[\"TEST_F1\"][\"PRAUC\"],\n",
        "    \"TEST_Acc\": summary[\"TEST_F1\"][\"Acc\"],\n",
        "    \"TEST_Recall\": summary[\"TEST_F1\"][\"R\"],\n",
        "    \"TEST_Precision\": summary[\"TEST_F1\"][\"P\"],\n",
        "    \"thr(F1)\": summary[\"TEST_F1\"][\"thr\"],\n",
        "    \"n\": summary[\"TEST_F1\"][\"n\"],\n",
        "}\n",
        "cmp_path = OUT / \"comparison_backbones_eval.csv\"\n",
        "try:\n",
        "    cmp_df = pd.read_csv(cmp_path)\n",
        "except Exception:\n",
        "    cmp_df = pd.DataFrame()\n",
        "cmp_df = pd.concat([cmp_df, pd.DataFrame([row])], ignore_index=True)\n",
        "cmp_df.to_csv(cmp_path, index=False)\n",
        "print(f\"ðŸ“ Actualizado: {cmp_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3PLJDkdE7X8",
        "outputId": "fcbab1c2-ab30-41cf-9b5d-942b7e5ffeea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"variant\": \"BackboneEnsemble_DIRICHLET_means\",\n",
            "  \"FEATURES\": [\n",
            "    \"SwinTiny_mean\",\n",
            "    \"convnext_tiny.in12k_ft_in1k_slices_mean\",\n",
            "    \"png_preds_d121_mean\"\n",
            "  ],\n",
            "  \"N_SAMPLES\": 800,\n",
            "  \"SEED\": 42,\n",
            "  \"weights\": {\n",
            "    \"SwinTiny_mean\": 0.9719824672875522,\n",
            "    \"convnext_tiny.in12k_ft_in1k_slices_mean\": 0.004054230018887773,\n",
            "    \"png_preds_d121_mean\": 0.023963302693559934\n",
            "  },\n",
            "  \"VAL_F1\": {\n",
            "    \"Acc\": 0.7,\n",
            "    \"P\": 0.5,\n",
            "    \"R\": 1.0,\n",
            "    \"thr\": 0.47400000000000003,\n",
            "    \"n\": 10,\n",
            "    \"AUC\": 0.7142857142857142,\n",
            "    \"PRAUC\": 0.6333333333333333\n",
            "  },\n",
            "  \"TEST_F1\": {\n",
            "    \"Acc\": 0.46808510638297873,\n",
            "    \"P\": 0.4444444444444444,\n",
            "    \"R\": 1.0,\n",
            "    \"thr\": 0.43518113616705306,\n",
            "    \"n\": 47,\n",
            "    \"AUC\": 0.5203703703703704,\n",
            "    \"PRAUC\": 0.5229506017798011\n",
            "  },\n",
            "  \"VAL_Youden\": {\n",
            "    \"Acc\": 0.7,\n",
            "    \"P\": 0.5,\n",
            "    \"R\": 1.0,\n",
            "    \"thr\": 0.474212409535214,\n",
            "    \"n\": 10,\n",
            "    \"AUC\": 0.7142857142857142,\n",
            "    \"PRAUC\": 0.6333333333333333\n",
            "  },\n",
            "  \"TEST_Youden\": {\n",
            "    \"Acc\": 0.6170212765957447,\n",
            "    \"P\": 0.6666666666666666,\n",
            "    \"R\": 0.2,\n",
            "    \"thr\": 0.4808294586644241,\n",
            "    \"n\": 47,\n",
            "    \"AUC\": 0.5203703703703704,\n",
            "    \"PRAUC\": 0.5229506017798011\n",
            "  },\n",
            "  \"VAL_REC90\": {\n",
            "    \"Acc\": 0.3,\n",
            "    \"P\": 0.3,\n",
            "    \"R\": 1.0,\n",
            "    \"thr\": 0.0,\n",
            "    \"n\": 10,\n",
            "    \"AUC\": 0.7142857142857142,\n",
            "    \"PRAUC\": 0.6333333333333333\n",
            "  },\n",
            "  \"TEST_REC90\": {\n",
            "    \"Acc\": 0.425531914893617,\n",
            "    \"P\": 0.425531914893617,\n",
            "    \"R\": 1.0,\n",
            "    \"thr\": 0.0,\n",
            "    \"n\": 47,\n",
            "    \"AUC\": 0.5203703703703704,\n",
            "    \"PRAUC\": 0.5229506017798011\n",
            "  },\n",
            "  \"VAL_REC100\": {\n",
            "    \"Acc\": 0.3,\n",
            "    \"P\": 0.3,\n",
            "    \"R\": 1.0,\n",
            "    \"thr\": 0.0,\n",
            "    \"n\": 10,\n",
            "    \"AUC\": 0.7142857142857142,\n",
            "    \"PRAUC\": 0.6333333333333333\n",
            "  },\n",
            "  \"TEST_REC100\": {\n",
            "    \"Acc\": 0.425531914893617,\n",
            "    \"P\": 0.425531914893617,\n",
            "    \"R\": 1.0,\n",
            "    \"thr\": 0.0,\n",
            "    \"n\": 47,\n",
            "    \"AUC\": 0.5203703703703704,\n",
            "    \"PRAUC\": 0.5229506017798011\n",
            "  },\n",
            "  \"top10_VAL_F1\": [\n",
            "    {\n",
            "      \"F1_VAL\": 0.6666666666666666,\n",
            "      \"weights\": {\n",
            "        \"SwinTiny_mean\": 0.9719824672875522,\n",
            "        \"convnext_tiny.in12k_ft_in1k_slices_mean\": 0.004054230018887773,\n",
            "        \"png_preds_d121_mean\": 0.023963302693559934\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"F1_VAL\": 0.6666666666666666,\n",
            "      \"weights\": {\n",
            "        \"SwinTiny_mean\": 0.948070311842012,\n",
            "        \"convnext_tiny.in12k_ft_in1k_slices_mean\": 0.018520300230898224,\n",
            "        \"png_preds_d121_mean\": 0.03340938792708975\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"F1_VAL\": 0.6666666666666666,\n",
            "      \"weights\": {\n",
            "        \"SwinTiny_mean\": 0.9467490298502999,\n",
            "        \"convnext_tiny.in12k_ft_in1k_slices_mean\": 0.005882574391200949,\n",
            "        \"png_preds_d121_mean\": 0.04736839575849928\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"F1_VAL\": 0.6666666666666666,\n",
            "      \"weights\": {\n",
            "        \"SwinTiny_mean\": 0.9456138880923491,\n",
            "        \"convnext_tiny.in12k_ft_in1k_slices_mean\": 0.03897485686573304,\n",
            "        \"png_preds_d121_mean\": 0.015411255041917952\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"F1_VAL\": 0.6666666666666666,\n",
            "      \"weights\": {\n",
            "        \"SwinTiny_mean\": 0.9306490333236793,\n",
            "        \"convnext_tiny.in12k_ft_in1k_slices_mean\": 0.06895315775418483,\n",
            "        \"png_preds_d121_mean\": 0.0003978089221357866\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"F1_VAL\": 0.6666666666666666,\n",
            "      \"weights\": {\n",
            "        \"SwinTiny_mean\": 0.9063046301408365,\n",
            "        \"convnext_tiny.in12k_ft_in1k_slices_mean\": 0.06923423410650192,\n",
            "        \"png_preds_d121_mean\": 0.024461135752661615\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"F1_VAL\": 0.6666666666666666,\n",
            "      \"weights\": {\n",
            "        \"SwinTiny_mean\": 0.8858278060711186,\n",
            "        \"convnext_tiny.in12k_ft_in1k_slices_mean\": 0.060003109150379055,\n",
            "        \"png_preds_d121_mean\": 0.054169084778502394\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"F1_VAL\": 0.6666666666666666,\n",
            "      \"weights\": {\n",
            "        \"SwinTiny_mean\": 0.8528139065660876,\n",
            "        \"convnext_tiny.in12k_ft_in1k_slices_mean\": 0.09139443128976633,\n",
            "        \"png_preds_d121_mean\": 0.05579166214414602\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"F1_VAL\": 0.6666666666666666,\n",
            "      \"weights\": {\n",
            "        \"SwinTiny_mean\": 0.8415568968176128,\n",
            "        \"convnext_tiny.in12k_ft_in1k_slices_mean\": 0.13779811802080005,\n",
            "        \"png_preds_d121_mean\": 0.020644985161587148\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"F1_VAL\": 0.6666666666666666,\n",
            "      \"weights\": {\n",
            "        \"SwinTiny_mean\": 0.8402202298499967,\n",
            "        \"convnext_tiny.in12k_ft_in1k_slices_mean\": 0.09739464284321815,\n",
            "        \"png_preds_d121_mean\": 0.06238512730678531\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "ðŸ“ Actualizado: /content/drive/MyDrive/CognitivaAI/p11_alt_backbones/comparison_backbones_eval.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# E3 â€” Dirichlet ampliado en simplex con mÃ¡s seÃ±ales Ãºtiles\n",
        "import json, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
        "\n",
        "OUT = Path(\"/content/drive/MyDrive/CognitivaAI/p11_alt_backbones\")\n",
        "VAL = pd.read_csv(OUT/\"val_patient_features_backbones.csv\")\n",
        "TEST= pd.read_csv(OUT/\"test_patient_features_backbones.csv\")\n",
        "\n",
        "# Subconjunto de features mÃ¡s informativas (backbones + EffNet P10-ext)\n",
        "FEATS = [\n",
        "    # SwinTiny (mejor: top7)\n",
        "    \"SwinTiny_top7\", \"SwinTiny_trimmed20\", \"SwinTiny_mean\",\n",
        "    # ConvNeXt\n",
        "    \"convnext_tiny.in12k_ft_in1k_slices_top7\", \"convnext_tiny.in12k_ft_in1k_slices_trimmed20\", \"convnext_tiny.in12k_ft_in1k_slices_mean\",\n",
        "    # DenseNet\n",
        "    \"png_preds_d121_top7\", \"png_preds_d121_trimmed20\", \"png_preds_d121_mean\",\n",
        "    # EffNet P10-ext (proxies del proyecto base)\n",
        "    \"patient_preds_plus_mean\", \"slice_preds_plus_mean\", \"slice_preds_seedENS_mean\"\n",
        "]\n",
        "\n",
        "# Filtro por columnas presentes (por si alguna falta)\n",
        "FEATS = [c for c in FEATS if c in VAL.columns and c in TEST.columns]\n",
        "\n",
        "yV = VAL[\"y_true\"].values.astype(float)\n",
        "yT = TEST[\"y_true\"].values.astype(float)\n",
        "XV = VAL[FEATS].values.astype(float)\n",
        "XT = TEST[FEATS].values.astype(float)\n",
        "\n",
        "def eval_split(y, p):\n",
        "    # mÃ©trica F1-opt + AUC/PR-AUC y umbral Ã³ptimo en VAL\n",
        "    thr_grid = np.linspace(0,1,401)\n",
        "    f1s = [f1_score(y, p>=t) for t in thr_grid]\n",
        "    i = int(np.argmax(f1s))\n",
        "    thr = float(thr_grid[i])\n",
        "    return {\n",
        "        \"AUC\": float(roc_auc_score(y, p)),\n",
        "        \"PRAUC\": float(average_precision_score(y, p)),\n",
        "        \"Acc\": float(((p>=thr)==y).mean()),\n",
        "        \"P\": float(((p>=thr) & (y==1)).sum() / max((p>=thr).sum(),1)),\n",
        "        \"R\": float(((p>=thr) & (y==1)).sum() / (y==1).sum()),\n",
        "        \"thr\": thr,\n",
        "        \"n\": int(len(y)),\n",
        "    }\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "N = 2000   # rÃ¡pido; puedes subir si quieres\n",
        "best = None\n",
        "\n",
        "for _ in range(N):\n",
        "    w = rng.dirichlet(np.ones(len(FEATS)))\n",
        "    pv = (XV @ w)\n",
        "    mv = eval_split(yV, pv)\n",
        "    # criterio: maximizar PR-AUC y luego AUC en VAL\n",
        "    cand = (mv[\"PRAUC\"], mv[\"AUC\"])\n",
        "    if (best is None) or (cand > best[0]):\n",
        "        # guarda tambiÃ©n test con el mismo umbral de VAL\n",
        "        thr = mv[\"thr\"]\n",
        "        pt = (XT @ w)\n",
        "        mt = {\n",
        "            \"AUC\": float(roc_auc_score(yT, pt)),\n",
        "            \"PRAUC\": float(average_precision_score(yT, pt)),\n",
        "            \"Acc\": float(((pt>=thr)==yT).mean()),\n",
        "            \"P\": float(((pt>=thr) & (yT==1)).sum() / max((pt>=thr).sum(),1)),\n",
        "            \"R\": float(((pt>=thr) & (yT==1)).sum() / (yT==1).sum()),\n",
        "            \"thr\": thr, \"n\": int(len(yT))\n",
        "        }\n",
        "        best = ((mv[\"PRAUC\"], mv[\"AUC\"]), {\n",
        "            \"weights\": {f: float(w[i]) for i,f in enumerate(FEATS)},\n",
        "            \"VAL_F1\": mv, \"TEST_F1\": mt\n",
        "        })\n",
        "\n",
        "res = {\n",
        "    \"variant\": \"BackboneEnsemble_DIRICHLET_EXT\",\n",
        "    \"FEATURES\": FEATS,\n",
        "    **best[1],\n",
        "}\n",
        "\n",
        "print(json.dumps(res, indent=2))\n",
        "# append a comparison row (para tu CSV comparativo de p11)\n",
        "row = {\n",
        "    \"variant\": \"DIRICHLET_EXT(\"+\",\".join([f.split('_')[0] for f in FEATS[:4]])+\"...)\",\n",
        "    \"VAL_AUC\": res[\"VAL_F1\"][\"AUC\"], \"VAL_PRAUC\": res[\"VAL_F1\"][\"PRAUC\"],\n",
        "    \"TEST_AUC\": res[\"TEST_F1\"][\"AUC\"], \"TEST_PRAUC\": res[\"TEST_F1\"][\"PRAUC\"],\n",
        "    \"TEST_Acc\": res[\"TEST_F1\"][\"Acc\"], \"TEST_Recall\": res[\"TEST_F1\"][\"R\"], \"TEST_Precision\": res[\"TEST_F1\"][\"P\"],\n",
        "    \"thr(F1)\": res[\"TEST_F1\"][\"thr\"], \"n\": res[\"TEST_F1\"][\"n\"]\n",
        "}\n",
        "comp_path = OUT/\"comparison_backbones_eval.csv\"\n",
        "try:\n",
        "    comp = pd.read_csv(comp_path)\n",
        "except Exception:\n",
        "    comp = pd.DataFrame()\n",
        "comp = pd.concat([comp, pd.DataFrame([row])], ignore_index=True)\n",
        "comp.to_csv(comp_path, index=False)\n",
        "print(\"ðŸ“ Actualizado:\", comp_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA71MwfuP369",
        "outputId": "c2bd06d0-dbb9-4933-df0f-55eed15c39c1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"variant\": \"BackboneEnsemble_DIRICHLET_EXT\",\n",
            "  \"FEATURES\": [\n",
            "    \"SwinTiny_top7\",\n",
            "    \"SwinTiny_trimmed20\",\n",
            "    \"SwinTiny_mean\",\n",
            "    \"convnext_tiny.in12k_ft_in1k_slices_top7\",\n",
            "    \"convnext_tiny.in12k_ft_in1k_slices_trimmed20\",\n",
            "    \"convnext_tiny.in12k_ft_in1k_slices_mean\",\n",
            "    \"png_preds_d121_top7\",\n",
            "    \"png_preds_d121_trimmed20\",\n",
            "    \"png_preds_d121_mean\",\n",
            "    \"patient_preds_plus_mean\",\n",
            "    \"slice_preds_plus_mean\",\n",
            "    \"slice_preds_seedENS_mean\"\n",
            "  ],\n",
            "  \"weights\": {\n",
            "    \"SwinTiny_top7\": 0.062062403503960833,\n",
            "    \"SwinTiny_trimmed20\": 0.08353838871471263,\n",
            "    \"SwinTiny_mean\": 0.07255728353673895,\n",
            "    \"convnext_tiny.in12k_ft_in1k_slices_top7\": 0.25426684217260964,\n",
            "    \"convnext_tiny.in12k_ft_in1k_slices_trimmed20\": 0.10858588786135814,\n",
            "    \"convnext_tiny.in12k_ft_in1k_slices_mean\": 0.023409202916122442,\n",
            "    \"png_preds_d121_top7\": 0.2671135273008145,\n",
            "    \"png_preds_d121_trimmed20\": 0.06085751693107517,\n",
            "    \"png_preds_d121_mean\": 0.014791899134586303,\n",
            "    \"patient_preds_plus_mean\": 0.014360306987098798,\n",
            "    \"slice_preds_plus_mean\": 0.0379188182127677,\n",
            "    \"slice_preds_seedENS_mean\": 0.0005379227281549382\n",
            "  },\n",
            "  \"VAL_F1\": {\n",
            "    \"AUC\": 0.7142857142857142,\n",
            "    \"PRAUC\": 0.6805555555555556,\n",
            "    \"Acc\": 0.4,\n",
            "    \"P\": 0.3333333333333333,\n",
            "    \"R\": 1.0,\n",
            "    \"thr\": 0.48,\n",
            "    \"n\": 10\n",
            "  },\n",
            "  \"TEST_F1\": {\n",
            "    \"AUC\": 0.36111111111111116,\n",
            "    \"PRAUC\": 0.40515498357766916,\n",
            "    \"Acc\": 0.44680851063829785,\n",
            "    \"P\": 0.425,\n",
            "    \"R\": 0.85,\n",
            "    \"thr\": 0.48,\n",
            "    \"n\": 47\n",
            "  }\n",
            "}\n",
            "ðŸ“ Actualizado: /content/drive/MyDrive/CognitivaAI/p11_alt_backbones/comparison_backbones_eval.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# E4 â€” Stacking con L1 fuerte sobre seÃ±ales \"fuertes\"\n",
        "import json, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
        "\n",
        "OUT = Path(\"/content/drive/MyDrive/CognitivaAI/p11_alt_backbones\")\n",
        "VAL = pd.read_csv(OUT/\"val_patient_features_backbones.csv\")\n",
        "TEST= pd.read_csv(OUT/\"test_patient_features_backbones.csv\")\n",
        "\n",
        "FEATS = [\n",
        "    \"SwinTiny_top7\",\n",
        "    \"convnext_tiny.in12k_ft_in1k_slices_top7\",\n",
        "    \"png_preds_d121_trimmed20\",\n",
        "    \"patient_preds_plus_mean\", \"slice_preds_plus_mean\", \"slice_preds_seedENS_mean\"\n",
        "]\n",
        "FEATS = [f for f in FEATS if f in VAL.columns]\n",
        "\n",
        "yV = VAL[\"y_true\"].values.astype(float)\n",
        "yT = TEST[\"y_true\"].values.astype(float)\n",
        "XV = VAL[FEATS].values.astype(float)\n",
        "XT = TEST[FEATS].values.astype(float)\n",
        "\n",
        "# L1 fuerte y C bajo para evitar sobreajuste en 10 casos\n",
        "clf = LogisticRegression(penalty=\"l1\", C=0.25, solver=\"liblinear\", max_iter=2000)\n",
        "clf.fit(XV, yV)\n",
        "pv = clf.predict_proba(XV)[:,1]\n",
        "\n",
        "# umbral F1 Ã³ptimo en VAL\n",
        "ths = np.linspace(0,1,401)\n",
        "f1s = [f1_score(yV, pv>=t) for t in ths]\n",
        "thr = float(ths[int(np.argmax(f1s))])\n",
        "\n",
        "def metr(y, p, thr):\n",
        "    return {\n",
        "        \"AUC\": float(roc_auc_score(y, p)),\n",
        "        \"PRAUC\": float(average_precision_score(y, p)),\n",
        "        \"Acc\": float(((p>=thr)==y).mean()),\n",
        "        \"P\": float(((p>=thr) & (y==1)).sum() / max((p>=thr).sum(),1)),\n",
        "        \"R\": float(((p>=thr) & (y==1)).sum() / (y==1).sum()),\n",
        "        \"thr\": thr, \"n\": int(len(y))\n",
        "    }\n",
        "\n",
        "mv = metr(yV, pv, thr)\n",
        "pt = clf.predict_proba(XT)[:,1]\n",
        "mt = metr(yT, pt, thr)\n",
        "\n",
        "res = {\n",
        "  \"variant\": \"STACK_L1_STRONG\",\n",
        "  \"FEATURES\": FEATS,\n",
        "  \"coef\": {f: float(c) for f,c in zip(FEATS, clf.coef_[0])},\n",
        "  \"intercept\": float(clf.intercept_[0]),\n",
        "  \"VAL_F1\": mv, \"TEST_F1\": mt\n",
        "}\n",
        "print(json.dumps(res, indent=2))\n",
        "\n",
        "# add to comparison\n",
        "row = {\n",
        "    \"variant\": \"STACK_L1_STRONG\",\n",
        "    \"VAL_AUC\": mv[\"AUC\"], \"VAL_PRAUC\": mv[\"PRAUC\"],\n",
        "    \"TEST_AUC\": mt[\"AUC\"], \"TEST_PRAUC\": mt[\"PRAUC\"],\n",
        "    \"TEST_Acc\": mt[\"Acc\"], \"TEST_Recall\": mt[\"R\"], \"TEST_Precision\": mt[\"P\"],\n",
        "    \"thr(F1)\": mt[\"thr\"], \"n\": mt[\"n\"]\n",
        "}\n",
        "comp_path = OUT/\"comparison_backbones_eval.csv\"\n",
        "try:\n",
        "    comp = pd.read_csv(comp_path)\n",
        "except Exception:\n",
        "    comp = pd.DataFrame()\n",
        "comp = pd.concat([comp, pd.DataFrame([row])], ignore_index=True)\n",
        "comp.to_csv(comp_path, index=False)\n",
        "print(\"ðŸ“ Actualizado:\", comp_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbG0uulvU5WK",
        "outputId": "8f5f535f-89fd-4415-9ae4-37c76b8b07f1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"variant\": \"STACK_L1_STRONG\",\n",
            "  \"FEATURES\": [\n",
            "    \"SwinTiny_top7\",\n",
            "    \"convnext_tiny.in12k_ft_in1k_slices_top7\",\n",
            "    \"png_preds_d121_trimmed20\",\n",
            "    \"patient_preds_plus_mean\",\n",
            "    \"slice_preds_plus_mean\",\n",
            "    \"slice_preds_seedENS_mean\"\n",
            "  ],\n",
            "  \"coef\": {\n",
            "    \"SwinTiny_top7\": 0.0,\n",
            "    \"convnext_tiny.in12k_ft_in1k_slices_top7\": 0.0,\n",
            "    \"png_preds_d121_trimmed20\": 0.0,\n",
            "    \"patient_preds_plus_mean\": 0.0,\n",
            "    \"slice_preds_plus_mean\": 0.0,\n",
            "    \"slice_preds_seedENS_mean\": 0.0\n",
            "  },\n",
            "  \"intercept\": 0.0,\n",
            "  \"VAL_F1\": {\n",
            "    \"AUC\": 0.5,\n",
            "    \"PRAUC\": 0.3,\n",
            "    \"Acc\": 0.3,\n",
            "    \"P\": 0.3,\n",
            "    \"R\": 1.0,\n",
            "    \"thr\": 0.0,\n",
            "    \"n\": 10\n",
            "  },\n",
            "  \"TEST_F1\": {\n",
            "    \"AUC\": 0.5,\n",
            "    \"PRAUC\": 0.425531914893617,\n",
            "    \"Acc\": 0.425531914893617,\n",
            "    \"P\": 0.425531914893617,\n",
            "    \"R\": 1.0,\n",
            "    \"thr\": 0.0,\n",
            "    \"n\": 47\n",
            "  }\n",
            "}\n",
            "ðŸ“ Actualizado: /content/drive/MyDrive/CognitivaAI/p11_alt_backbones/comparison_backbones_eval.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# E5 â€” CalibraciÃ³n isotÃ³nica sobre la mejor seÃ±al unitaria (SwinTiny_top7)\n",
        "import json, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
        "\n",
        "OUT = Path(\"/content/drive/MyDrive/CognitivaAI/p11_alt_backbones\")\n",
        "VAL = pd.read_csv(OUT/\"val_patient_features_backbones.csv\")\n",
        "TEST= pd.read_csv(OUT/\"test_patient_features_backbones.csv\")\n",
        "\n",
        "feat = \"SwinTiny_top7\" if \"SwinTiny_top7\" in VAL.columns else \"SwinTiny_mean\"\n",
        "yV = VAL[\"y_true\"].values.astype(float); pV = VAL[feat].values.astype(float)\n",
        "yT = TEST[\"y_true\"].values.astype(float); pT = TEST[feat].values.astype(float)\n",
        "\n",
        "iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
        "pV_cal = iso.fit_transform(pV, yV)\n",
        "pT_cal = iso.transform(pT)\n",
        "\n",
        "def f1opt(y,p):\n",
        "    ths = np.linspace(0,1,401)\n",
        "    f1s = [f1_score(y, p>=t) for t in ths]\n",
        "    i = int(np.argmax(f1s)); thr = float(ths[i])\n",
        "    return thr\n",
        "\n",
        "thr = f1opt(yV, pV_cal)\n",
        "\n",
        "def metr(y, p, thr):\n",
        "    return {\n",
        "        \"AUC\": float(roc_auc_score(y, p)),\n",
        "        \"PRAUC\": float(average_precision_score(y, p)),\n",
        "        \"Acc\": float(((p>=thr)==y).mean()),\n",
        "        \"P\": float(((p>=thr) & (y==1)).sum() / max((p>=thr).sum(),1)),\n",
        "        \"R\": float(((p>=thr) & (y==1)).sum() / (y==1).sum()),\n",
        "        \"thr\": thr, \"n\": int(len(y))\n",
        "    }\n",
        "\n",
        "res = {\n",
        "  \"variant\": f\"{feat}_ISOTONIC\",\n",
        "  \"VAL_F1\": metr(yV, pV_cal, thr),\n",
        "  \"TEST_F1\": metr(yT, pT_cal, thr)\n",
        "}\n",
        "print(json.dumps(res, indent=2))\n",
        "\n",
        "# add to comparison\n",
        "row = {\n",
        "    \"variant\": f\"{feat}_ISOTONIC\",\n",
        "    \"VAL_AUC\": res[\"VAL_F1\"][\"AUC\"], \"VAL_PRAUC\": res[\"VAL_F1\"][\"PRAUC\"],\n",
        "    \"TEST_AUC\": res[\"TEST_F1\"][\"AUC\"], \"TEST_PRAUC\": res[\"TEST_F1\"][\"PRAUC\"],\n",
        "    \"TEST_Acc\": res[\"TEST_F1\"][\"Acc\"], \"TEST_Recall\": res[\"TEST_F1\"][\"R\"], \"TEST_Precision\": res[\"TEST_F1\"][\"P\"],\n",
        "    \"thr(F1)\": res[\"TEST_F1\"][\"thr\"], \"n\": res[\"TEST_F1\"][\"n\"]\n",
        "}\n",
        "comp_path = OUT/\"comparison_backbones_eval.csv\"\n",
        "try:\n",
        "    comp = pd.read_csv(comp_path)\n",
        "except Exception:\n",
        "    comp = pd.DataFrame()\n",
        "comp = pd.concat([comp, pd.DataFrame([row])], ignore_index=True)\n",
        "comp.to_csv(comp_path, index=False)\n",
        "print(\"ðŸ“ Actualizado:\", comp_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsr_Eh4nU8zW",
        "outputId": "7bee5b8a-47b2-4f17-93a0-6516ac3b1412"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"variant\": \"SwinTiny_top7_ISOTONIC\",\n",
            "  \"VAL_F1\": {\n",
            "    \"AUC\": 0.7142857142857143,\n",
            "    \"PRAUC\": 0.5555555555555556,\n",
            "    \"Acc\": 0.4,\n",
            "    \"P\": 0.3333333333333333,\n",
            "    \"R\": 1.0,\n",
            "    \"thr\": 0.0025,\n",
            "    \"n\": 10\n",
            "  },\n",
            "  \"TEST_F1\": {\n",
            "    \"AUC\": 0.5657407407407408,\n",
            "    \"PRAUC\": 0.45797826494635,\n",
            "    \"Acc\": 0.5531914893617021,\n",
            "    \"P\": 0.48717948717948717,\n",
            "    \"R\": 0.95,\n",
            "    \"thr\": 0.0025,\n",
            "    \"n\": 47\n",
            "  }\n",
            "}\n",
            "ðŸ“ Actualizado: /content/drive/MyDrive/CognitivaAI/p11_alt_backbones/comparison_backbones_eval.csv\n"
          ]
        }
      ]
    }
  ]
}