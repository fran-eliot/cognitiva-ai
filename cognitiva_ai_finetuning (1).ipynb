{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e05a7c34623e47aa887dfc9305cd1aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_645a0ccce3694c17a6121115f3eaa31d",
              "IPY_MODEL_067aaab3cf1345d4b601d00c6354ae07",
              "IPY_MODEL_ad3a6f2234ff4f4199f686a6b16a3283"
            ],
            "layout": "IPY_MODEL_bb45967aa60e40e18249b9ae6105ac10"
          }
        },
        "645a0ccce3694c17a6121115f3eaa31d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4a2e14132ec4bf0b1f9af91be645874",
            "placeholder": "​",
            "style": "IPY_MODEL_7fd6d6921fad499fad9922b86e761c38",
            "value": "model.safetensors: 100%"
          }
        },
        "067aaab3cf1345d4b601d00c6354ae07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27c983ad5032439fbaa5639d6e13c059",
            "max": 49335454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cdc92b7ad8c44acc89399c74681e9581",
            "value": 49335454
          }
        },
        "ad3a6f2234ff4f4199f686a6b16a3283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66adf76c5b674eed9489c718a006bfc9",
            "placeholder": "​",
            "style": "IPY_MODEL_a61efa444c4e4805ad970aa27acbf58c",
            "value": " 49.3M/49.3M [00:00&lt;00:00, 198MB/s]"
          }
        },
        "bb45967aa60e40e18249b9ae6105ac10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4a2e14132ec4bf0b1f9af91be645874": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fd6d6921fad499fad9922b86e761c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27c983ad5032439fbaa5639d6e13c059": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdc92b7ad8c44acc89399c74681e9581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66adf76c5b674eed9489c718a006bfc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a61efa444c4e4805ad970aa27acbf58c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgl_cS5TQb4p",
        "outputId": "dda567a1-c782-4752-cc7a-ea981450d7ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m132.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Celda 0: Dependencias y configuración\n",
        "!pip -q install timm==1.0.9 --no-deps\n",
        "\n",
        "import os, math, time, json, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "\n",
        "import timm\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, accuracy_score, precision_score, recall_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# Rutas\n",
        "DATA_DIR = \"/content/drive/MyDrive/CognitivaAI/oas1_data\"\n",
        "CSV_TRAIN = f\"{DATA_DIR}/oas1_train_colab_mapped.csv\"\n",
        "CSV_VAL   = f\"{DATA_DIR}/oas1_val_colab_mapped.csv\"\n",
        "CSV_TEST  = f\"{DATA_DIR}/oas1_test_colab_mapped.csv\"\n",
        "OUT_DIR   = \"/content/drive/MyDrive/CognitivaAI/ft_effb3_colab\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Hiperparámetros base\n",
        "IMG_SIZE   = 300            # recomendado para EfficientNet-B3\n",
        "BATCH_SIZE = 32             # T4 friendly (ajusta a 24-40 si falta memoria)\n",
        "NUM_WORKERS= 2\n",
        "EPOCHS     = 12             # entrenamiento corto con early stopping\n",
        "BASE_LR    = 3e-4\n",
        "WD         = 1e-4\n",
        "PATIENCE   = 4              # early stopping\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1: Dataset MRI slices y DataLoaders\n",
        "\n",
        "class MRISliceDataset(Dataset):\n",
        "    def __init__(self, csv_path, transform=None):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        assert {'png_path','target','patient_id','scan_id'}.issubset(df.columns), \"CSV con columnas requeridas\"\n",
        "        self.paths = df['png_path'].astype(str).tolist()\n",
        "        self.labels = df['target'].astype(int).to_numpy()\n",
        "        self.pids = df['patient_id'].astype(str).to_numpy()\n",
        "        self.sids = df['scan_id'].astype(str).to_numpy()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.paths[idx]\n",
        "        img = Image.open(path).convert('L')  # imágenes axiales en escala de grises\n",
        "        img = img.resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR)\n",
        "        img = np.array(img, dtype=np.float32) / 255.0\n",
        "        img = np.stack([img, img, img], axis=0)  # 1->3 canales\n",
        "        if self.transform:\n",
        "            # transform de torchvision espera PIL o tensor HWC; convertimos\n",
        "            img_t = transforms.functional.to_pil_image(img.transpose(1,2,0))\n",
        "            img_t = self.transform(img_t)\n",
        "        else:\n",
        "            img_t = torch.from_numpy(img)\n",
        "        y = self.labels[idx]\n",
        "        return img_t, y, self.pids[idx], self.sids[idx], path\n",
        "\n",
        "# Transforms\n",
        "mean_std = ([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.85,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=7),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(*mean_std),\n",
        "])\n",
        "eval_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(*mean_std),\n",
        "])\n",
        "\n",
        "ds_tr = MRISliceDataset(CSV_TRAIN, transform=train_tfms)\n",
        "ds_va = MRISliceDataset(CSV_VAL,   transform=eval_tfms)\n",
        "ds_te = MRISliceDataset(CSV_TEST,  transform=eval_tfms)\n",
        "\n",
        "# Sampler balanceado por clase (opcional pero útil)\n",
        "class_counts = np.bincount(ds_tr.labels, minlength=2)\n",
        "w_neg, w_pos = 1.0/class_counts[0], 1.0/class_counts[1]\n",
        "sample_weights = np.where(ds_tr.labels==1, w_pos, w_neg)\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, sampler=sampler,  num_workers=NUM_WORKERS, pin_memory=True)\n",
        "dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False,     num_workers=NUM_WORKERS, pin_memory=True)\n",
        "dl_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False,     num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(\"TRAIN slices:\", len(ds_tr), \"| VAL:\", len(ds_va), \"| TEST:\", len(ds_te))\n",
        "print(\"Class counts train:\", class_counts, \"→ pos_weight≈\", round(class_counts[0]/max(1,class_counts[1]),3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5shRNeJxQzhg",
        "outputId": "47142603-2f92-452e-9ead-9cf933e72e7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN slices: 2820 | VAL: 940 | TEST: 940\n",
            "Class counts train: [1620 1200] → pos_weight≈ 1.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 2: Modelo EfficientNet-B3 con fine-tuning parcial\n",
        "\n",
        "BACKBONE = \"tf_efficientnet_b3_ns\"  # timm\n",
        "model = timm.create_model(BACKBONE, pretrained=True, num_classes=0, in_chans=3)\n",
        "feat_dim = model.num_features\n",
        "\n",
        "# Head ligera\n",
        "head = nn.Sequential(\n",
        "    nn.Dropout(p=0.3),\n",
        "    nn.Linear(feat_dim, 1)\n",
        ")\n",
        "\n",
        "net = nn.Sequential(model, head).to(DEVICE)\n",
        "\n",
        "# Congelar todo menos el último bloque del backbone + head\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Descongelar el último bloque de EfficientNet-B3\n",
        "# Identificamos módulos finales típicos en timm\n",
        "for name, module in model.named_modules():\n",
        "    last_block = ('blocks.6', 'blocks.7')  # por si la variante incluye más\n",
        "    if any(name.startswith(lb) for lb in last_block):\n",
        "        for p in module.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "# Head entrenable\n",
        "for p in head.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Optimizador\n",
        "trainable_params = [p for p in net.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.AdamW(trainable_params, lr=BASE_LR, weight_decay=WD)\n",
        "# Cosine schedule\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "# Pérdida con pos_weight\n",
        "pos_weight = torch.tensor([class_counts[0]/max(1,class_counts[1])], device=DEVICE, dtype=torch.float32)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "print(\"Trainable params:\", sum(p.numel() for p in net.parameters() if p.requires_grad))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "e05a7c34623e47aa887dfc9305cd1aee",
            "645a0ccce3694c17a6121115f3eaa31d",
            "067aaab3cf1345d4b601d00c6354ae07",
            "ad3a6f2234ff4f4199f686a6b16a3283",
            "bb45967aa60e40e18249b9ae6105ac10",
            "f4a2e14132ec4bf0b1f9af91be645874",
            "7fd6d6921fad499fad9922b86e761c38",
            "27c983ad5032439fbaa5639d6e13c059",
            "cdc92b7ad8c44acc89399c74681e9581",
            "66adf76c5b674eed9489c718a006bfc9",
            "a61efa444c4e4805ad970aa27acbf58c"
          ]
        },
        "id": "BKjuSfKeRFGL",
        "outputId": "2a36eaea-8f27-46ec-841e-6163887f024b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b3_ns to current tf_efficientnet_b3.ns_jft_in1k.\n",
            "  model = create_fn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e05a7c34623e47aa887dfc9305cd1aee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 3285755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 3: Entrenamiento\n",
        "\n",
        "def run_epoch(dataloader, train=True):\n",
        "    net.train(train)\n",
        "    total_loss = 0.0\n",
        "    logits_all, y_all = [], []\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "    for xb, yb, *_ in dataloader:\n",
        "        xb, yb = xb.to(DEVICE, non_blocking=True), yb.float().to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(True):\n",
        "            logits = net(xb).squeeze(1)\n",
        "            loss = criterion(logits, yb)\n",
        "        if train:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        logits_all.append(logits.detach().float().cpu().numpy())\n",
        "        y_all.append(yb.detach().float().cpu().numpy())\n",
        "    if not train:\n",
        "        with torch.no_grad():\n",
        "            pass\n",
        "    if train:\n",
        "        scheduler.step()\n",
        "    y_all = np.concatenate(y_all)\n",
        "    logits_all = np.concatenate(logits_all)\n",
        "    probs_all = 1/(1+np.exp(-logits_all))\n",
        "    # Métricas slice-level\n",
        "    auc = roc_auc_score(y_all, probs_all) if len(np.unique(y_all))>1 else np.nan\n",
        "    pr  = average_precision_score(y_all, probs_all) if len(np.unique(y_all))>1 else np.nan\n",
        "    # Thr 0.5\n",
        "    yhat = (probs_all >= 0.5).astype(int)\n",
        "    acc = accuracy_score(y_all, yhat)\n",
        "    pre = precision_score(y_all, yhat, zero_division=0)\n",
        "    rec = recall_score(y_all, yhat, zero_division=0)\n",
        "    brier = np.mean((probs_all - y_all)**2)\n",
        "    return {\n",
        "        \"loss\": float(total_loss/len(dataloader.dataset)),\n",
        "        \"auc\": float(auc) if not np.isnan(auc) else None,\n",
        "        \"pr\": float(pr) if not np.isnan(pr) else None,\n",
        "        \"acc\": float(acc),\n",
        "        \"pre\": float(pre),\n",
        "        \"rec\": float(rec),\n",
        "        \"brier\": float(brier),\n",
        "        \"probs\": probs_all.tolist(), # Convert numpy array to list\n",
        "        \"logits\": logits_all.tolist(), # Convert numpy array to list\n",
        "        \"y\": y_all.tolist() # Convert numpy array to list\n",
        "    }\n",
        "\n",
        "best_val = -np.inf\n",
        "pat = 0\n",
        "hist = []\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr = run_epoch(dl_tr, train=True)\n",
        "    va = run_epoch(dl_va, train=False)\n",
        "    # criterio: PR-AUC slice en VAL (más sensible a clase positiva)\n",
        "    score = va[\"pr\"] if va[\"pr\"] is not None else va[\"auc\"]\n",
        "    hist.append({\"epoch\":epoch, \"train\":tr, \"val\":va})\n",
        "    print(f\"[{epoch:02d}] TR loss={tr['loss']:.4f} | VAL AUC={va['auc']:.3f} PR-AUC={va['pr']:.3f} Brier={va['brier']:.3f}\")\n",
        "    if score is not None and score > best_val:\n",
        "        best_val = score\n",
        "        pat = 0\n",
        "        torch.save(net.state_dict(), f\"{OUT_DIR}/best_ft_effb3.pth\")\n",
        "    else:\n",
        "        pat += 1\n",
        "        if pat >= PATIENCE:\n",
        "            print(\"→ Early stopping.\")\n",
        "            break\n",
        "\n",
        "# Guardar historial\n",
        "with open(f\"{OUT_DIR}/train_history.json\",\"w\") as f:\n",
        "    json.dump(hist, f)\n",
        "print(\"Entrenamiento finalizado. Mejor PR-AUC VAL:\", round(best_val,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdaAPRurRM-d",
        "outputId": "6b97dff9-9566-417a-fa89-66fe3e09e7d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01] TR loss=0.7466 | VAL AUC=0.674 PR-AUC=0.558 Brier=0.237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02] TR loss=0.6407 | VAL AUC=0.652 PR-AUC=0.563 Brier=0.237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[03] TR loss=0.5669 | VAL AUC=0.666 PR-AUC=0.568 Brier=0.246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04] TR loss=0.5075 | VAL AUC=0.671 PR-AUC=0.583 Brier=0.242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05] TR loss=0.4643 | VAL AUC=0.665 PR-AUC=0.583 Brier=0.249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06] TR loss=0.4370 | VAL AUC=0.655 PR-AUC=0.577 Brier=0.257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07] TR loss=0.3698 | VAL AUC=0.667 PR-AUC=0.573 Brier=0.260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[08] TR loss=0.3477 | VAL AUC=0.666 PR-AUC=0.572 Brier=0.277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "/tmp/ipython-input-1855844539.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "/tmp/ipython-input-1855844539.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[09] TR loss=0.3292 | VAL AUC=0.661 PR-AUC=0.568 Brier=0.278\n",
            "→ Early stopping.\n",
            "Entrenamiento finalizado. Mejor PR-AUC VAL: 0.5833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 4: Inferencia + pooling paciente (mean y attention)\n",
        "\n",
        "# Cargar mejor modelo\n",
        "net.load_state_dict(torch.load(f\"{OUT_DIR}/best_ft_effb3.pth\", map_location=DEVICE))\n",
        "net.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer(dataloader):\n",
        "    logits_all, y_all, pids_all = [], [], []\n",
        "    for xb, yb, pids, *_ in dataloader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        lg = net(xb).squeeze(1)\n",
        "        logits_all.append(lg.float().cpu().numpy())\n",
        "        y_all.append(yb.numpy())\n",
        "        pids_all += list(pids)\n",
        "    logits = np.concatenate(logits_all)\n",
        "    y = np.concatenate(y_all)\n",
        "    probs = 1/(1+np.exp(-logits))\n",
        "    return logits, probs, y, np.array(pids_all)\n",
        "\n",
        "log_tr, pr_tr, y_tr, pid_tr = infer(dl_tr)\n",
        "log_va, pr_va, y_va, pid_va = infer(dl_va)\n",
        "log_te, pr_te, y_te, pid_te = infer(dl_te)\n",
        "\n",
        "def patient_pool_mean(probs, labels, pids):\n",
        "    df = pd.DataFrame({\"pid\":pids, \"y\":labels, \"p\":probs})\n",
        "    g = df.groupby(\"pid\")\n",
        "    p_pool = g[\"p\"].mean().values\n",
        "    y_pool = g[\"y\"].mean().round().astype(int).values\n",
        "    return y_pool, p_pool, g.size().values\n",
        "\n",
        "def patient_pool_attention(logits, labels, pids, temp=1.0):\n",
        "    # Atención softmax sobre |logits| como importancias (simple, estable)\n",
        "    df = pd.DataFrame({\"pid\":pids, \"y\":labels, \"z\":logits})\n",
        "    outs = []\n",
        "    for pid, grp in df.groupby(\"pid\"):\n",
        "        z = grp[\"z\"].values\n",
        "        # pesos ~ softmax(|z|/T) para resaltar slices informativos\n",
        "        w = np.exp(np.abs(z)/temp); w = w / (w.sum()+1e-8)\n",
        "        p = 1/(1+np.exp(-z))\n",
        "        p_att = (w*p).sum()\n",
        "        y = int(round(grp[\"y\"].mean()))\n",
        "        outs.append((pid, y, p_att, len(grp)))\n",
        "    outs = pd.DataFrame(outs, columns=[\"pid\",\"y\",\"p\",\"n\"])\n",
        "    return outs[\"y\"].values, outs[\"p\"].values, outs[\"n\"].values\n",
        "\n",
        "def eval_patient(y, p, thr=0.5):\n",
        "    auc = roc_auc_score(y, p) if len(np.unique(y))>1 else np.nan\n",
        "    pr  = average_precision_score(y, p) if len(np.unique(y))>1 else np.nan\n",
        "    yhat = (p>=thr).astype(int)\n",
        "    acc = accuracy_score(y, yhat)\n",
        "    pre = precision_score(y, yhat, zero_division=0)\n",
        "    rec = recall_score(y, yhat, zero_division=0)\n",
        "    return {\"AUC\":auc,\"PR-AUC\":pr,\"Acc\":acc,\"P\":pre,\"R\":rec,\"thr\":thr,\"n\":len(y)}\n",
        "\n",
        "# Mean pooling\n",
        "yV_m, pV_m, _ = patient_pool_mean(pr_va, y_va, pid_va)\n",
        "yT_m, pT_m, _ = patient_pool_mean(pr_te, y_te, pid_te)\n",
        "\n",
        "# Attention pooling\n",
        "yV_a, pV_a, _ = patient_pool_attention(log_va, y_va, pid_va, temp=1.0)\n",
        "yT_a, pT_a, _ = patient_pool_attention(log_te, y_te, pid_te, temp=1.0)\n",
        "\n",
        "print(\"VAL (mean@0.5):\", eval_patient(yV_m, pV_m, 0.5))\n",
        "print(\"TEST(mean@0.5):\", eval_patient(yT_m, pT_m, 0.5))\n",
        "print(\"VAL (attn@0.5):\", eval_patient(yV_a, pV_a, 0.5))\n",
        "print(\"TEST(attn@0.5):\", eval_patient(yT_a, pT_a, 0.5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzEP2NS9ZiRP",
        "outputId": "0e8ddd4a-f34a-487f-8418-066533f30ab8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL (mean@0.5): {'AUC': np.float64(0.7388888888888889), 'PR-AUC': np.float64(0.6587843825001534), 'Acc': 0.6382978723404256, 'P': 0.6, 'R': 0.45, 'thr': 0.5, 'n': 47}\n",
            "TEST(mean@0.5): {'AUC': np.float64(0.875925925925926), 'PR-AUC': np.float64(0.7626011139703089), 'Acc': 0.723404255319149, 'P': 0.7333333333333333, 'R': 0.55, 'thr': 0.5, 'n': 47}\n",
            "VAL (attn@0.5): {'AUC': np.float64(0.7611111111111111), 'PR-AUC': np.float64(0.6851116491294511), 'Acc': 0.6382978723404256, 'P': 0.6153846153846154, 'R': 0.4, 'thr': 0.5, 'n': 47}\n",
            "TEST(attn@0.5): {'AUC': np.float64(0.8722222222222222), 'PR-AUC': np.float64(0.764498046830885), 'Acc': 0.7659574468085106, 'P': 0.8461538461538461, 'R': 0.55, 'thr': 0.5, 'n': 47}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 5: Temperature scaling (ajuste en VAL) y evaluación paciente\n",
        "\n",
        "class TemperatureScaler(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.logT = nn.Parameter(torch.zeros(1))  # T = exp(logT) >= 1\n",
        "\n",
        "    def forward(self, logits):\n",
        "        T = torch.exp(self.logT) + 1e-6\n",
        "        return logits / T\n",
        "\n",
        "def fit_temperature(logits_val, y_val, max_iter=2000, lr=0.01):\n",
        "    y = torch.tensor(y_val, dtype=torch.float32, device=DEVICE)\n",
        "    z = torch.tensor(logits_val, dtype=torch.float32, device=DEVICE)\n",
        "    ts = TemperatureScaler().to(DEVICE)\n",
        "    opt = torch.optim.LBFGS(ts.parameters(), lr=lr, max_iter=50, line_search_fn=\"strong_wolfe\")\n",
        "\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "    def closure():\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        zT = ts(z)\n",
        "        loss = bce(zT, y)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    last = 1e9\n",
        "    for _ in range(30):\n",
        "        loss = opt.step(closure)\n",
        "        if abs(loss.item()-last) < 1e-7:\n",
        "            break\n",
        "        last = loss.item()\n",
        "    with torch.no_grad():\n",
        "        T = torch.exp(ts.logT).item() + 1e-6\n",
        "    return ts, T\n",
        "\n",
        "ts, T_val = fit_temperature(log_va, y_va)\n",
        "print(\"Temperatura ajustada (VAL):\", round(T_val,4))\n",
        "\n",
        "def apply_T(logits, T):\n",
        "    return logits / (T + 1e-6)\n",
        "\n",
        "# Aplicar T\n",
        "pV_m_T = 1/(1+np.exp(-apply_T(log_va, T_val)))\n",
        "pT_m_T = 1/(1+np.exp(-apply_T(log_te, T_val)))\n",
        "\n",
        "# Recalcular pooling\n",
        "yV_mean, pV_mean, _ = patient_pool_mean(pV_m_T, y_va, pid_va)\n",
        "yT_mean, pT_mean, _ = patient_pool_mean(pT_m_T, y_te, pid_te)\n",
        "\n",
        "yV_attn, pV_attn, _ = patient_pool_attention(apply_T(log_va, T_val), y_va, pid_va, temp=1.0)\n",
        "yT_attn, pT_attn, _ = patient_pool_attention(apply_T(log_te, T_val), y_te, pid_te, temp=1.0)\n",
        "\n",
        "print(\"VAL mean (temp):\", eval_patient(yV_mean, pV_mean, 0.5))\n",
        "print(\"TEST mean(temp):\", eval_patient(yT_mean, pT_mean, 0.5))\n",
        "print(\"VAL attn (temp):\", eval_patient(yV_attn, pV_attn, 0.5))\n",
        "print(\"TEST attn(temp):\", eval_patient(yT_attn, pT_attn, 0.5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDtnpj_4buea",
        "outputId": "9fb5e8f4-859f-40d6-f36c-af843325f40c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperatura ajustada (VAL): 2.6732\n",
            "VAL mean (temp): {'AUC': np.float64(0.7481481481481482), 'PR-AUC': np.float64(0.664989747813566), 'Acc': 0.6382978723404256, 'P': 0.6, 'R': 0.45, 'thr': 0.5, 'n': 47}\n",
            "TEST mean(temp): {'AUC': np.float64(0.8759259259259259), 'PR-AUC': np.float64(0.7620865452057403), 'Acc': 0.723404255319149, 'P': 0.7333333333333333, 'R': 0.55, 'thr': 0.5, 'n': 47}\n",
            "VAL attn (temp): {'AUC': np.float64(0.75), 'PR-AUC': np.float64(0.660088903151692), 'Acc': 0.6382978723404256, 'P': 0.6153846153846154, 'R': 0.4, 'thr': 0.5, 'n': 47}\n",
            "TEST attn(temp): {'AUC': np.float64(0.8777777777777778), 'PR-AUC': np.float64(0.7617757509275546), 'Acc': 0.723404255319149, 'P': 0.7333333333333333, 'R': 0.55, 'thr': 0.5, 'n': 47}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 6: Umbral clínico (VAL recall≥0.90) y evaluación en TEST\n",
        "\n",
        "def pick_threshold_for_recall(y, p, min_recall=0.90):\n",
        "    prec, rec, thr = precision_recall_curve(y, p)\n",
        "    # precision_recall_curve devuelve thr para todos menos el primer punto\n",
        "    thr = np.append(thr, 1.0)  # para igualar longitudes\n",
        "    # buscamos el primer punto con recall >= min_recall que maximice precisión\n",
        "    mask = (rec >= min_recall)\n",
        "    if mask.any():\n",
        "        idx = np.argmax(prec[mask])\n",
        "        thr_sel = thr[mask][idx]\n",
        "        return float(thr_sel), float(prec[mask][idx]), float(rec[mask][idx])\n",
        "    else:\n",
        "        # si no hay, devolvemos el que más recall tenga\n",
        "        idx = np.argmax(rec)\n",
        "        return float(thr[idx]), float(prec[idx]), float(rec[idx])\n",
        "\n",
        "# Elegimos el *mejor pooling* en VAL (entre mean y attn tras temperature scaling) por PR-AUC\n",
        "def pr_auc(y,p):\n",
        "    return average_precision_score(y,p) if len(np.unique(y))>1 else np.nan\n",
        "\n",
        "pr_val_mean = pr_auc(yV_mean, pV_mean)\n",
        "pr_val_attn = pr_auc(yV_attn, pV_attn)\n",
        "use_attn = (pr_val_attn > pr_val_mean)\n",
        "print(f\"Comparativa VAL PR-AUC: mean={pr_val_mean:.3f} | attn={pr_val_attn:.3f} → usar {'ATTN' if use_attn else 'MEAN'}\")\n",
        "\n",
        "if use_attn:\n",
        "    thr, prec, rec = pick_threshold_for_recall(yV_attn, pV_attn, min_recall=0.90)\n",
        "    val_metrics  = eval_patient(yV_attn, pV_attn, thr)\n",
        "    test_metrics = eval_patient(yT_attn, pT_attn, thr)\n",
        "else:\n",
        "    thr, prec, rec = pick_threshold_for_recall(yV_mean, pV_mean, min_recall=0.90)\n",
        "    val_metrics  = eval_patient(yV_mean, pV_mean, thr)\n",
        "    test_metrics = eval_patient(yT_mean, pT_mean, thr)\n",
        "\n",
        "print(f\"→ Umbral clínico (VAL recall≥0.90): thr={thr:.4f} | precision={prec:.3f} | recall={rec:.3f}\")\n",
        "print(\"[VAL-final]\", val_metrics)\n",
        "print(\"[TEST-final]\", test_metrics)\n",
        "\n",
        "# Guardar resumen\n",
        "res = {\n",
        "    \"pooling_used\": \"attention\" if use_attn else \"mean\",\n",
        "    \"temperature\": T_val,\n",
        "    \"threshold\": thr,\n",
        "    \"val_metrics\": val_metrics,\n",
        "    \"test_metrics\": test_metrics\n",
        "}\n",
        "with open(f\"{OUT_DIR}/ft_effb3_patient_eval.json\",\"w\") as f:\n",
        "    json.dump(res, f, indent=2)\n",
        "print(\"Resumen guardado en:\", f\"{OUT_DIR}/ft_effb3_patient_eval.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z45kRVu6bxKZ",
        "outputId": "8169de03-8040-4d43-dd48-806d3bd403bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparativa VAL PR-AUC: mean=0.665 | attn=0.660 → usar MEAN\n",
            "→ Umbral clínico (VAL recall≥0.90): thr=0.3651 | precision=0.588 | recall=1.000\n",
            "[VAL-final] {'AUC': np.float64(0.7481481481481482), 'PR-AUC': np.float64(0.664989747813566), 'Acc': 0.7021276595744681, 'P': 0.5882352941176471, 'R': 1.0, 'thr': 0.3651449978351593, 'n': 47}\n",
            "[TEST-final] {'AUC': np.float64(0.8759259259259259), 'PR-AUC': np.float64(0.7620865452057403), 'Acc': 0.7446808510638298, 'P': 0.625, 'R': 1.0, 'thr': 0.3651449978351593, 'n': 47}\n",
            "Resumen guardado en: /content/drive/MyDrive/CognitivaAI/ft_effb3_colab/ft_effb3_patient_eval.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# 🔒 Celda robusta: lee ft_effb3_colab/*.json → genera ROC/PR/Calibración/Confusión → descarga\n",
        "# No depende de variables del notebook. Solo de los archivos que tu propio pipeline guarda.\n",
        "# ===============================================\n",
        "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, brier_score_loss, confusion_matrix\n",
        "\n",
        "BASE = \"ft_effb3_colab\"\n",
        "EVAL_JSON = os.path.join(BASE, \"ft_effb3_patient_eval.json\")\n",
        "HIST_JSON = os.path.join(BASE, \"train_history.json\")\n",
        "OUTDIR = os.path.join(BASE, \"graphs\")\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "# 0) Comprobaciones mínimas\n",
        "assert os.path.exists(EVAL_JSON), f\"No existe {EVAL_JSON}. Vuelve a ejecutar las celdas del notebook que lo generan.\"\n",
        "\n",
        "# 1) Cargar JSON de evaluación y detectar campos\n",
        "with open(EVAL_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    d = json.load(f)\n",
        "\n",
        "def _extract_patient_arrays(d):\n",
        "    # Formato A: {\"patients\":[{\"patient_id\":..,\"y_true\":..,\"y_score\"/\"logits\":..}, ...], \"threshold\":..., \"calibration\":{\"T\":...}}\n",
        "    if isinstance(d.get(\"patients\"), list) and d[\"patients\"]:\n",
        "        ids = [str(it.get(\"patient_id\", it.get(\"id\", i))) for i,it in enumerate(d[\"patients\"])]\n",
        "        y   = [int(it.get(\"y_true\", it.get(\"label\", 0))) for it in d[\"patients\"]]\n",
        "        p   = []\n",
        "        lg  = []\n",
        "        for it in d[\"patients\"]:\n",
        "            if \"y_score\" in it: p.append(float(it[\"y_score\"]))\n",
        "            elif \"prob\" in it:  p.append(float(it[\"prob\"]))\n",
        "            elif \"probs\" in it: p.append(float(it[\"probs\"]))\n",
        "            elif \"logits\" in it: lg.append(float(it[\"logits\"]))\n",
        "        return np.array(ids), np.array(y), (np.array(p) if p else None), (np.array(lg) if lg else None)\n",
        "\n",
        "    # Formato B: claves sueltas\n",
        "    if \"patient_id\" in d and \"y_true\" in d:\n",
        "        ids = np.array([str(x) for x in d[\"patient_id\"]])\n",
        "        y   = np.array(d[\"y_true\"]).astype(int)\n",
        "        p   = d.get(\"y_score\") or d.get(\"prob\") or d.get(\"probs\")\n",
        "        lg  = d.get(\"logits\")\n",
        "        p   = (np.array(p) if p is not None else None)\n",
        "        lg  = (np.array(lg) if lg is not None else None)\n",
        "        return ids, y, p, lg\n",
        "    return None, None, None, None\n",
        "\n",
        "ids, y, p, lg = _extract_patient_arrays(d)\n",
        "assert ids is not None, \"No se pudieron extraer arrays de pacientes del JSON. Imprime d y revisa sus claves.\"\n",
        "\n",
        "# Calibración por T si solo hay logits\n",
        "T = None\n",
        "if isinstance(d.get(\"calibration\"), dict) and \"T\" in d[\"calibration\"]:\n",
        "    T = float(d[\"calibration\"][\"T\"])\n",
        "for k in (\"T\",\"temperature\"):\n",
        "    if k in d and T is None:\n",
        "        T = float(d[k])\n",
        "if p is None and lg is not None:\n",
        "    T = 1.0 if T is None else T\n",
        "    p = 1.0/(1.0+np.exp(-lg/T))\n",
        "p = np.asarray(p).astype(float)\n",
        "y = np.asarray(y).astype(int)\n",
        "\n",
        "# Umbral clínico\n",
        "thr = None\n",
        "for k in (\"threshold\",\"thr\",\"best_thr\",\"clinical_threshold\"):\n",
        "    if k in d:\n",
        "        thr = float(d[k]); break\n",
        "if thr is None: thr = 0.3651\n",
        "\n",
        "# 2) Guardar CSV a nivel paciente\n",
        "csv_out = os.path.join(BASE, \"finetuning_patient_predictions.csv\")\n",
        "pd.DataFrame({\"patient_id\":ids, \"y_true\":y, \"y_score\":p}).to_csv(csv_out, index=False)\n",
        "\n",
        "# 3) Gráficas y métricas (sin estilos ni colores fijos)\n",
        "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
        "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    ece = 0.0\n",
        "    for i in range(n_bins):\n",
        "        m = (y_prob >= bins[i]) & (y_prob < bins[i+1])\n",
        "        if not np.any(m): continue\n",
        "        ece += m.mean() * abs(y_true[m].mean() - y_prob[m].mean())\n",
        "    return float(ece)\n",
        "\n",
        "def plot_roc(y_true, y_score, outpath):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure(); plt.plot(fpr, tpr, label=f\"AUC={roc_auc:.3f}\")\n",
        "    plt.plot([0,1],[0,1],'--'); plt.xlabel(\"1 - Especificidad (FPR)\"); plt.ylabel(\"Sensibilidad (TPR)\")\n",
        "    plt.title(\"ROC — Fine-Tuning EfficientNet-B3 (Paciente)\"); plt.legend(loc=\"lower right\"); plt.tight_layout()\n",
        "    plt.savefig(outpath, dpi=180); plt.close()\n",
        "    return float(roc_auc)\n",
        "\n",
        "def plot_pr(y_true, y_score, outpath):\n",
        "    prec, rec, _ = precision_recall_curve(y_true, y_score)\n",
        "    ap = average_precision_score(y_true, y_score)\n",
        "    plt.figure(); plt.plot(rec, prec, label=f\"PR-AUC={ap:.3f}\")\n",
        "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"PR — Fine-Tuning EfficientNet-B3 (Paciente)\")\n",
        "    plt.legend(loc=\"lower left\"); plt.tight_layout(); plt.savefig(outpath, dpi=180); plt.close()\n",
        "    return float(ap)\n",
        "\n",
        "def plot_calibration(y_true, y_score, outpath, n_bins=10):\n",
        "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    accs, confs = [], []\n",
        "    for i in range(n_bins):\n",
        "        m = (y_score >= bins[i]) & (y_score < bins[i+1])\n",
        "        if not np.any(m): continue\n",
        "        accs.append(float(y_true[m].mean())); confs.append(float(y_score[m].mean()))\n",
        "    plt.figure(); plt.plot([0,1],[0,1],'--'); plt.plot(confs, accs, \"o-\")\n",
        "    plt.xlabel(\"Confianza media\"); plt.ylabel(\"Frecuencia empírica\")\n",
        "    plt.title(\"Curva de Calibración — Paciente\"); plt.tight_layout(); plt.savefig(outpath, dpi=180); plt.close()\n",
        "\n",
        "def plot_confusion(y_true, y_score, thr, outpath):\n",
        "    y_pred = (y_score >= thr).astype(int)\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
        "    plt.figure(); plt.imshow(cm)\n",
        "    plt.xticks([0,1],[\"Pred 0\",\"Pred 1\"]); plt.yticks([0,1],[\"True 0\",\"True 1\"])\n",
        "    for (i,j),v in np.ndenumerate(cm): plt.text(j,i,str(v),ha=\"center\",va=\"center\")\n",
        "    plt.title(f\"Matriz de confusión (thr={thr})\"); plt.tight_layout(); plt.savefig(outpath, dpi=180); plt.close()\n",
        "    return cm\n",
        "\n",
        "roc_auc = plot_roc(y, p, os.path.join(OUTDIR, \"ft_b3_patient_roc.png\"))\n",
        "pr_auc  = plot_pr(y, p, os.path.join(OUTDIR, \"ft_b3_patient_pr.png\"))\n",
        "brier   = brier_score_loss(y, p)\n",
        "ece     = expected_calibration_error(y, p, n_bins=10)\n",
        "cm      = plot_confusion(y, p, thr, os.path.join(OUTDIR, f\"ft_b3_patient_confusion_thr{str(thr).replace('.','')}.png\"))\n",
        "plot_calibration(y, p, os.path.join(OUTDIR, \"ft_b3_patient_calibration.png\"))\n",
        "\n",
        "with open(os.path.join(OUTDIR, \"ft_b3_patient_metrics.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(f\"ROC-AUC: {roc_auc:.3f}\\nPR-AUC: {pr_auc:.3f}\\nBrier: {brier:.3f}\\nECE(10): {ece:.3f}\\n\")\n",
        "    f.write(f\"TP={int(cm[1,1])} FP={int(cm[0,1])} TN={int(cm[0,0])} FN={int(cm[1,0])} (thr={thr})\\n\")\n",
        "\n",
        "print(\"✅ Listo. CSV y gráficas en:\", BASE)\n",
        "\n",
        "# 4) Descarga directa si estás en Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(os.path.join(BASE, \"finetuning_patient_predictions.csv\"))\n",
        "    for fn in os.listdir(OUTDIR):\n",
        "        files.download(os.path.join(OUTDIR, fn))\n",
        "except Exception:\n",
        "    pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "r5Z5hPidqYwz",
        "outputId": "256f3d27-80b5-4084-c856-e9d2b033566e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "No existe ft_effb3_colab/ft_effb3_patient_eval.json. Vuelve a ejecutar las celdas del notebook que lo generan.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1616639929.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 0) Comprobaciones mínimas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEVAL_JSON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"No existe {EVAL_JSON}. Vuelve a ejecutar las celdas del notebook que lo generan.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 1) Cargar JSON de evaluación y detectar campos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: No existe ft_effb3_colab/ft_effb3_patient_eval.json. Vuelve a ejecutar las celdas del notebook que lo generan."
          ]
        }
      ]
    }
  ]
}