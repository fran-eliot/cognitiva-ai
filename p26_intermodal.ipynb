{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Uc_mVVIplAP",
        "outputId": "87ecc3d2-104d-46c4-dc80-449775305d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "BASE: /content/drive/MyDrive/CognitivaAI\n",
            "OUT : /content/drive/MyDrive/CognitivaAI/p26_intermodal\n"
          ]
        }
      ],
      "source": [
        "# Celda 0 — Montar Drive y definir rutas\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "BASE = Path(\"/content/drive/MyDrive/CognitivaAI\")\n",
        "OUT  = BASE/\"p26_intermodal\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"BASE:\", BASE)\n",
        "print(\"OUT :\", OUT)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1 — Utilidades generales\n",
        "import json, numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
        "\n",
        "def clean_cols(df):\n",
        "    df.columns = [str(c).replace(\"\\ufeff\",\"\").strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "def infer_cohort(pid):\n",
        "    s = str(pid).strip().upper()\n",
        "    if s.startswith(\"OAS1\"): return \"OAS1\"\n",
        "    if s.startswith(\"OAS2\"): return \"OAS2\"\n",
        "    return \"OAS1\"\n",
        "\n",
        "LEAK_PATTERNS = [\"cdr\",\"dement\",\"dx\",\"diagnos\",\"group\",\"converted\",\"label\",\"target\",\"y_true\",\"y\"]\n",
        "\n",
        "def is_leak_col(name):\n",
        "    s = str(name).lower()\n",
        "    return any(p in s for p in LEAK_PATTERNS) or s in {\"patient_id\",\"cohort\"}\n",
        "\n",
        "def metrics_from_scores(y, p):\n",
        "    y = np.asarray(y).astype(int); p = np.asarray(p).astype(float)\n",
        "    has_var = len(np.unique(y))>1\n",
        "    return dict(\n",
        "        AUC   = float(roc_auc_score(y,p)) if has_var else float(\"nan\"),\n",
        "        PRAUC = float(average_precision_score(y,p)) if has_var else float(\"nan\"),\n",
        "        Brier = float(brier_score_loss(y,p))\n",
        "    )\n",
        "\n",
        "def choose_thr_cost(y, p, C_FN=5.0, C_FP=1.0, n=1001):\n",
        "    y = np.asarray(y).astype(int); p = np.asarray(p).astype(float)\n",
        "    thr = np.linspace(0,1,n)\n",
        "    best = None\n",
        "    for t in thr:\n",
        "        yhat = (p>=t).astype(int)\n",
        "        FP = int(((yhat==1)&(y==0)).sum())\n",
        "        FN = int(((yhat==0)&(y==1)).sum())\n",
        "        cost = C_FN*FN + C_FP*FP\n",
        "        if best is None or cost < best[0]:\n",
        "            best = (cost, t, FP, FN)\n",
        "    cost, t, FP, FN = best\n",
        "    TP = int(((p>=t)&(y==1)).sum()); TN = int(((p< t)&(y==0)).sum())\n",
        "    prec = TP/(TP+FP) if (TP+FP)>0 else float(\"nan\")\n",
        "    rec  = TP/(TP+FN) if (TP+FN)>0 else float(\"nan\")\n",
        "    acc  = (TP+TN)/(TP+TN+FP+FN)\n",
        "    return dict(thr=float(t), cost=float(cost), TP=TP, FP=FP, TN=TN, FN=FN, Precision=float(prec), Recall=float(rec), Acc=float(acc))\n",
        "\n",
        "def to_patient_id(id_val, cohort):\n",
        "    s = str(id_val).strip().replace(\"\\u200b\",\"\").replace(\"\\ufeff\",\"\")\n",
        "    if s.upper().startswith((\"OAS1_\",\"OAS2_\")):\n",
        "        return s.upper()\n",
        "    # numérico -> zero-pad 4\n",
        "    if s.isdigit():\n",
        "        return f\"{cohort}_{int(s):04d}\"\n",
        "    s2 = s.replace(\"-\", \"_\").upper()\n",
        "    if not s2.startswith((\"OAS1_\",\"OAS2_\")):\n",
        "        s2 = f\"{cohort}_{s2}\"\n",
        "    return s2\n"
      ],
      "metadata": {
        "id": "WDCF6q5Rp7rM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 2 — Construir clínico consolidado desde Excels (OASIS-1 y OASIS-2)\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "search_dirs = [\n",
        "    BASE/\"clinical\"/\"raw\",\n",
        "    BASE/\"clinical\",\n",
        "    BASE,\n",
        "    BASE.parent,  # ← carpeta padre del proyecto\n",
        "]\n",
        "xls_files = []\n",
        "for d in search_dirs:\n",
        "    if d.exists():\n",
        "        xls_files += list(d.glob(\"*.xlsx\")) + list(d.glob(\"*.xls\"))\n",
        "\n",
        "if not xls_files:\n",
        "    raise FileNotFoundError(\n",
        "        f\"No encontré Excels clínicos en {search_dirs}. \"\n",
        "        \"Sube los ficheros OASIS-1 (cross-sectional) y OASIS-2 (longitudinal).\"\n",
        "    )\n",
        "\n",
        "def load_xls(p):\n",
        "    try:\n",
        "        df = pd.read_excel(p)\n",
        "    except Exception:\n",
        "        # fallback\n",
        "        df = pd.read_excel(p, engine=\"openpyxl\")\n",
        "    df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "df1_raw, df2_raw = None, None\n",
        "for p in xls_files:\n",
        "    df = load_xls(p)\n",
        "    cols = set(c.lower() for c in df.columns)\n",
        "    if {\"subject id\",\"mri id\",\"group\",\"visit\"}.issubset(cols):\n",
        "        df2_raw = df\n",
        "    elif {\"id\",\"m/f\",\"mmse\",\"cdr\"}.issubset(cols) or {\"id\",\"sex\",\"mmse\",\"cdr\"}.issubset(cols):\n",
        "        df1_raw = df\n",
        "\n",
        "assert df1_raw is not None and df2_raw is not None, \"No pude distinguir cuál es OASIS-1 y cuál OASIS-2.\"\n",
        "\n",
        "# Renombrado estándar\n",
        "df1 = df1_raw.rename(columns={\n",
        "    \"ID\":\"ID\", \"M/F\":\"Sex\", \"Educ\":\"Education\", \"Hand\":\"Hand\", \"Delay\":\"Delay\"\n",
        "})\n",
        "df2 = df2_raw.rename(columns={\n",
        "    \"Subject ID\":\"ID\", \"M/F\":\"Sex\", \"EDUC\":\"Education\", \"MR Delay\":\"Delay\"\n",
        "})\n",
        "\n",
        "df1[\"Cohort\"] = \"OASIS1\"\n",
        "df2[\"Cohort\"] = \"OASIS2\"\n",
        "\n",
        "# Target de referencia (NO se usará como feature)\n",
        "if \"Group\" in df2.columns:\n",
        "    df2[\"Target\"] = df2[\"Group\"].replace({\"Nondemented\":0, \"Demented\":1, \"Converted\":1})\n",
        "else:\n",
        "    df2[\"Target\"] = np.nan\n",
        "\n",
        "df1[\"Target\"] = df1[\"CDR\"].apply(lambda x: 0 if x==0 else 1)\n",
        "\n",
        "# OASIS-2: conservar primera visita por paciente\n",
        "if \"Visit\" in df2.columns:\n",
        "    df2 = df2.sort_values([\"ID\",\"Visit\"]).groupby(\"ID\").first().reset_index()\n",
        "\n",
        "cols_common = [\"ID\",\"Age\",\"Sex\",\"Education\",\"SES\",\"MMSE\",\"CDR\",\"eTIV\",\"nWBV\",\"ASF\",\"Target\",\"Delay\"]\n",
        "df1c = df1.reindex(columns=[c for c in cols_common if c in df1.columns]).copy()\n",
        "df2c = df2.reindex(columns=[c for c in cols_common if c in df2.columns]).copy()\n",
        "df1c[\"Cohort\"]=\"OASIS1\"; df2c[\"Cohort\"]=\"OASIS2\"\n",
        "df_all = pd.concat([df1c, df2c], ignore_index=True)\n",
        "\n",
        "# Imputación suave en Education/SES si existen\n",
        "if \"Education\" in df_all.columns:\n",
        "    df_all[\"Education\"] = pd.to_numeric(df_all[\"Education\"], errors=\"coerce\")\n",
        "    df_all[\"Education\"].fillna(df_all[\"Education\"].median(), inplace=True)\n",
        "if \"SES\" in df_all.columns:\n",
        "    df_all[\"SES\"] = pd.to_numeric(df_all[\"SES\"], errors=\"coerce\")\n",
        "    df_all[\"SES\"].fillna(df_all[\"SES\"].median(), inplace=True)\n",
        "\n",
        "# patient_id compatible con P24\n",
        "df_all[\"patient_id\"] = [to_patient_id(i, c) for i,c in zip(df_all[\"ID\"], df_all[\"Cohort\"])]\n",
        "\n",
        "# Anti-fuga: quitar columnas proxy de etiqueta\n",
        "leak_cols = [c for c in [\"Target\",\"CDR\",\"Group\"] if c in df_all.columns]\n",
        "clin_features = df_all.drop(columns=[\"ID\",\"Cohort\"] + leak_cols, errors=\"ignore\").copy()\n",
        "if \"Sex\" in clin_features.columns:\n",
        "    clin_features[\"Sex\"] = clin_features[\"Sex\"].astype(str).str.strip()\n",
        "\n",
        "# Dejar 1 fila por paciente\n",
        "clin_features = clin_features.drop_duplicates(subset=[\"patient_id\"]).reset_index(drop=True)\n",
        "\n",
        "# Guardar consolidado\n",
        "clin_path = OUT/\"p26_clinical_consolidado.csv\"\n",
        "clin_features.to_csv(clin_path, index=False)\n",
        "print(\"✅ Clínico consolidado:\", clin_features.shape, \"->\", clin_path)\n",
        "print(\"Columnas clínicas:\", clin_features.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTFljc_Mp-5F",
        "outputId": "39560070-4149-467a-efe2-c295823bc3f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Clínico consolidado: (586, 10) -> /content/drive/MyDrive/CognitivaAI/p26_intermodal/p26_clinical_consolidado.csv\n",
            "Columnas clínicas: ['Age', 'Sex', 'Education', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF', 'Delay', 'patient_id']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2162075536.py:55: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df2[\"Target\"] = df2[\"Group\"].replace({\"Nondemented\":0, \"Demented\":1, \"Converted\":1})\n",
            "/tmp/ipython-input-2162075536.py:74: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_all[\"Education\"].fillna(df_all[\"Education\"].median(), inplace=True)\n",
            "/tmp/ipython-input-2162075536.py:77: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_all[\"SES\"].fillna(df_all[\"SES\"].median(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 3 — Cargar P24 (imagen) y el consolidado clínico\n",
        "p24_val = clean_cols(pd.read_csv(BASE/\"p24_meta_simple\"/\"p24_val_preds.csv\"))\n",
        "p24_tst = clean_cols(pd.read_csv(BASE/\"p24_meta_simple\"/\"p24_test_preds.csv\"))\n",
        "\n",
        "# Cohort si falta\n",
        "for df in (p24_val, p24_tst):\n",
        "    if \"cohort\" not in df.columns:\n",
        "        df[\"cohort\"] = df[\"patient_id\"].map(infer_cohort)\n",
        "\n",
        "# Lista de 56 features de imagen (P24)\n",
        "coef_df = pd.read_csv(BASE/\"p24_meta_simple\"/\"p24_coefficients.csv\")\n",
        "IMG_FEATS = coef_df[\"feature\"].tolist()\n",
        "print(f\"Features imagen (P24): {len(IMG_FEATS)} columnas\")\n",
        "\n",
        "# Clínico consolidado\n",
        "clin = clean_cols(pd.read_csv(OUT/\"p26_clinical_consolidado.csv\"))\n",
        "assert \"patient_id\" in clin.columns, \"El consolidado clínico debe tener 'patient_id'.\"\n",
        "\n",
        "# Merge con VAL/TEST de P24\n",
        "val = p24_val.merge(clin, on=\"patient_id\", how=\"left\", suffixes=(\"\",\"_clin\"))\n",
        "tst = p24_tst.merge(clin, on=\"patient_id\", how=\"left\", suffixes=(\"\",\"_clin\"))\n",
        "\n",
        "# Columnas clínicas finales (excluyendo patient_id)\n",
        "CLIN_COLS = [c for c in clin.columns if c!=\"patient_id\"]\n",
        "print(f\"VAL: {val.shape} | TEST: {tst.shape} | #clin_cols={len(CLIN_COLS)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ol48QjoNqtFa",
        "outputId": "a6ac37e3-0891-4f21-ed12-6c71cec989cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features imagen (P24): 56 columnas\n",
            "VAL: (69, 13) | TEST: (70, 13) | #clin_cols=9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 4 — Separar matrices IMG / CLIN y vectores\n",
        "def split_xy(df):\n",
        "    X_img = df.reindex(columns=IMG_FEATS, fill_value=np.nan)\n",
        "    y     = df[\"y_true\"].astype(int).values\n",
        "    coh   = df[\"cohort\"].astype(str).values\n",
        "    y_img = df[\"y_prob\"].astype(float).values  # proba calibrada de P24\n",
        "    return X_img, y, coh, y_img\n",
        "\n",
        "X_img_val, y_val, coh_val, yimg_val = split_xy(val)\n",
        "X_img_tst, y_tst, coh_tst, yimg_tst = split_xy(tst)\n",
        "\n",
        "X_clin_val = val[CLIN_COLS].copy()\n",
        "X_clin_tst = tst[CLIN_COLS].copy()\n",
        "\n",
        "print(f\"VAL: X_img={X_img_val.shape}, X_clin={X_clin_val.shape}\")\n",
        "print(f\"TEST: X_img={X_img_tst.shape}, X_clin={X_clin_tst.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZkqrqGsqyBu",
        "outputId": "4aa7ba08-ef8b-4f43-d914-d65ee78555a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL: X_img=(69, 56), X_clin=(69, 9)\n",
            "TEST: X_img=(70, 56), X_clin=(70, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 5 — Modelo clínico LR-EN (Repeated Stratified KFold)\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clin_num = [c for c in CLIN_COLS if X_clin_val[c].dtype!=object]\n",
        "clin_cat = [c for c in CLIN_COLS if X_clin_val[c].dtype==object]\n",
        "\n",
        "pre_clin = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "                          (\"scaler\", StandardScaler())]), clin_num),\n",
        "        (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "                          (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]), clin_cat),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    sparse_threshold=0.3\n",
        ")\n",
        "\n",
        "clf_clin = Pipeline(steps=[\n",
        "    (\"pre\", pre_clin),\n",
        "    (\"lr\", LogisticRegression(penalty=\"elasticnet\", solver=\"saga\",\n",
        "                              l1_ratio=0.5, C=0.5, max_iter=5000))\n",
        "])\n",
        "\n",
        "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
        "oof = np.zeros(len(y_val), dtype=float)\n",
        "for tr, va in rskf.split(X_clin_val, y_val):\n",
        "    m = clf_clin\n",
        "    m.fit(X_clin_val.iloc[tr], y_val[tr])\n",
        "    oof[va] = m.predict_proba(X_clin_val.iloc[va])[:,1]\n",
        "\n",
        "clf_clin.fit(X_clin_val, y_val)\n",
        "p_clin_tst = clf_clin.predict_proba(X_clin_tst)[:,1]\n",
        "\n",
        "m_clin_val = metrics_from_scores(y_val, oof)\n",
        "m_clin_tst = metrics_from_scores(y_tst, p_clin_tst)\n",
        "print(\"CLÍNICO OOF VAL:\", m_clin_val)\n",
        "print(\"CLÍNICO TEST  :\", m_clin_tst)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KmWUc0Cq0VA",
        "outputId": "7665a101-669f-4a54-8e07-61210b4b1c4e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLÍNICO OOF VAL: {'AUC': 0.58276740237691, 'PRAUC': 0.5248918812433309, 'Brier': 0.25712928295173343}\n",
            "CLÍNICO TEST  : {'AUC': 0.5863486842105263, 'PRAUC': 0.5599631358460779, 'Brier': 0.23997569158513588}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 6_alt_v3 — Reintento con artefacto p3 (sin fuga y sin \"drop\" de columnas all-NaN)\n",
        "import pandas as pd, numpy as np, pickle, joblib, json, warnings\n",
        "from pathlib import Path\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"InconsistentVersionWarning\")\n",
        "\n",
        "def find_first(cand_dirs, names):\n",
        "    for d in cand_dirs:\n",
        "        for n in names:\n",
        "            p = d/n\n",
        "            if p.exists():\n",
        "                return p\n",
        "    return None\n",
        "\n",
        "cand_dirs = [\n",
        "    BASE/\"clinical\"/\"final_models\",\n",
        "    BASE/\"artifacts\"/\"clinic\"/\"final_models\",\n",
        "    BASE/\"clinical\",\n",
        "    BASE/\"artifacts\"/\"clinic\",\n",
        "    BASE,\n",
        "    BASE.parent/\"artifacts\"/\"clinic\"/\"final_models\",\n",
        "    BASE.parent\n",
        "]\n",
        "model_priority = [\n",
        "    \"model_lr_isotonic.pkl\",\"model_lr_balanced.pkl\",\n",
        "    \"model_xgb_isotonic.pkl\",\"model_xgb_balanced.pkl\",\n",
        "    \"model_rf_isotonic.pkl\",\"model_rf_balanced.pkl\",\n",
        "]\n",
        "model_path = find_first(cand_dirs, model_priority)\n",
        "\n",
        "# deployment_config.json (opcional)\n",
        "dep_cfg = find_first([BASE/\"clinical\", BASE/\"artifacts\"/\"clinic\", BASE, BASE.parent], [\"deployment_config.json\"])\n",
        "if dep_cfg and dep_cfg.exists():\n",
        "    try:\n",
        "        cfg = json.load(open(dep_cfg))\n",
        "        preferred = cfg.get(\"selected_model\") or cfg.get(\"model_name\")\n",
        "        if preferred:\n",
        "            p2 = find_first(cand_dirs, [preferred])\n",
        "            if p2 and p2.exists():\n",
        "                model_path = p2\n",
        "                print(\"ℹ️ deployment_config.json ->\", preferred)\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ deployment_config.json:\", e)\n",
        "\n",
        "print(\"📦 Modelo clínico candidato:\", model_path if model_path else \"no encontrado\")\n",
        "\n",
        "feat_path = find_first([BASE/\"clinical\", BASE/\"artifacts\"/\"clinic\", BASE, BASE.parent], [\"feature_columns.joblib\"])\n",
        "feature_cols = None\n",
        "if feat_path and Path(feat_path).exists():\n",
        "    try:\n",
        "        feature_cols = [str(c) for c in joblib.load(feat_path)]\n",
        "        # Anti-fuga por si en joblib venían estas columnas:\n",
        "        feature_cols = [c for c in feature_cols if str(c).lower() not in {\"cdr\",\"group\",\"target\",\"y\",\"label\"}]\n",
        "        print(\"🧩 feature_columns.joblib (#cols tras anti-fuga):\", len(feature_cols))\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ feature_columns.joblib:\", e)\n",
        "\n",
        "def try_load_model(p):\n",
        "    if p is None: return None\n",
        "    for loader in (joblib.load, lambda x: pickle.load(open(x,\"rb\")),\n",
        "                   lambda x: pickle.load(open(x,\"rb\"), fix_imports=True, encoding=\"latin1\")):\n",
        "        try:\n",
        "            return loader(p)\n",
        "        except Exception as e:\n",
        "            print(\"loader falló:\", type(e).__name__, \"-\", e)\n",
        "    return None\n",
        "\n",
        "model_clin = try_load_model(model_path)\n",
        "\n",
        "# Construir diseño SIN perder columnas (relleno constante para all-NaN)\n",
        "def build_design(df_clin, expected_cols):\n",
        "    if expected_cols is None:\n",
        "        expected_cols = list(df_clin.columns)\n",
        "    # Intersección + añade las que falten como NaN\n",
        "    cols_in = [c for c in expected_cols if c in df_clin.columns]\n",
        "    cols_out = [c for c in expected_cols if c not in df_clin.columns]\n",
        "    X = df_clin.reindex(columns=cols_in, fill_value=np.nan).copy()\n",
        "    # Rellenar all-NaN con 0 (evita drop del imputer por \"sin observaciones\")\n",
        "    for c in X.columns:\n",
        "        col = X[c]\n",
        "        if (col.isna().all()):\n",
        "            X[c] = 0.0\n",
        "        elif col.dtype==object:\n",
        "            X[c] = col.astype(str)\n",
        "    # Añade explícitamente las ausentes como 0\n",
        "    for c in cols_out:\n",
        "        X[c] = 0.0\n",
        "    return X[expected_cols].copy()\n",
        "\n",
        "# Construye matrices con anti-fuga para el modelo (usa sólo CLIN_COLS conocidos)\n",
        "expected = feature_cols if feature_cols is not None else CLIN_COLS\n",
        "X_val_p3_raw = build_design(X_clin_val, expected)\n",
        "X_tst_p3_raw = build_design(X_clin_tst, expected)\n",
        "\n",
        "def predict_proba_robust(m, X):\n",
        "    try:\n",
        "        return m.predict_proba(X)[:,1]\n",
        "    except Exception as e1:\n",
        "        # fallback ligero numérico\n",
        "        X2 = X.copy()\n",
        "        num_cols = [c for c in X2.columns if X2[c].dtype != object]\n",
        "        if num_cols:\n",
        "            imp = SimpleImputer(strategy=\"constant\", fill_value=0.0)  # ← clave: constante, no median\n",
        "            X2[num_cols] = imp.fit_transform(X2[num_cols])\n",
        "            sca = StandardScaler(with_mean=False)\n",
        "            X2[num_cols] = sca.fit_transform(X2[num_cols])\n",
        "        try:\n",
        "            return m.predict_proba(X2)[:,1]\n",
        "        except Exception as e2:\n",
        "            raise RuntimeError(f\"predict_proba fallo: {e1} | {e2}\")\n",
        "\n",
        "CLIN_OUT = BASE/\"clinical\"; CLIN_OUT.mkdir(parents=True, exist_ok=True)\n",
        "p3_csv = CLIN_OUT/\"p3_clinical_probs.csv\"\n",
        "\n",
        "if model_clin is not None:\n",
        "    try:\n",
        "        p_val_p3 = predict_proba_robust(model_clin, X_val_p3_raw)\n",
        "        p_tst_p3 = predict_proba_robust(model_clin, X_tst_p3_raw)\n",
        "        p3_df = pd.concat([\n",
        "            pd.DataFrame({\"patient_id\": val[\"patient_id\"], \"split\":\"VAL\",  \"y_prob_clin\": p_val_p3}),\n",
        "            pd.DataFrame({\"patient_id\": tst[\"patient_id\"], \"split\":\"TEST\", \"y_prob_clin\": p_tst_p3}),\n",
        "        ], ignore_index=True)\n",
        "        p3_df.to_csv(p3_csv, index=False)\n",
        "        print(\"💾 Guardado (artefacto):\", p3_csv, \"→\", p3_df.shape)\n",
        "        # Override en memoria\n",
        "        mv = val.merge(p3_df[p3_df[\"split\"]==\"VAL\"][[\"patient_id\",\"y_prob_clin\"]], on=\"patient_id\", how=\"left\")\n",
        "        mt = tst.merge(p3_df[p3_df[\"split\"]==\"TEST\"][[\"patient_id\",\"y_prob_clin\"]], on=\"patient_id\", how=\"left\")\n",
        "        oof = mv[\"y_prob_clin\"].to_numpy()\n",
        "        p_clin_tst = mt[\"y_prob_clin\"].to_numpy()\n",
        "        print(\"🔁 Override aplicado desde artefacto p3.\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Artefacto no usable para predicción segura:\", e)\n",
        "        # Fall-back definitivo: usa Celda 5\n",
        "        p3_df = pd.concat([\n",
        "            pd.DataFrame({\"patient_id\": val[\"patient_id\"], \"split\":\"VAL\",  \"y_prob_clin\": oof}),\n",
        "            pd.DataFrame({\"patient_id\": tst[\"patient_id\"], \"split\":\"TEST\", \"y_prob_clin\": p_clin_tst}),\n",
        "        ], ignore_index=True)\n",
        "        p3_df.to_csv(p3_csv, index=False)\n",
        "        print(\"💾 Guardado (fallback Celda 5):\", p3_csv, \"→\", p3_df.shape)\n",
        "else:\n",
        "    print(\"⚠️ No se pudo cargar el modelo p3. Se mantiene el fallback de Celda 5 (ya guardado).\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9n71OzRq54V",
        "outputId": "43918010-5c54-4f6d-be96-6570213d7c1d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Modelo clínico candidato: /content/drive/MyDrive/CognitivaAI/clinical/final_models/model_lr_isotonic.pkl\n",
            "🧩 feature_columns.joblib (#cols tras anti-fuga): 8\n",
            "⚠️ Artefacto no usable para predicción segura: predict_proba fallo: The feature names should match those that were passed during fit.\n",
            "Feature names seen at fit time, yet now missing:\n",
            "- CDR\n",
            " | The feature names should match those that were passed during fit.\n",
            "Feature names seen at fit time, yet now missing:\n",
            "- CDR\n",
            "\n",
            "💾 Guardado (fallback Celda 5): /content/drive/MyDrive/CognitivaAI/clinical/p3_clinical_probs.csv → (139, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.7.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.7.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator IsotonicRegression from version 1.7.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator CalibratedClassifierCV from version 1.7.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "p3_csv = BASE/\"clinical\"/\"p3_clinical_probs.csv\"\n",
        "assert p3_csv.exists(), \"No existe p3_clinical_probs.csv (fallback). Vuelve a ejecutar la Celda 6_alt_v2/v3.\"\n",
        "\n",
        "p3 = pd.read_csv(p3_csv)\n",
        "print(\"p3_clinical_probs.csv:\", p3.shape, \"filas · splits:\", p3[\"split\"].value_counts().to_dict())\n",
        "\n",
        "# Comprobamos merge con VAL/TEST de P24\n",
        "mv = val.merge(p3[p3[\"split\"]==\"VAL\"][[\"patient_id\",\"y_prob_clin\"]], on=\"patient_id\", how=\"left\")\n",
        "mt = tst.merge(p3[p3[\"split\"]==\"TEST\"][[\"patient_id\",\"y_prob_clin\"]], on=\"patient_id\", how=\"left\")\n",
        "print(\"VAL merge OK:\", mv[\"y_prob_clin\"].notna().mean(), \"TEST merge OK:\", mt[\"y_prob_clin\"].notna().mean())\n",
        "\n",
        "# Si todo OK, seguimos con las celdas 7→11\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H44hqhrv5tV",
        "outputId": "0fc55e1a-6ab9-47b4-ff32-b396d5a89cdf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p3_clinical_probs.csv: (139, 3) filas · splits: {'TEST': 70, 'VAL': 69}\n",
            "VAL merge OK: 1.0 TEST merge OK: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 7_gen_p1 — Generar p1_oas2_img_probs.csv desde artefactos p13/p14 (si existen)\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def _clean_cols(df):\n",
        "    df.columns = [str(c).replace(\"\\ufeff\",\"\").strip() for c in df.columns];\n",
        "    return df\n",
        "\n",
        "def _to_patient_id(id_val, cohort=\"OAS2\"):\n",
        "    s = str(id_val).strip().replace(\"\\u200b\",\"\").replace(\"\\ufeff\",\"\")\n",
        "    if s.upper().startswith((\"OAS1_\",\"OAS2_\")): return s.upper()\n",
        "    if s.isdigit(): return f\"{cohort}_{int(s):04d}\"\n",
        "    s2 = s.replace(\"-\", \"_\").upper()\n",
        "    if not s2.startswith((\"OAS1_\",\"OAS2_\")): s2 = f\"{cohort}_{s2}\"\n",
        "    return s2\n",
        "\n",
        "def _pick_prob_col(df):\n",
        "    # candidatos habituales\n",
        "    cand = [c for c in df.columns if str(c).lower() in\n",
        "            {\"y_prob\",\"prob\",\"yprob\",\"pred_prob\",\"prob1\",\"score\",\"y_hat\",\"yhat\",\"p\",\"proba\"}]\n",
        "    cand = [c for c in cand if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    nums = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    # filtra etiquetas conocidas\n",
        "    bad = {\"y_true\",\"label\",\"target\",\"cdr\",\"group\",\"diagnosis\",\"dx\"}\n",
        "    nums = [c for c in nums if str(c).lower() not in bad]\n",
        "    # dentro de [0,1] mayormente\n",
        "    def in01(series):\n",
        "        s = series.dropna()\n",
        "        return len(s)>0 and (s.between(0,1).mean()>=0.95)\n",
        "    for c in cand:\n",
        "        if in01(df[c]): return c\n",
        "    for c in nums:\n",
        "        if in01(df[c]): return c\n",
        "    return None\n",
        "\n",
        "# 1) Candidatos\n",
        "cands_dirs = [\n",
        "    BASE/\"p14_oasis2_images\",\n",
        "    BASE/\"p13_oasis2_images\",\n",
        "    BASE.parent/\"p14_oasis2_images\",\n",
        "    BASE.parent/\"p13_oasis2_images\",\n",
        "]\n",
        "cand_files = []\n",
        "for d in cands_dirs:\n",
        "    if d.exists():\n",
        "        cand_files += list(d.glob(\"*patient_preds*.csv\")) + list(d.glob(\"*patient_features*.csv\"))\n",
        "if not cand_files:\n",
        "    print(\"ℹ️ No encontré CSV de p13/p14. Continuo sin p1 (no pasa nada).\")\n",
        "    p1_val = np.full(len(val), np.nan); p1_tst = np.full(len(tst), np.nan)\n",
        "else:\n",
        "    frames = []\n",
        "    for f in cand_files:\n",
        "        try:\n",
        "            df = _clean_cols(pd.read_csv(f))\n",
        "            # Detecta id de paciente\n",
        "            idcol = None\n",
        "            for k in [\"patient_id\",\"patient\",\"id\",\"subject id\",\"subject_id\",\"ID\"]:\n",
        "                if k in map(str.lower, df.columns):\n",
        "                    # obtener nombre real respetando mayúsculas\n",
        "                    idcol = [c for c in df.columns if c.lower()==k][0]; break\n",
        "            if idcol is None:\n",
        "                continue\n",
        "            df[\"patient_id\"] = df[idcol].astype(str).map(lambda x: _to_patient_id(x, \"OAS2\"))\n",
        "            # Detecta prob\n",
        "            pcol = _pick_prob_col(df)\n",
        "            if pcol is None:\n",
        "                continue\n",
        "            sub = df[[\"patient_id\", pcol]].rename(columns={pcol:\"y_prob_img\"})\n",
        "            # Solo OAS2 + prob válida\n",
        "            sub = sub[sub[\"patient_id\"].str.upper().str.startswith(\"OAS2_\")]\n",
        "            sub = sub.dropna(subset=[\"y_prob_img\"])\n",
        "            frames.append(sub)\n",
        "            print(f\"↳ {f.name}: {len(sub)} filas con y_prob_img\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ {f.name}: {e}\")\n",
        "\n",
        "    if not frames:\n",
        "        print(\"ℹ️ No logré extraer probabilidades de los CSVs. Sigo sin p1.\")\n",
        "        p1_val = np.full(len(val), np.nan); p1_tst = np.full(len(tst), np.nan)\n",
        "    else:\n",
        "        p1 = pd.concat(frames, ignore_index=True)\n",
        "        # Si hay duplicados por paciente, promediamos\n",
        "        p1 = p1.groupby(\"patient_id\", as_index=False)[\"y_prob_img\"].mean()\n",
        "        # Guardar\n",
        "        CLIN = BASE/\"clinical\"; CLIN.mkdir(parents=True, exist_ok=True)\n",
        "        out_csv = CLIN/\"p1_oas2_img_probs.csv\"\n",
        "        p1.to_csv(out_csv, index=False)\n",
        "        print(\"💾 Guardado:\", out_csv, \"→\", p1.shape)\n",
        "\n",
        "        # Alinear con VAL/TEST actuales\n",
        "        mv = val[[\"patient_id\"]].merge(p1, on=\"patient_id\", how=\"left\")\n",
        "        mt = tst[[\"patient_id\"]].merge(p1, on=\"patient_id\", how=\"left\")\n",
        "        p1_val = mv[\"y_prob_img\"].to_numpy()\n",
        "        p1_tst = mt[\"y_prob_img\"].to_numpy()\n",
        "\n",
        "        # Diagnóstico de cobertura\n",
        "        m_val = (~np.isnan(p1_val)).mean()\n",
        "        m_tst = (~np.isnan(p1_tst)).mean()\n",
        "        print(f\"Coverage VAL OAS2 p1: {m_val:.2%} | TEST OAS2 p1: {m_tst:.2%} (NaN en OAS1 por diseño)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLtsFAwxxzfC",
        "outputId": "579d7db5-bacf-4bd4-e37f-4322ce99534f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "↳ val_patient_preds_oas2_effb3_p14.csv: 22 filas con y_prob_img\n",
            "↳ test_patient_preds_oas2_effb3_p14.csv: 23 filas con y_prob_img\n",
            "↳ val_patient_features_oas2_effb3_p14.csv: 22 filas con y_prob_img\n",
            "↳ test_patient_features_oas2_effb3_p14.csv: 23 filas con y_prob_img\n",
            "↳ val_patient_preds_oas2_effb3.csv: 22 filas con y_prob_img\n",
            "↳ test_patient_preds_oas2_effb3.csv: 23 filas con y_prob_img\n",
            "↳ val_patient_features_oas2_effb3.csv: 2 filas con y_prob_img\n",
            "↳ test_patient_features_oas2_effb3.csv: 3 filas con y_prob_img\n",
            "💾 Guardado: /content/drive/MyDrive/CognitivaAI/clinical/p1_oas2_img_probs.csv → (49, 2)\n",
            "Coverage VAL OAS2 p1: 31.88% | TEST OAS2 p1: 32.86% (NaN en OAS1 por diseño)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sustituye el contenido de tu Celda 7 (o añádelo tras la 7_gen_p1 que ya corriste)\n",
        "\n",
        "import numpy as np\n",
        "# Si no existe p1_val/p1_tst, definelos como NaN (por si vienes del fallback)\n",
        "if 'p1_val' not in globals(): p1_val = np.full(len(val), np.nan)\n",
        "if 'p1_tst' not in globals(): p1_tst = np.full(len(tst), np.nan)\n",
        "\n",
        "# Máscaras de cohorte\n",
        "mask_oas2_val = (val[\"cohort\"].values == \"OAS2\")\n",
        "mask_oas2_tst = (tst[\"cohort\"].values == \"OAS2\")\n",
        "\n",
        "# Flag de presencia\n",
        "p1_has_val = ~np.isnan(p1_val)\n",
        "p1_has_tst = ~np.isnan(p1_tst)\n",
        "\n",
        "# Media OAS2 en VAL (¡solo VAL para evitar fuga!)\n",
        "m_oas2_val = np.nanmean(p1_val[mask_oas2_val]) if np.any(mask_oas2_val) else 0.5\n",
        "\n",
        "# Imputación coherente\n",
        "p1_fill_val = p1_val.copy()\n",
        "p1_fill_tst = p1_tst.copy()\n",
        "\n",
        "# En OAS2: faltantes → media VAL OAS2\n",
        "p1_fill_val[~p1_has_val & mask_oas2_val] = m_oas2_val\n",
        "p1_fill_tst[~p1_has_tst & mask_oas2_tst] = m_oas2_val  # usa SIEMPRE la media de VAL\n",
        "\n",
        "# En OAS1: todo p1 es ausente → neutral 0.5\n",
        "p1_fill_val[~mask_oas2_val] = 0.5\n",
        "p1_fill_tst[~mask_oas2_tst] = 0.5\n",
        "\n",
        "print(f\"p1 imputado: m_oas2_val={m_oas2_val:.3f} | has_val={(p1_has_val).mean():.2%} | has_tst={(p1_has_tst).mean():.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKDg5nAWy2HA",
        "outputId": "1f025afb-e335-416c-f1f1-80ff33008098"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p1 imputado: m_oas2_val=0.575 | has_val=31.88% | has_tst=32.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "def stack_cols(*cols):\n",
        "    cols = [c.reshape(-1,1) if c.ndim==1 else c for c in cols]\n",
        "    return np.hstack(cols)\n",
        "\n",
        "# Variante A: SIN p1\n",
        "X_metaA_val = stack_cols(yimg_val, oof)\n",
        "X_metaA_tst = stack_cols(yimg_tst, p_clin_tst)\n",
        "metaA = LogisticRegression(penalty=None, solver=\"lbfgs\", max_iter=1000).fit(X_metaA_val, y_val)\n",
        "pA_val = metaA.predict_proba(X_metaA_val)[:,1]\n",
        "pA_tst = metaA.predict_proba(X_metaA_tst)[:,1]\n",
        "mA_val = metrics_from_scores(y_val, pA_val)\n",
        "mA_tst = metrics_from_scores(y_tst, pA_tst)\n",
        "\n",
        "# Variante B: CON p1 (p1_fill + flag p1_has)\n",
        "X_metaB_val = stack_cols(yimg_val, oof, p1_fill_val, p1_has_val.astype(float))\n",
        "X_metaB_tst = stack_cols(yimg_tst, p_clin_tst, p1_fill_tst, p1_has_tst.astype(float))\n",
        "metaB = LogisticRegression(penalty=None, solver=\"lbfgs\", max_iter=1000).fit(X_metaB_val, y_val)\n",
        "pB_val = metaB.predict_proba(X_metaB_val)[:,1]\n",
        "pB_tst = metaB.predict_proba(X_metaB_tst)[:,1]\n",
        "mB_val = metrics_from_scores(y_val, pB_val)\n",
        "mB_tst = metrics_from_scores(y_tst, pB_tst)\n",
        "\n",
        "print(\"LATE A (sin p1) VAL:\", mA_val, \"\\nLATE A TEST:\", mA_tst)\n",
        "print(\"LATE B (con p1) VAL:\", mB_val, \"\\nLATE B TEST:\", mB_tst)\n",
        "\n",
        "# Elegimos por AUC(VAL)\n",
        "use_B = (mB_val[\"AUC\"] > (mA_val[\"AUC\"] + 1e-6))\n",
        "p_meta_val = pB_val if use_B else pA_val\n",
        "p_meta_tst = pB_tst if use_B else pA_tst\n",
        "m_meta_val = mB_val if use_B else mA_val\n",
        "m_meta_tst = mB_tst if use_B else mA_tst\n",
        "meta_feats = [\"p_img\",\"p_clin\",\"p1_fill\",\"p1_has\"] if use_B else [\"p_img\",\"p_clin\"]\n",
        "print(\"➡️ LATE elegido:\", \"con p1\" if use_B else \"sin p1\", \"| feats:\", meta_feats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EAXGSvjy8W7",
        "outputId": "25e906b5-1d03-4f7f-9d6d-0a5ab0cae5cf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LATE A (sin p1) VAL: {'AUC': 0.9142614601018676, 'PRAUC': 0.9233963998617327, 'Brier': 0.11381905289515709} \n",
            "LATE A TEST: {'AUC': 0.6965460526315789, 'PRAUC': 0.693915321534084, 'Brier': 0.24068705782718486}\n",
            "LATE B (con p1) VAL: {'AUC': 0.9159592529711376, 'PRAUC': 0.9209221898268721, 'Brier': 0.11120259520047246} \n",
            "LATE B TEST: {'AUC': 0.7129934210526315, 'PRAUC': 0.7121843775894818, 'Brier': 0.23385491237404987}\n",
            "➡️ LATE elegido: con p1 | feats: ['p_img', 'p_clin', 'p1_fill', 'p1_has']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 9 — Mid fusion (reconstruyendo las 56 features de imagen desde p11/p14/p13)\n",
        "\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def _clean_cols(df):\n",
        "    df.columns = [str(c).replace(\"\\ufeff\",\"\").strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "def _std_pid(s):\n",
        "    s = str(s).strip().replace(\"\\u200b\",\"\").replace(\"\\ufeff\",\"\")\n",
        "    u = s.upper()\n",
        "    if u.startswith((\"OAS1_\",\"OAS2_\")): return u\n",
        "    if s.isdigit(): return f\"OAS1_{int(s):04d}\"  # por seguridad; se sobreescribe según origen\n",
        "    return u\n",
        "\n",
        "def _load_feature_file(path, cohort_hint=None):\n",
        "    df = _clean_cols(pd.read_csv(path))\n",
        "    # Detectar columna id\n",
        "    idcol = None\n",
        "    for k in [\"patient_id\",\"patient\",\"id\",\"subject id\",\"subject_id\",\"ID\"]:\n",
        "        for c in df.columns:\n",
        "            if c.lower() == k:\n",
        "                idcol = c; break\n",
        "        if idcol: break\n",
        "    if idcol is None:\n",
        "        return None\n",
        "    df[\"patient_id\"] = df[idcol].astype(str).map(_std_pid)\n",
        "    # Si cohort_hint == \"OAS2\" y los ids no llevan prefijo, añádelo\n",
        "    if cohort_hint == \"OAS2\":\n",
        "        df[\"patient_id\"] = df[\"patient_id\"].apply(lambda x: x if x.startswith(\"OAS2_\") else x.replace(\"OAS1_\",\"OAS2_\") if x.startswith(\"OAS1_\") else (\"OAS2_\"+x if not x.startswith(\"OAS2_\") else x))\n",
        "    # Filtrar a columnas de interés (intersección con IMG_FEATS)\n",
        "    keep = [\"patient_id\"] + [c for c in df.columns if c in IMG_FEATS]\n",
        "    df = df[keep].copy()\n",
        "    # Asegurar numéricas\n",
        "    for c in df.columns:\n",
        "        if c!=\"patient_id\":\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def _merge_features(base_ids, dfs):\n",
        "    \"\"\"Combina DataFrames de features evitando sufijos: toma el primer no-NaN por columna.\"\"\"\n",
        "    out = pd.DataFrame({\"patient_id\": base_ids})\n",
        "    for df in dfs:\n",
        "        if df is None or df.empty:\n",
        "            continue\n",
        "        out = out.merge(df, on=\"patient_id\", how=\"left\", suffixes=(None, None))\n",
        "        # Si aparecieran duplicados, resolveremos abajo al reindexar\n",
        "    # Garantiza todas IMG_FEATS presentes\n",
        "    for c in IMG_FEATS:\n",
        "        if c not in out.columns:\n",
        "            out[c] = np.nan\n",
        "    # Ordena columnas\n",
        "    out = out[[\"patient_id\"] + IMG_FEATS]\n",
        "    return out\n",
        "\n",
        "# Rutas candidatas por split\n",
        "sources_val = []\n",
        "sources_tst = []\n",
        "\n",
        "# OAS1 (p11)\n",
        "p11 = BASE/\"p11_alt_backbones\"\n",
        "if p11.exists():\n",
        "    f_val_oas1 = p11/\"val_patient_features_backbones.csv\"\n",
        "    f_tst_oas1 = p11/\"test_patient_features_backbones.csv\"\n",
        "    if f_val_oas1.exists(): sources_val.append((\"OAS1\", f_val_oas1))\n",
        "    if f_tst_oas1.exists(): sources_tst.append((\"OAS1\", f_tst_oas1))\n",
        "\n",
        "# OAS2 (p14/p13)\n",
        "for d in [BASE/\"p14_oasis2_images\", BASE/\"p13_oasis2_images\", BASE.parent/\"p14_oasis2_images\", BASE.parent/\"p13_oasis2_images\"]:\n",
        "    if d.exists():\n",
        "        for name in d.glob(\"*val*patient_features*.csv\"):\n",
        "            sources_val.append((\"OAS2\", name))\n",
        "        for name in d.glob(\"*test*patient_features*.csv\"):\n",
        "            sources_tst.append((\"OAS2\", name))\n",
        "\n",
        "print(\"Fuentes VAL:\", [str(p) for _,p in sources_val])\n",
        "print(\"Fuentes TEST:\", [str(p) for _,p in sources_tst])\n",
        "\n",
        "# Cargar y filtrar a columnas IMG_FEATS\n",
        "val_dfs = []\n",
        "for coh, path in sources_val:\n",
        "    try:\n",
        "        val_dfs.append(_load_feature_file(path, cohort_hint=coh))\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ {path.name}: {e}\")\n",
        "\n",
        "tst_dfs = []\n",
        "for coh, path in sources_tst:\n",
        "    try:\n",
        "        tst_dfs.append(_load_feature_file(path, cohort_hint=coh))\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ {path.name}: {e}\")\n",
        "\n",
        "# Construir matrices de features alineadas al orden de val/tst\n",
        "feat_val = _merge_features(val[\"patient_id\"].values, val_dfs)\n",
        "feat_tst = _merge_features(tst[\"patient_id\"].values, tst_dfs)\n",
        "\n",
        "# Diagnóstico de cobertura por columnas\n",
        "cov_val = feat_val[IMG_FEATS].notna().mean()\n",
        "cov_tst = feat_tst[IMG_FEATS].notna().mean()\n",
        "print(\"Cobertura VAL (media por col):\", float(cov_val.mean()))\n",
        "print(\"Cobertura TEST (media por col):\", float(cov_tst.mean()))\n",
        "\n",
        "# === Preparar Mid: concat imagen + clínico + p1 (prellenado y flag) ===\n",
        "# Nota: p1_fill_val/p1_fill_tst y p1_has_val/p1_has_tst fueron creados en el parche de Celda 7\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "val_p1_df = pd.DataFrame({\"p1_fill\": p1_fill_val, \"p1_has\": p1_has_val.astype(int)}, index=val.index)\n",
        "tst_p1_df = pd.DataFrame({\"p1_fill\": p1_fill_tst, \"p1_has\": p1_has_tst.astype(int)}, index=tst.index)\n",
        "\n",
        "# Usar feat_val/feat_tst para las columnas IMG_FEATS (no val[img_cols])\n",
        "X_mid_val = pd.concat([feat_val[IMG_FEATS].reset_index(drop=True), X_clin_val.reset_index(drop=True), val_p1_df.reset_index(drop=True)], axis=1)\n",
        "X_mid_tst = pd.concat([feat_tst[IMG_FEATS].reset_index(drop=True), X_clin_tst.reset_index(drop=True), tst_p1_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "all_cols = list(X_mid_val.columns)\n",
        "# clínicas num/cat ya detectadas antes (CLIN_COLS); extendemos con imagen + p1\n",
        "mid_num = [c for c in all_cols if (c in IMG_FEATS) or (c in [*CLIN_COLS]) and (X_clin_val[c].dtype!=object) or (c in [\"p1_fill\",\"p1_has\"])]\n",
        "mid_cat = [c for c in CLIN_COLS if X_clin_val[c].dtype==object and c in all_cols]\n",
        "\n",
        "# El ColumnTransformer se encargará de imputar medianas; si alguna col está all-NaN, sklearn la “saltará” sin romper\n",
        "pre_mid = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "                          (\"scaler\", StandardScaler())]), mid_num),\n",
        "        (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "                          (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]), mid_cat),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    sparse_threshold=0.3\n",
        ")\n",
        "\n",
        "mid_lr = Pipeline([\n",
        "    (\"pre\", pre_mid),\n",
        "    (\"lr\", LogisticRegression(penalty=\"elasticnet\", solver=\"saga\", l1_ratio=0.5, C=0.5, max_iter=5000))\n",
        "])\n",
        "\n",
        "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
        "oof_mid = np.zeros(len(y_val), dtype=float)\n",
        "for tr, va in rskf.split(X_mid_val, y_val):\n",
        "    m = mid_lr\n",
        "    m.fit(X_mid_val.iloc[tr], y_val[tr])\n",
        "    oof_mid[va] = m.predict_proba(X_mid_val.iloc[va])[:,1]\n",
        "\n",
        "mid_lr.fit(X_mid_val, y_val)\n",
        "p_mid_tst = mid_lr.predict_proba(X_mid_tst)[:,1]\n",
        "\n",
        "m_mid_val = metrics_from_scores(y_val, oof_mid)\n",
        "m_mid_tst = metrics_from_scores(y_tst, p_mid_tst)\n",
        "print(\"MID FUSION VAL :\", m_mid_val)\n",
        "print(\"MID FUSION TEST:\", m_mid_tst)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47rw-rg9zgAq",
        "outputId": "4b98c4b4-82e6-4aaa-ec01-6e303585f55b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fuentes VAL: ['/content/drive/MyDrive/CognitivaAI/p11_alt_backbones/val_patient_features_backbones.csv', '/content/drive/MyDrive/CognitivaAI/p14_oasis2_images/val_patient_features_oas2_effb3_p14.csv', '/content/drive/MyDrive/CognitivaAI/p13_oasis2_images/val_patient_features_oas2_effb3.csv']\n",
            "Fuentes TEST: ['/content/drive/MyDrive/CognitivaAI/p11_alt_backbones/test_patient_features_backbones.csv', '/content/drive/MyDrive/CognitivaAI/p14_oasis2_images/test_patient_features_oas2_effb3_p14.csv', '/content/drive/MyDrive/CognitivaAI/p13_oasis2_images/test_patient_features_oas2_effb3.csv']\n",
            "Cobertura VAL (media por col): 0.5144927536231884\n",
            "Cobertura TEST (media por col): 0.6224489795918366\n",
            "MID FUSION VAL : {'AUC': 0.7971137521222411, 'PRAUC': 0.7766370505604872, 'Brier': 0.18454527361873402}\n",
            "MID FUSION TEST: {'AUC': 0.6973684210526316, 'PRAUC': 0.6573379171919493, 'Brier': 0.22967954621192932}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 10 — Selección + umbrales coste 5:1 (por cohorte) + artefactos P26\n",
        "import numpy as np, pandas as pd, json\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
        "\n",
        "OUT = BASE/\"p26_intermodal\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def metrics_from_scores(y, p):\n",
        "    y = np.asarray(y).astype(int); p = np.asarray(p, float)\n",
        "    return dict(\n",
        "        AUC=float(roc_auc_score(y, p)),\n",
        "        PRAUC=float(average_precision_score(y, p)),\n",
        "        Brier=float(brier_score_loss(y, p)),\n",
        "    )\n",
        "\n",
        "def confusion_at_thr(y_true, y_prob, thr):\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    TP = int(((y_true==1)&(y_pred==1)).sum())\n",
        "    FP = int(((y_true==0)&(y_pred==1)).sum())\n",
        "    TN = int(((y_true==0)&(y_pred==0)).sum())\n",
        "    FN = int(((y_true==1)&(y_pred==0)).sum())\n",
        "    prec = TP/(TP+FP) if (TP+FP)>0 else np.nan\n",
        "    rec  = TP/(TP+FN) if (TP+FN)>0 else np.nan\n",
        "    acc  = (TP+TN)/(TP+TN+FP+FN)\n",
        "    return TP,FP,TN,FN,prec,rec,acc\n",
        "\n",
        "def best_cost_thr(y_true, y_prob, C_FN=5.0, C_FP=1.0, grid=1001):\n",
        "    y_true = np.asarray(y_true).astype(int)\n",
        "    y_prob = np.asarray(y_prob, float)\n",
        "    thrs = np.linspace(0,1,grid)\n",
        "    best = None\n",
        "    for t in thrs:\n",
        "        TP,FP,TN,FN,_,_,_ = confusion_at_thr(y_true,y_prob,t)\n",
        "        cost = C_FN*FN + C_FP*FP\n",
        "        if (best is None) or (cost < best[\"Cost\"] - 1e-9):\n",
        "            best = dict(Thr=float(t), Cost=float(cost), TP=TP, FP=FP, TN=TN, FN=FN)\n",
        "    return best\n",
        "\n",
        "# === 1) Tomamos el mejor \"Late o Mid\" por AUC(VAL) ===\n",
        "# Variables creadas en celdas previas:\n",
        "#   Late elegido → p_meta_val, p_meta_tst, m_meta_val, m_meta_tst, meta_feats\n",
        "#   Mid          → oof_mid,     p_mid_tst,  m_mid_val,  m_mid_tst\n",
        "late_is_better = (m_meta_val[\"AUC\"] >= m_mid_val[\"AUC\"] - 1e-6)\n",
        "winner = \"LATE\" if late_is_better else \"MID\"\n",
        "\n",
        "y_val_arr = y_val.astype(int)\n",
        "y_tst_arr = y_tst.astype(int)\n",
        "\n",
        "p_val_w = p_meta_val if winner==\"LATE\" else oof_mid\n",
        "p_tst_w = p_meta_tst if winner==\"LATE\" else p_mid_tst\n",
        "\n",
        "m_val_w = metrics_from_scores(y_val_arr, p_val_w)\n",
        "m_tst_w = metrics_from_scores(y_tst_arr, p_tst_w)\n",
        "\n",
        "print(f\"🏁 P26 seleccionado: {winner}  |  VAL AUC={m_val_w['AUC']:.3f}  TEST AUC={m_tst_w['AUC']:.3f}\")\n",
        "\n",
        "# === 2) Umbrales por cohorte (coste FN:FP = 5:1) aprendidos en VAL y aplicados a TEST ===\n",
        "C_FN, C_FP = 5.0, 1.0\n",
        "rows_thr = []\n",
        "rows_test = []\n",
        "\n",
        "for coh in [\"OAS1\",\"OAS2\"]:\n",
        "    mask_val = (val[\"cohort\"].values==coh)\n",
        "    mask_tst = (tst[\"cohort\"].values==coh)\n",
        "    best = best_cost_thr(y_val_arr[mask_val], p_val_w[mask_val], C_FN=C_FN, C_FP=C_FP, grid=1001)\n",
        "    thr = best[\"Thr\"]\n",
        "    TP,FP,TN,FN,prec,rec,acc = confusion_at_thr(y_tst_arr[mask_tst], p_tst_w[mask_tst], thr)\n",
        "    rows_thr.append(dict(Cohort=coh, Thr_VAL=thr, Cost_VAL=best[\"Cost\"], C_FN=C_FN, C_FP=C_FP))\n",
        "    rows_test.append(dict(\n",
        "        Cohort=coh, Thr=thr, TP=TP, FP=FP, TN=TN, FN=FN,\n",
        "        Precision=float(prec), Recall=float(rec), Acc=float(acc),\n",
        "        Cost=float(C_FN*FN + C_FP*FP)\n",
        "    ))\n",
        "\n",
        "thr_df  = pd.DataFrame(rows_thr)\n",
        "test_df = pd.DataFrame(rows_test)\n",
        "thr_df.to_csv(OUT/\"p26_thresholds_cost_5to1.csv\", index=False)\n",
        "test_df.to_csv(OUT/\"p26_test_report_cost_5to1.csv\", index=False)\n",
        "\n",
        "print(\"💾 Guardado umbrales:\", OUT/\"p26_thresholds_cost_5to1.csv\")\n",
        "print(\"💾 Guardado test@umbrales:\", OUT/\"p26_test_report_cost_5to1.csv\")\n",
        "print(test_df)\n",
        "\n",
        "# === 3) Guardar predicciones y resumen ===\n",
        "val_preds = pd.DataFrame({\n",
        "    \"patient_id\": val[\"patient_id\"].values,\n",
        "    \"cohort\": val[\"cohort\"].values,\n",
        "    \"y_true\": y_val_arr,\n",
        "    \"y_prob\": p_val_w\n",
        "})\n",
        "tst_preds = pd.DataFrame({\n",
        "    \"patient_id\": tst[\"patient_id\"].values,\n",
        "    \"cohort\": tst[\"cohort\"].values,\n",
        "    \"y_true\": y_tst_arr,\n",
        "    \"y_prob\": p_tst_w\n",
        "})\n",
        "val_preds.to_csv(OUT/\"p26_val_preds.csv\", index=False)\n",
        "tst_preds.to_csv(OUT/\"p26_test_preds.csv\", index=False)\n",
        "\n",
        "summary = dict(\n",
        "    winner=winner,\n",
        "    meta_features=(meta_feats if winner==\"LATE\" else \"MID(IMG56+CLIN+p1)\"),\n",
        "    cost_weights=dict(C_FN=C_FN, C_FP=C_FP),\n",
        "    VAL=m_val_w, TEST=m_tst_w,\n",
        "    late_VAL=m_meta_val, late_TEST=m_meta_tst,\n",
        "    mid_VAL=m_mid_val,  mid_TEST=m_mid_tst\n",
        ")\n",
        "json.dump(summary, open(OUT/\"p26_summary.json\",\"w\"), indent=2)\n",
        "print(\"💾 Guardado summary:\", OUT/\"p26_summary.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFq1VTCB3Dg9",
        "outputId": "dce22393-1ce1-49b9-b6a7-fd35e7b9b38a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏁 P26 seleccionado: LATE  |  VAL AUC=0.916  TEST AUC=0.713\n",
            "💾 Guardado umbrales: /content/drive/MyDrive/CognitivaAI/p26_intermodal/p26_thresholds_cost_5to1.csv\n",
            "💾 Guardado test@umbrales: /content/drive/MyDrive/CognitivaAI/p26_intermodal/p26_test_report_cost_5to1.csv\n",
            "  Cohort    Thr  TP  FP  TN  FN  Precision    Recall       Acc  Cost\n",
            "0   OAS1  0.307  14   9  18   6   0.608696  0.700000  0.680851  39.0\n",
            "1   OAS2  0.195   8   4   7   4   0.666667  0.666667  0.652174  24.0\n",
            "💾 Guardado summary: /content/drive/MyDrive/CognitivaAI/p26_intermodal/p26_summary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 11_fix — Bloques README / Informe / Bitácora (robustos, sin f-strings con if en el formato)\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "from datetime import date\n",
        "\n",
        "OUT = BASE/\"p26_intermodal\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Carga lo que acabamos de guardar en la Celda 10\n",
        "thr_df  = pd.read_csv(OUT/\"p26_thresholds_cost_5to1.csv\")\n",
        "test_df = pd.read_csv(OUT/\"p26_test_report_cost_5to1.csv\")\n",
        "\n",
        "# Extrae por cohorte para poner cifras en el texto\n",
        "def _row(coh):\n",
        "    r = test_df.loc[test_df[\"Cohort\"]==coh]\n",
        "    return r.iloc[0].to_dict() if len(r) else {}\n",
        "\n",
        "r1 = _row(\"OAS1\")\n",
        "r2 = _row(\"OAS2\")\n",
        "\n",
        "# Helpers para strings con 3 decimales o '—' si falta\n",
        "f3 = lambda x: f\"{float(x):.3f}\" if pd.notna(x) else \"—\"\n",
        "\n",
        "# Valores del resumen (ya estaban en 'summary' de Celda 10)\n",
        "late_val_auc  = f3(summary['late_VAL']['AUC'])\n",
        "late_val_pra  = f3(summary['late_VAL']['PRAUC'])\n",
        "late_val_bri  = f3(summary['late_VAL']['Brier'])\n",
        "late_tst_auc  = f3(summary['late_TEST']['AUC'])\n",
        "late_tst_pra  = f3(summary['late_TEST']['PRAUC'])\n",
        "late_tst_bri  = f3(summary['late_TEST']['Brier'])\n",
        "\n",
        "mid_val_auc   = f3(summary['mid_VAL']['AUC'])\n",
        "mid_val_pra   = f3(summary['mid_VAL']['PRAUC'])\n",
        "mid_val_bri   = f3(summary['mid_VAL']['Brier'])\n",
        "mid_tst_auc   = f3(summary['mid_TEST']['AUC'])\n",
        "mid_tst_pra   = f3(summary['mid_TEST']['PRAUC'])\n",
        "mid_tst_bri   = f3(summary['mid_TEST']['Brier'])\n",
        "\n",
        "winner_str = summary['winner']\n",
        "meta_feats = summary['meta_features'] if isinstance(summary['meta_features'], list) else [str(summary['meta_features'])]\n",
        "\n",
        "# Texto por cohorte\n",
        "oas1_line = (\n",
        "    f\"OAS1 @ thr={f3(r1.get('Thr'))} → \"\n",
        "    f\"TP={int(r1.get('TP',np.nan))}, FP={int(r1.get('FP',np.nan))}, TN={int(r1.get('TN',np.nan))}, FN={int(r1.get('FN',np.nan))} \"\n",
        "    f\"→ R={f3(r1.get('Recall'))}, P={f3(r1.get('Precision'))}, Acc={f3(r1.get('Acc'))}, Coste={f3(r1.get('Cost'))}\"\n",
        ")\n",
        "oas2_line = (\n",
        "    f\"OAS2 @ thr={f3(r2.get('Thr'))} → \"\n",
        "    f\"TP={int(r2.get('TP',np.nan))}, FP={int(r2.get('FP',np.nan))}, TN={int(r2.get('TN',np.nan))}, FN={int(r2.get('FN',np.nan))} \"\n",
        "    f\"→ R={f3(r2.get('Recall'))}, P={f3(r2.get('Precision'))}, Acc={f3(r2.get('Acc'))}, Coste={f3(r2.get('Cost'))}\"\n",
        ")\n",
        "\n",
        "# === README block ===\n",
        "blk_readme = f\"\"\"\n",
        "### P26 — Intermodal (imagen + clínico) con fusión Late/Mid\n",
        "\n",
        "**Selección por VAL:** {winner_str}\n",
        "- **Late (p_img, p_clin{\", p1_fill, p1_has\" if len(meta_feats)>2 else \"\"})**\n",
        "  - VAL: AUC={late_val_auc} | PR-AUC={late_val_pra} | Brier={late_val_bri}\n",
        "  - TEST: AUC={late_tst_auc} | PR-AUC={late_tst_pra} | Brier={late_tst_bri}\n",
        "- **Mid (IMG56 + clínico + p1)**\n",
        "  - VAL: AUC={mid_val_auc} | PR-AUC={mid_val_pra} | Brier={mid_val_bri}\n",
        "  - TEST: AUC={mid_tst_auc} | PR-AUC={mid_tst_pra} | Brier={mid_tst_bri}\n",
        "\n",
        "**Decisión por coste (FN:FP=5:1, umbral aprendido en VAL y aplicado en TEST):**\n",
        "- {oas1_line}\n",
        "- {oas2_line}\n",
        "\n",
        "_Artefactos_: `p26_val_preds.csv`, `p26_test_preds.csv`, `p26_thresholds_cost_5to1.csv`, `p26_test_report_cost_5to1.csv`, `p26_summary.json`.\n",
        "\"\"\"\n",
        "\n",
        "# === InformeTecnico block ===\n",
        "blk_informe = f\"\"\"\n",
        "## P26 — Intermodal (imagen + clínico)\n",
        "\n",
        "**Diseño:**\n",
        "1) **Clínico consolidado** OASIS-1/2 con anti-fuga (sin CDR/Group), imputación ligera y OHE.\n",
        "2) Señales de imagen: **probabilidad P24** + **matriz 56 features** (p11 OAS1 + p14/p13 OAS2).\n",
        "3) Señal parcial p1-OAS2 (~32% cobertura) integrada con **imputación por cohorte** (media VAL OAS2) + **flag de presencia**.\n",
        "4) Dos estrategias:\n",
        "   - **Late:** meta-LR sobre {{p_img, p_clin}} (+ p1_fill, p1_has).\n",
        "   - **Mid:** LR-EN sobre {{IMG56, clínico, p1}}.\n",
        "5) Selección por **AUC(VAL)** y decisión por **coste 5:1** (umbral por cohorte aprendido en VAL).\n",
        "\n",
        "**Resultados:**\n",
        "- LATE (seleccionado): VAL AUC={late_val_auc} · TEST AUC={late_tst_auc} (Brier TEST={late_tst_bri}).\n",
        "- MID: VAL AUC={mid_val_auc} · TEST AUC={mid_tst_auc} (Brier TEST={mid_tst_bri}).\n",
        "\n",
        "**Decisión clínico-operativa (5:1):**\n",
        "- {oas1_line}\n",
        "- {oas2_line}\n",
        "\n",
        "**Notas:**\n",
        "- La cobertura parcial de p1 se maneja con imputación **solo en OAS2** y `p1_has`.\n",
        "- Late supera Mid en este dataset; en despliegue, monitorizar ECE/MCE por cohorte y considerar recalibración si ECE>0.2.\n",
        "\"\"\"\n",
        "\n",
        "# === Bitácora block ===\n",
        "blk_bitacora = f\"\"\"\n",
        "### {date.today()} — P26 intermodal completado\n",
        "\n",
        "- Estrategia seleccionada: **{winner_str}** (meta-features: {\", \".join(meta_feats)}).\n",
        "- Late VAL: AUC={late_val_auc} | TEST: AUC={late_tst_auc}.\n",
        "- Mid  VAL: AUC={mid_val_auc}  | TEST: AUC={mid_tst_auc}.\n",
        "- Umbrales 5:1 por cohorte (aprendidos en VAL → aplicados en TEST):\n",
        "  - {oas1_line}\n",
        "  - {oas2_line}\n",
        "- Artefactos guardados en `p26_intermodal/`.\n",
        "\"\"\"\n",
        "\n",
        "# Guarda bloques a disco y muestra README block como preview\n",
        "(Path(OUT/\"p26_readme_block.md\").write_text(blk_readme, encoding=\"utf-8\"))\n",
        "(Path(OUT/\"p26_informe_block.md\").write_text(blk_informe, encoding=\"utf-8\"))\n",
        "(Path(OUT/\"p26_bitacora_block.md\").write_text(blk_bitacora, encoding=\"utf-8\"))\n",
        "\n",
        "print(\"✅ Bloques guardados en:\", OUT)\n",
        "print(blk_readme)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imgDzVt13MkO",
        "outputId": "bcb91eb6-d70d-4d5a-837d-fa37da40048f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Bloques guardados en: /content/drive/MyDrive/CognitivaAI/p26_intermodal\n",
            "\n",
            "### P26 — Intermodal (imagen + clínico) con fusión Late/Mid\n",
            "\n",
            "**Selección por VAL:** LATE  \n",
            "- **Late (p_img, p_clin, p1_fill, p1_has)**  \n",
            "  - VAL: AUC=0.916 | PR-AUC=0.921 | Brier=0.111  \n",
            "  - TEST: AUC=0.713 | PR-AUC=0.712 | Brier=0.234\n",
            "- **Mid (IMG56 + clínico + p1)**  \n",
            "  - VAL: AUC=0.797 | PR-AUC=0.777 | Brier=0.185  \n",
            "  - TEST: AUC=0.697 | PR-AUC=0.657 | Brier=0.230\n",
            "\n",
            "**Decisión por coste (FN:FP=5:1, umbral aprendido en VAL y aplicado en TEST):**  \n",
            "- OAS1 @ thr=0.307 → TP=14, FP=9, TN=18, FN=6 → R=0.700, P=0.609, Acc=0.681, Coste=39.000  \n",
            "- OAS2 @ thr=0.195 → TP=8, FP=4, TN=7, FN=4 → R=0.667, P=0.667, Acc=0.652, Coste=24.000\n",
            "\n",
            "_Artefactos_: `p26_val_preds.csv`, `p26_test_preds.csv`, `p26_thresholds_cost_5to1.csv`, `p26_test_report_cost_5to1.csv`, `p26_summary.json`.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 12 — Insertar P26 en P25 (master table + executive table)\n",
        "import pandas as pd, numpy as np, json\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/CognitivaAI\")\n",
        "P25 = BASE/\"p25_informe_final\"\n",
        "P26 = BASE/\"p26_intermodal\"\n",
        "assert (P25/\"p25_master_table.csv\").exists(), \"No encuentro p25_master_table.csv\"\n",
        "assert (P26/\"p26_val_preds.csv\").exists() and (P26/\"p26_test_preds.csv\").exists(), \"Faltan preds de P26\"\n",
        "assert (P26/\"p26_test_report_cost_5to1.csv\").exists(), \"Falta test_report_cost_5to1 de P26\"\n",
        "assert (P26/\"p26_summary.json\").exists(), \"Falta summary de P26\"\n",
        "\n",
        "mt = pd.read_csv(P25/\"p25_master_table.csv\")\n",
        "val = pd.read_csv(P26/\"p26_val_preds.csv\")\n",
        "tst = pd.read_csv(P26/\"p26_test_preds.csv\")\n",
        "rep = pd.read_csv(P26/\"p26_test_report_cost_5to1.csv\")\n",
        "summary = json.load(open(P26/\"p26_summary.json\"))\n",
        "\n",
        "def _metrics(df):\n",
        "    y, p = df[\"y_true\"].astype(int).to_numpy(), df[\"y_prob\"].astype(float).to_numpy()\n",
        "    return dict(\n",
        "        AUC=float(roc_auc_score(y,p)),\n",
        "        PRAUC=float(average_precision_score(y,p)),\n",
        "        Brier=float(brier_score_loss(y,p))\n",
        "    )\n",
        "\n",
        "rows = []\n",
        "# ALL\n",
        "rows.append(dict(Pipeline=\"P26\", Split=\"VAL\",  Cohort=\"ALL\", **_metrics(val),  Acc=np.nan, Precision=np.nan, Recall=np.nan, Thr=np.nan, Cost=np.nan, Notas=summary[\"winner\"]))\n",
        "rows.append(dict(Pipeline=\"P26\", Split=\"TEST\", Cohort=\"ALL\", **_metrics(tst),  Acc=np.nan, Precision=np.nan, Recall=np.nan, Thr=np.nan, Cost=np.nan, Notas=summary[\"winner\"]))\n",
        "# OAS1 / OAS2\n",
        "for coh in [\"OAS1\",\"OAS2\"]:\n",
        "    rows.append(dict(Pipeline=\"P26\", Split=\"VAL\",  Cohort=coh, **_metrics(val[val[\"cohort\"]==coh]),  Acc=np.nan, Precision=np.nan, Recall=np.nan, Thr=np.nan, Cost=np.nan, Notas=summary[\"winner\"]))\n",
        "    rows.append(dict(Pipeline=\"P26\", Split=\"TEST\", Cohort=coh, **_metrics(tst[tst[\"cohort\"]==coh]), Acc=np.nan, Precision=np.nan, Recall=np.nan, Thr=np.nan, Cost=np.nan, Notas=summary[\"winner\"]))\n",
        "\n",
        "mt2 = pd.concat([mt, pd.DataFrame(rows)], ignore_index=True)\n",
        "mt2.to_csv(P25/\"p25_master_table.csv\", index=False)\n",
        "print(\"✅ Actualizado:\", P25/\"p25_master_table.csv\")\n",
        "\n",
        "# Executive table (añadimos líneas P26: métricas y coste)\n",
        "def f3(x):\n",
        "    try: return f\"{float(x):.3f}\"\n",
        "    except: return \"—\"\n",
        "\n",
        "lines = []\n",
        "\n",
        "# Métricas P26\n",
        "all_val = _metrics(val); all_tst = _metrics(tst)\n",
        "lines.append({\"Pipeline\":\"P26\",\"Cohorte\":\"ALL\",\"Método\":summary[\"winner\"],\n",
        "              \"AUC\":f3(all_tst[\"AUC\"]),\"PR-AUC\":f3(all_tst[\"PRAUC\"]),\"Brier\":f3(all_tst[\"Brier\"]),\n",
        "              \"Acc\":\"nan\",\"Prec\":\"nan\",\"Rec\":\"nan\",\"Thr\":\"nan\",\"Coste\":\"nan\"})\n",
        "for coh in [\"OAS1\",\"OAS2\"]:\n",
        "    m = _metrics(tst[tst[\"cohort\"]==coh])\n",
        "    lines.append({\"Pipeline\":\"P26\",\"Cohorte\":coh,\"Método\":summary[\"winner\"],\n",
        "                  \"AUC\":f3(m[\"AUC\"]),\"PR-AUC\":f3(m[\"PRAUC\"]),\"Brier\":f3(m[\"Brier\"]),\n",
        "                  \"Acc\":\"nan\",\"Prec\":\"nan\",\"Rec\":\"nan\",\"Thr\":\"nan\",\"Coste\":\"nan\"})\n",
        "\n",
        "# Filas de coste 5:1 P26 @ TEST\n",
        "for _,r in rep.iterrows():\n",
        "    lines.append({\"Pipeline\":\"P26\",\"Cohorte\":r[\"Cohort\"],\"Método\":\"cost-5:1\",\n",
        "                  \"AUC\":\"—\",\"PR-AUC\":\"—\",\"Brier\":\"—\",\n",
        "                  \"Acc\":f3(r[\"Acc\"]),\"Prec\":f3(r[\"Precision\"]),\"Rec\":f3(r[\"Recall\"]),\n",
        "                  \"Thr\":f3(r[\"Thr\"]),\"Coste\":f3(r[\"Cost\"])})\n",
        "\n",
        "# Releer executive existente y añadir P26 al final\n",
        "exec_md = P25/\"p25_executive_table.md\"\n",
        "from io import StringIO\n",
        "def mk_table(rows):\n",
        "    cols=[\"Pipeline\",\"Cohorte\",\"Método\",\"AUC\",\"PR-AUC\",\"Brier\",\"Acc\",\"Prec\",\"Rec\",\"Thr\",\"Coste\"]\n",
        "    out=\"| \" + \" | \".join([\"Pipeline\",\"Cohorte\",\"Método\",\"AUC\",\"PR-AUC\",\"Brier\",\"Acc\",\"Prec\",\"Rec\",\"Thr\",\"Coste\"]) + \" |\\n\"\n",
        "    out+=\"|---|---|---:|---:|---:|---:|---:|---:|---:|---:|---:|\\n\"\n",
        "    for r in rows:\n",
        "        out+=\"| {Pipeline} | {Cohorte} | {Método} | {AUC} | {PR-AUC} | {Brier} | {Acc} | {Prec} | {Rec} | {Thr} | {Coste} |\\n\".format(**r)\n",
        "    return out\n",
        "\n",
        "# Cargamos tabla anterior si existe\n",
        "old = \"\"\n",
        "if exec_md.exists():\n",
        "    old = exec_md.read_text(encoding=\"utf-8\")\n",
        "\n",
        "tbl = mk_table(lines)\n",
        "new_md = (old.rstrip() + \"\\n\" if old else \"\") + tbl\n",
        "exec_md.write_text(new_md, encoding=\"utf-8\")\n",
        "print(\"✅ Actualizado:\", exec_md)\n",
        "print(tbl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7h6J8OZ6qRo",
        "outputId": "7c7eeb25-b415-4673-a97a-410dea99c0b4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Actualizado: /content/drive/MyDrive/CognitivaAI/p25_informe_final/p25_master_table.csv\n",
            "✅ Actualizado: /content/drive/MyDrive/CognitivaAI/p25_informe_final/p25_executive_table.md\n",
            "| Pipeline | Cohorte | Método | AUC | PR-AUC | Brier | Acc | Prec | Rec | Thr | Coste |\n",
            "|---|---|---:|---:|---:|---:|---:|---:|---:|---:|---:|\n",
            "| P26 | ALL | LATE | 0.713 | 0.712 | 0.234 | nan | nan | nan | nan | nan |\n",
            "| P26 | OAS1 | LATE | 0.754 | 0.736 | 0.208 | nan | nan | nan | nan | nan |\n",
            "| P26 | OAS2 | LATE | 0.652 | 0.728 | 0.288 | nan | nan | nan | nan | nan |\n",
            "| P26 | OAS1 | cost-5:1 | — | — | — | 0.681 | 0.609 | 0.700 | 0.307 | 39.000 |\n",
            "| P26 | OAS2 | cost-5:1 | — | — | — | 0.652 | 0.667 | 0.667 | 0.195 | 24.000 |\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 13 — P26 con umbrales de P24 (OAS1=0.435, OAS2=0.332)\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "P26 = Path(\"/content/drive/MyDrive/CognitivaAI/p26_intermodal\")\n",
        "tst = pd.read_csv(P26/\"p26_test_preds.csv\")\n",
        "\n",
        "def confusion(y_true, y_prob, thr):\n",
        "    y_pred = (y_prob>=thr).astype(int)\n",
        "    TP = int(((y_true==1)&(y_pred==1)).sum())\n",
        "    FP = int(((y_true==0)&(y_pred==1)).sum())\n",
        "    TN = int(((y_true==0)&(y_pred==0)).sum())\n",
        "    FN = int(((y_true==1)&(y_pred==0)).sum())\n",
        "    prec = TP/(TP+FP) if (TP+FP)>0 else np.nan\n",
        "    rec  = TP/(TP+FN) if (TP+FN)>0 else np.nan\n",
        "    acc  = (TP+TN)/len(y_true)\n",
        "    return TP,FP,TN,FN,prec,rec,acc, (5*FN + 1*FP)\n",
        "\n",
        "rows=[]\n",
        "for coh,thr in [(\"OAS1\",0.435),(\"OAS2\",0.332)]:\n",
        "    df = tst[tst[\"cohort\"]==coh]\n",
        "    TP,FP,TN,FN,P,R,A,C = confusion(df[\"y_true\"].values, df[\"y_prob\"].values, thr)\n",
        "    rows.append(dict(Cohort=coh, Thr=thr, TP=TP, FP=FP, TN=TN, FN=FN,\n",
        "                     Precision=P, Recall=R, Acc=A, Cost=C))\n",
        "alt = pd.DataFrame(rows)\n",
        "print(alt)\n",
        "out = P26/\"p26_test_report_cost_5to1_ALTthr_fromP24.csv\"\n",
        "alt.to_csv(out, index=False)\n",
        "print(\"💾 Guardado:\", out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lruh8mxz6u08",
        "outputId": "6679635e-7c4c-402a-e5f2-9f54b0448bc7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Cohort    Thr  TP  FP  TN  FN  Precision    Recall       Acc  Cost\n",
            "0   OAS1  0.435  11   6  21   9   0.647059  0.550000  0.680851    51\n",
            "1   OAS2  0.332   7   4   7   5   0.636364  0.583333  0.608696    29\n",
            "💾 Guardado: /content/drive/MyDrive/CognitivaAI/p26_intermodal/p26_test_report_cost_5to1_ALTthr_fromP24.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 14 — ECE/MCE por cohorte para P26 (10 bins)\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def ece_mce(y_true, y_prob, bins=10):\n",
        "    y_true = np.asarray(y_true).astype(int)\n",
        "    y_prob = np.asarray(y_prob).astype(float)\n",
        "    edges = np.linspace(0,1,bins+1)\n",
        "    ece=0.0; mce=0.0; n=len(y_true)\n",
        "    for i in range(bins):\n",
        "        m = (y_prob>=edges[i]) & (y_prob<edges[i+1] if i<bins-1 else y_prob<=edges[i+1])\n",
        "        if m.sum()==0: continue\n",
        "        conf = y_prob[m].mean()\n",
        "        acc  = y_true[m].mean()\n",
        "        gap = abs(acc-conf)\n",
        "        ece += (m.mean())*gap\n",
        "        mce = max(mce, gap)\n",
        "    return ece, mce\n",
        "\n",
        "P26 = Path(\"/content/drive/MyDrive/CognitivaAI/p26_intermodal\")\n",
        "tst = pd.read_csv(P26/\"p26_test_preds.csv\")\n",
        "\n",
        "rows=[]\n",
        "for coh in [\"ALL\",\"OAS1\",\"OAS2\"]:\n",
        "    df = tst if coh==\"ALL\" else tst[tst[\"cohort\"]==coh]\n",
        "    ece,mce = ece_mce(df[\"y_true\"], df[\"y_prob\"], bins=10)\n",
        "    rows.append(dict(Cohort=coh, ECE10=ece, MCE10=mce))\n",
        "cal = pd.DataFrame(rows)\n",
        "cal.to_csv(P26/\"p26_test_calibration_ece.csv\", index=False)\n",
        "print(cal)\n",
        "print(\"💾 Guardado:\", P26/\"p26_test_calibration_ece.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW8FCab76z0l",
        "outputId": "7c9a387f-b107-48bb-82ad-e86f7d3b79b9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Cohort     ECE10     MCE10\n",
            "0    ALL  0.178378  0.406751\n",
            "1   OAS1  0.150002  0.577521\n",
            "2   OAS2  0.312514  0.765920\n",
            "💾 Guardado: /content/drive/MyDrive/CognitivaAI/p26_intermodal/p26_test_calibration_ece.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# P26b — Calibración por cohorte (Platt) + re-umbrales 5:1\n",
        "import numpy as np, pandas as pd, json\n",
        "from pathlib import Path\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
        "\n",
        "P26 = BASE/\"p26_intermodal\"\n",
        "val = pd.read_csv(P26/\"p26_val_preds.csv\")\n",
        "tst = pd.read_csv(P26/\"p26_test_preds.csv\")\n",
        "\n",
        "def platt_fit(y, p):\n",
        "    # calibrador estilo Platt (LR binaria sobre el score)\n",
        "    m = LogisticRegression(solver=\"lbfgs\")\n",
        "    m.fit(p.reshape(-1,1), y.astype(int))\n",
        "    return m\n",
        "\n",
        "def platt_pred(m, p):\n",
        "    return m.predict_proba(p.reshape(-1,1))[:,1]\n",
        "\n",
        "def best_cost_thr(y_true, y_prob, C_FN=5.0, C_FP=1.0, grid=1001):\n",
        "    thrs = np.linspace(0,1,grid); best=None\n",
        "    for t in thrs:\n",
        "        y_pred = (y_prob>=t).astype(int)\n",
        "        TP=((y_true==1)&(y_pred==1)).sum()\n",
        "        FP=((y_true==0)&(y_pred==1)).sum()\n",
        "        FN=((y_true==1)&(y_pred==0)).sum()\n",
        "        cost = C_FN*FN + C_FP*FP\n",
        "        if (best is None) or (cost < best[\"Cost\"]-1e-9):\n",
        "            best=dict(Thr=float(t), Cost=float(cost))\n",
        "    return best\n",
        "\n",
        "out = []\n",
        "cal_preds = []\n",
        "\n",
        "for coh in [\"OAS1\",\"OAS2\"]:\n",
        "    v = val[val[\"cohort\"]==coh]; t = tst[tst[\"cohort\"]==coh]\n",
        "    yv, pv = v[\"y_true\"].to_numpy(), v[\"y_prob\"].to_numpy()\n",
        "    yt, pt = t[\"y_true\"].to_numpy(), t[\"y_prob\"].to_numpy()\n",
        "\n",
        "    # Calibración Platt por cohorte\n",
        "    pl = platt_fit(yv, pv)\n",
        "    pv_cal = platt_pred(pl, pv)\n",
        "    pt_cal = platt_pred(pl, pt)\n",
        "\n",
        "    # Métricas post-calibración\n",
        "    auc_val = roc_auc_score(yv, pv_cal); auc_tst = roc_auc_score(yt, pt_cal)\n",
        "    pr_val = average_precision_score(yv, pv_cal); pr_tst = average_precision_score(yt, pt_cal)\n",
        "    bri_val = brier_score_loss(yv, pv_cal); bri_tst = brier_score_loss(yt, pt_cal)\n",
        "\n",
        "    # Re-umbrales (coste 5:1) aprendidos en VAL-cal y aplicados en TEST-cal\n",
        "    best = best_cost_thr(yv, pv_cal, C_FN=5.0, C_FP=1.0)\n",
        "    thr = best[\"Thr\"]\n",
        "    ypred = (pt_cal>=thr).astype(int)\n",
        "    TP=int(((yt==1)&(ypred==1)).sum())\n",
        "    FP=int(((yt==0)&(ypred==1)).sum())\n",
        "    TN=int(((yt==0)&(ypred==0)).sum())\n",
        "    FN=int(((yt==1)&(ypred==0)).sum())\n",
        "    prec = TP/(TP+FP) if (TP+FP)>0 else np.nan\n",
        "    rec  = TP/(TP+FN) if (TP+FN)>0 else np.nan\n",
        "    acc  = (TP+TN)/len(yt)\n",
        "    cost = 5*FN + 1*FP\n",
        "\n",
        "    out.append(dict(\n",
        "        Cohort=coh, Thr=thr,\n",
        "        VAL_AUC=auc_val, TEST_AUC=auc_tst,\n",
        "        VAL_PRAUC=pr_val, TEST_PRAUC=pr_tst,\n",
        "        VAL_Brier=bri_val, TEST_Brier=bri_tst,\n",
        "        TP=TP, FP=FP, TN=TN, FN=FN,\n",
        "        Precision=prec, Recall=rec, Acc=acc, Cost=cost\n",
        "    ))\n",
        "\n",
        "    cal_preds.append(pd.DataFrame({\n",
        "        \"patient_id\": t[\"patient_id\"].values, \"cohort\": coh,\n",
        "        \"y_true\": yt, \"y_prob_cal\": pt_cal\n",
        "    }))\n",
        "\n",
        "res = pd.DataFrame(out)\n",
        "cal_preds = pd.concat(cal_preds, ignore_index=True)\n",
        "\n",
        "res_path = P26/\"p26b_percohort_platt_cost5to1.csv\"\n",
        "cal_path = P26/\"p26b_test_preds_calibrated.csv\"\n",
        "res.to_csv(res_path, index=False)\n",
        "cal_preds.to_csv(cal_path, index=False)\n",
        "print(\"💾 Guardado:\", res_path)\n",
        "print(res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D933X6dm8aQU",
        "outputId": "79da0c23-35af-4dc1-ae9d-e68f6302868f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Guardado: /content/drive/MyDrive/CognitivaAI/p26_intermodal/p26b_percohort_platt_cost5to1.csv\n",
            "  Cohort    Thr   VAL_AUC  TEST_AUC  VAL_PRAUC  TEST_PRAUC  VAL_Brier  \\\n",
            "0   OAS1  0.340  0.909259  0.753704   0.920794    0.735858   0.130844   \n",
            "1   OAS2  0.374  0.942149  0.651515   0.944233    0.727862   0.164272   \n",
            "\n",
            "   TEST_Brier  TP  FP  TN  FN  Precision    Recall       Acc  Cost  \n",
            "0    0.199075  14   9  18   6   0.608696  0.700000  0.680851    39  \n",
            "1    0.240842   8   4   7   4   0.666667  0.666667  0.652174    24  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/CognitivaAI\")\n",
        "P25 = BASE/\"p25_informe_final\"\n",
        "P26 = BASE/\"p26_intermodal\"\n",
        "\n",
        "mt = pd.read_csv(P25/\"p25_master_table.csv\")\n",
        "\n",
        "valb = pd.read_csv(P26/\"p26_val_preds.csv\")  # mismas VAL (pre-cal), ok para AUC\n",
        "tstb = pd.read_csv(P26/\"p26b_test_preds_calibrated.csv\")  # probas calibradas por cohorte\n",
        "tstb = tstb.rename(columns={\"y_prob_cal\":\"y_prob\"})\n",
        "\n",
        "def _metrics(df):\n",
        "    from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
        "    y, p = df[\"y_true\"].astype(int).to_numpy(), df[\"y_prob\"].astype(float).to_numpy()\n",
        "    return dict(AUC=float(roc_auc_score(y,p)),\n",
        "                PRAUC=float(average_precision_score(y,p)),\n",
        "                Brier=float(brier_score_loss(y,p)))\n",
        "\n",
        "rows=[]\n",
        "# ALL\n",
        "rows.append(dict(Pipeline=\"P26b\", Split=\"VAL\",  Cohort=\"ALL\", **_metrics(valb), Acc=np.nan, Precision=np.nan, Recall=np.nan, Thr=np.nan, Cost=np.nan, Notas=\"LATE+Platt\"))\n",
        "rows.append(dict(Pipeline=\"P26b\", Split=\"TEST\", Cohort=\"ALL\", **_metrics(tstb), Acc=np.nan, Precision=np.nan, Recall=np.nan, Thr=np.nan, Cost=np.nan, Notas=\"LATE+Platt\"))\n",
        "# Cohortes\n",
        "for coh in [\"OAS1\",\"OAS2\"]:\n",
        "    rows.append(dict(Pipeline=\"P26b\", Split=\"VAL\",  Cohort=coh, **_metrics(valb[valb[\"cohort\"]==coh]), Acc=np.nan, Precision=np.nan, Recall=np.nan, Thr=np.nan, Cost=np.nan, Notas=\"LATE+Platt\"))\n",
        "    rows.append(dict(Pipeline=\"P26b\", Split=\"TEST\", Cohort=coh, **_metrics(tstb[tstb[\"cohort\"]==coh]), Acc=np.nan, Precision=np.nan, Recall=np.nan, Thr=np.nan, Cost=np.nan, Notas=\"LATE+Platt\"))\n",
        "\n",
        "mt2 = pd.concat([mt, pd.DataFrame(rows)], ignore_index=True)\n",
        "mt2.to_csv(P25/\"p25_master_table.csv\", index=False)\n",
        "print(\"✅ Master table con P26b:\", P25/\"p25_master_table.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzhZgyqI9hch",
        "outputId": "715cf157-c961-4e9e-8f6c-b409ca41ade4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Master table con P26b: /content/drive/MyDrive/CognitivaAI/p25_informe_final/p25_master_table.csv\n"
          ]
        }
      ]
    }
  ]
}