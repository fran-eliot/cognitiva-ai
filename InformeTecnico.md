# üìë Informe T√©cnico ‚Äì Proyecto CognitivaAI  

---

## 1. Introducci√≥n General  

El presente informe constituye una documentaci√≥n exhaustiva, detallada y profundamente anal√≠tica del proyecto **CognitivaAI**, cuyo prop√≥sito ha sido explorar, dise√±ar, entrenar y evaluar modelos de aprendizaje profundo aplicados a la detecci√≥n de **deterioro cognitivo temprano** a partir de im√°genes de resonancia magn√©tica (MRI) de la base de datos **OASIS-2**, complementadas con informaci√≥n cl√≠nica.  

El proyecto surge con una doble motivaci√≥n:  

1. **Cient√≠fica y social:** el diagn√≥stico temprano de enfermedades neurodegenerativas, como la enfermedad de Alzheimer, es un desaf√≠o prioritario para la medicina actual. El uso de inteligencia artificial puede ayudar a identificar biomarcadores sutiles en im√°genes cerebrales y en datos cl√≠nicos que son dif√≠ciles de detectar por m√©todos tradicionales.  

2. **T√©cnica y experimental:** comprobar hasta qu√© punto distintas arquitecturas de redes neuronales convolucionales (CNNs) y transformadores visuales pueden capturar informaci√≥n discriminativa en un dataset limitado en tama√±o, enfrent√°ndose a problemas comunes como el sobreajuste, la calibraci√≥n de probabilidades y la estabilidad en validaci√≥n y test.  

La ejecuci√≥n del proyecto ha supuesto un recorrido iterativo y progresivo, articulado en una serie de **pipelines experimentales**, cada uno dise√±ado con objetivos concretos: desde probar un modelo sencillo con datos cl√≠nicos (P1) hasta explorar ensembles de arquitecturas avanzadas (P11).  

Cada pipeline ha sido registrado con su motivaci√≥n, configuraci√≥n, incidencias, m√©tricas y reflexi√≥n cr√≠tica, con el objetivo de construir no solo un conjunto de resultados, sino un **mapa de decisiones** que documenta los aprendizajes y trade-offs a lo largo del camino.  

---

## 2. Metodolog√≠a Global  

### 2.1 Dataset y preprocesamiento  

El dataset utilizado es **OASIS-2** (Open Access Series of Imaging Studies), que incluye im√°genes de resonancia magn√©tica (MRI) estructurales de individuos con y sin deterioro cognitivo, junto con informaci√≥n cl√≠nica asociada.  

- **MRI**: cada sujeto cuenta con una o m√°s adquisiciones de MRI estructural.  
- **Etiquetas cl√≠nicas**: la variable principal es el **CDR (Clinical Dementia Rating)**, dicotomizada en ‚Äúcontrol‚Äù vs ‚Äúdeterioro cognitivo‚Äù.  
- **Metadatos adicionales**: edad, sexo, puntuaciones cognitivas, entre otros.  

El preprocesamiento incluy√≥:  
- Mapeo de rutas a pacientes mediante ficheros `oas1_val_colab_mapped.csv` y `oas1_test_colab_mapped.csv`.  
- Normalizaci√≥n de intensidades.  
- Creaci√≥n de slices 2D a partir de vol√∫menes 3D para facilitar el entrenamiento en arquitecturas CNN est√°ndar.  
- Balanceo parcial mediante augmentaciones (flip horizontal, rotaciones leves).  

### 2.2 Infraestructura  

- **Google Colab Pro+**: principal entorno de ejecuci√≥n.  
- **GPU asignada**: NVIDIA T4 o A100, seg√∫n disponibilidad.  
- **Almacenamiento persistente**: Google Drive (`/MyDrive/CognitivaAI`).  
- **Librer√≠as clave**: PyTorch, timm (colecci√≥n de modelos), scikit-learn, pandas, matplotlib.  
- **Gesti√≥n de experimentos**: notebooks separados por pipeline, con guardado autom√°tico de m√©tricas y predicciones en CSV.  

### 2.3 M√©tricas de evaluaci√≥n  

Dada la naturaleza binaria y desbalanceada de la tarea, se utilizaron m√∫ltiples m√©tricas:  

- **AUC (ROC)**: medida global de discriminaci√≥n.  
- **PR-AUC**: m√°s informativa en datasets desbalanceados.  
- **Accuracy**: proporci√≥n de aciertos.  
- **Recall (sensibilidad)**: clave, ya que la tarea exige detectar la mayor cantidad de pacientes con deterioro cognitivo.  
- **Precision**: importante para evitar falsos positivos.  
- **Youden Index**: criterio alternativo de umbralizaci√≥n.  
- **@REC90 / @REC100**: configuraciones que maximizan el recall (sensibilidad) incluso a costa de la precisi√≥n.  

---

## 3. Evoluci√≥n por Pipelines  

### 3.1 Pipeline 1 ‚Äì Modelo cl√≠nico (XGB con OASIS-2)  

**Motivaci√≥n:**  
Probar un primer modelo utilizando exclusivamente variables cl√≠nicas del dataset OASIS-2, sin im√°genes. El objetivo era establecer un baseline puramente tabular.  

**Configuraci√≥n:**  
- Modelo: **XGBoost**.  
- Variables: edad, sexo, puntuaciones cognitivas, CDR.  
- Validaci√≥n cruzada interna.  

**Resultados:**  
- AUC (Test): 0.897.  
- Sin m√©tricas de PR-AUC reportadas, aunque se observ√≥ buen desempe√±o en t√©rminos de recall.  

**Reflexi√≥n:**  
Este baseline confirm√≥ que incluso con variables cl√≠nicas simples se puede alcanzar un nivel competitivo de discriminaci√≥n. Sin embargo, la dependencia exclusiva de variables cl√≠nicas limita la aplicabilidad general.  

---

### 3.2 Pipeline 2 ‚Äì Fusi√≥n cl√≠nica extendida  

**Motivaci√≥n:**  
Ampliar el modelo cl√≠nico con m√°s variables y verificar hasta qu√© punto un modelo tabular puede llegar cerca de la perfecci√≥n.  

**Configuraci√≥n:**  
- Modelo: **XGB**.  
- Variables: todas las disponibles en OASIS-2.  
- Validaci√≥n cruzada m√°s exhaustiva.  

**Resultados:**  
- AUC (Test): 0.991.  
- Recall cercano al 100%.  

**Reflexi√≥n:**  
El modelo cl√≠nico extendido alcanz√≥ una performance casi perfecta, aunque con riesgo evidente de **overfitting** dada la baja dimensionalidad del dataset. Sirvi√≥ como techo de referencia para comparar con modelos de MRI.  

---

### 3.3 Pipeline 3 ‚Äì MRI OASIS-2 con ResNet50  

**Motivaci√≥n:**  
Dar el salto a im√°genes MRI, usando ResNet50 como backbone en el dataset original de OASIS-2.  

**Configuraci√≥n:**  
- Modelo: **ResNet50 preentrenada en ImageNet**.  
- Dataset: MRI OASIS-2 (formato local).  
- Entrenamiento limitado por GPU local.  

**Resultados:**  
- AUC (Test): 0.938.  

**Reflexi√≥n:**  
Este pipeline demostr√≥ que el uso de im√°genes cerebrales puede ofrecer resultados competitivos con los cl√≠nicos, aunque a√∫n no superiores.  

---

### 3.4 Pipeline 5 ‚Äì ResNet18 + calibraci√≥n  

**Motivaci√≥n:**  
Probar una arquitectura m√°s ligera (ResNet18) con calibraci√≥n de probabilidades.  

**Configuraci√≥n:**  
- ResNet18 preentrenada.  
- Post-procesado: **Platt scaling**.  

**Resultados:**  
- AUC (Test): 0.724.  
- PR-AUC: 0.606.  
- Accuracy: 0.60.  
- Recall: 0.80.  
- Precision: 0.52.  

**Reflexi√≥n:**  
La calibraci√≥n mejor√≥ la interpretaci√≥n de scores, pero el backbone result√≥ limitado para esta tarea.  

---

### 3.5 Pipeline 6 ‚Äì EffNet-B3 en modo embeddings  

**Motivaci√≥n:**  
Usar EfficientNet-B3 como extractor de embeddings, sin fine-tuning completo.  

**Resultados:**  
- AUC (Test): 0.704.  
- PR-AUC: 0.623.  
- Accuracy: 0.70.  
- Recall: 0.90.  
- Precision: 0.60.  

**Reflexi√≥n:**  
Buen recall, aunque precision limitada.  

---

### 3.6 Pipeline 7 ‚Äì Fine-tuning completo de EfficientNet-B3  

**Motivaci√≥n:**  
Explotar la capacidad completa de EfficientNet-B3, ajustando todos los pesos.  

**Resultados:**  
- AUC (Test): 0.876.  
- PR-AUC: 0.762.  
- Accuracy: 0.745.  
- Recall: 1.0.  
- Precision: 0.625.  

**Reflexi√≥n:**  
Gran salto respecto a embeddings. El modelo captur√≥ patrones discriminativos m√°s profundos.  

---

### 3.7 Pipeline 9 ‚Äì EffNet-B3 stable  

**Motivaci√≥n:**  
Introducir t√©cnicas de estabilizaci√≥n para reducir variabilidad.  

**Resultados:**  
- AUC (Test): 0.740.  
- PR-AUC: 0.630.  
- Accuracy: 0.72.  
- Recall: 0.65.  
- Precision: 0.62.  

---

### 3.8 Pipeline 10 ‚Äì EffNet-B3 stable + calibraci√≥n  

**Motivaci√≥n:**  
Aplicar calibraci√≥n expl√≠cita de scores.  

**Resultados:**  
- AUC (Test): entre 0.546‚Äì0.583.  
- PR-AUC: 0.50‚Äì0.53.  
- Accuracy: 0.51‚Äì0.55.  
- Recall: 1.0.  
- Precision: 0.47‚Äì0.49.  

**Reflexi√≥n:**  
La calibraci√≥n no mejor√≥ la discriminaci√≥n; sacrific√≥ precision.  

---

### 3.9 Pipeline 10-ext ‚Äì Variantes TRIMMED y ensembles intra-backbone  

**Motivaci√≥n:**  
Explorar variantes de pooling de slices y ensembles dentro de EfficientNet-B3.  

**Resultados:**  
- TRIMMED: AUC (Test) 0.744, PR-AUC 0.746.  
- Ensemble (mean+trimmed+top7): AUC (Test) 0.754, PR-AUC 0.737.  

**Reflexi√≥n:**  
Los ensembles intra-backbone s√≠ ofrecieron mejoras.  

---

### 3.10 Pipeline 11 ‚Äì Backbones alternativos  

**Motivaci√≥n:**  
Explorar arquitecturas m√°s all√° de EfficientNet.  

**Modelos probados:**  
- ResNet-50.  
- DenseNet-121.  
- ConvNeXt-Tiny.  
- Swin-Tiny.  

**Resultados:**  
- **ResNet-50**: competitivo, AUC similar a EffNet-B3 estable.  
- **DenseNet-121**: resultados bajos (AUC ~0.34‚Äì0.46).  
- **ConvNeXt-Tiny**: desempe√±o bajo (AUC ~0.50).  
- **Swin-Tiny**: desempe√±o moderado, con variante top7 alcanzando AUC ~0.64.  

**Reflexi√≥n:**  
Ning√∫n backbone super√≥ claramente a EfficientNet-B3, aunque Swin mostr√≥ cierto potencial.  

---

### 3.11 Ensembles inter-backbone  

Se exploraron combinaciones entre diferentes backbones:  

- **Dirichlet ensembles**: priorizaron SwinTiny y ResNet, con mejoras modestas.  
- **Stacking con regresi√≥n log√≠stica**: no lograron generalizar bien, tendencia a sobreajuste.  
- **Isotonic calibration** sobre SwinTiny top7: mejor√≥ la estabilidad.  

---

## Experimentos en OASIS-2 (p13, p14 y p15)

### Preparaci√≥n de datos
- 367 exploraciones de OASIS-2, de las cuales 150 ten√≠an label cl√≠nico.  
- Generaci√≥n de 7340 slices (20 por volumen, edge_crop=0.08, z-score + CLAHE).  
- M√°scaras cerebrales FSL u Otsu.  
- Criterio de **una visita por paciente** para evitar leakage.  

---

### Pipeline p13
- Entrenamiento base con EfficientNet-B3 en cohortes reducidas.  
- 150 pacientes ‚Üí 105 train, 22 val, 23 test.  
- Resultados preliminares con recall alto, pero riesgo de sobreajuste.  

---

### Pipeline p14
- Reentrenamiento en GPU con **class weights** y datos copiados a **SSD local de Colab** para optimizar E/S.  
- M√©tricas robustas en validaci√≥n (AUC‚âà0.88) y recall=100% en test.  
- Integraci√≥n en cat√°logo de backbones como `oas2_effb3_p14`.  

---

### Pipeline p15 (Consolidaci√≥n y ensamble)
- Objetivo: consolidar resultados de p13/p14 con OASIS-1 (p11).  
- Se a√±adieron al cat√°logo de backbones (`oas2_effb3`, `oas2_effb3_p14`).  
- Generaci√≥n de features combinadas (69 pacientes en val, 70 en test).  
- Manejo de NaN: descarte de columnas con >40% y uso de modelos compatibles con NaN (HistGradientBoosting).  

**Resultados comparativos (VAL/TEST):**

| Pipeline | VAL AUC | VAL Acc | VAL Recall | TEST AUC | TEST Acc | TEST Recall |
|----------|---------|---------|------------|----------|----------|-------------|
| p13      | ~0.90   | 0.86    | 0.82       | ~0.77    | 0.78     | 0.83        |
| p14      | 0.88    | 0.86    | 0.82       | 0.71     | 0.70     | 1.00        |
| p15      | 0.94    | 0.84    | ~1.0       | 0.71     | 0.63‚Äì0.71 | 0.78‚Äì1.0   |

---

### Conclusi√≥n
- p13: prueba de integraci√≥n OASIS-2.  
- p14: optimizaci√≥n con balanceo y SSD local.  
- p15: consolidaci√≥n de resultados e integraci√≥n en ensambles OASIS-1+2, demostrando que la combinaci√≥n de backbones mejora recall y robustez del sistema.

---

## Fase 8 ‚Äì Ensembles refinados (p15 y p16)

**Contexto:**  
Tras la exploraci√≥n con OASIS-2 en los pipelines p13 y p14, se busc√≥ consolidar resultados y refinar el rendimiento a trav√©s de ensembles patient-level.

**Pipeline p15 (Consolidaci√≥n):**
- Revisi√≥n de inventarios y labels cl√≠nicos (367 scans totales, 150 con etiqueta).  
- Confirmaci√≥n del criterio de **una sesi√≥n por paciente** para evitar leakage.  
- Dataset resultante: 3.000 slices etiquetadas (20 slices por scan).  
- Dificultades: la latencia de E/S en Google Drive volvi√≥ a confirmar la necesidad de copiar los datos a SSD de Colab.

**Pipeline p16 (Refinamiento de ensembles):**
- Construcci√≥n de features patient-level a partir de m√∫ltiples backbones (`oas2_effb3`, `oas2_effb3_p14`, SwinTiny, ConvNeXt, etc.).  
- Manejo de NaNs:  
  - Eliminaci√≥n de features con >40% de valores perdidos.  
  - Imputaci√≥n + flags en Logistic Regression.  
  - Soporte nativo de NaN en HistGradientBoosting.  
- Ensayos con **Logistic Regression, HistGradientBoosting y blending (LR+HGB)**.

**Resultados comparativos:**
- Validaci√≥n (VAL):  
  - AUC‚âà0.95 con el blend, recall‚âà1.0 en cohortes OAS1.  
  - Cohorte OAS2 m√°s reducida ‚Üí m√©tricas inestables, pero recall=1.0.  
- Test (TEST):  
  - AUC‚âà0.69, recall‚âà0.78, mejorando a modelos individuales.  
  - LR e HGB muestran rendimientos complementarios.  
  - Blend logra mejor equilibrio entre sensibilidad y precisi√≥n.

**Conclusi√≥n:**  
La consolidaci√≥n en p15 y el refinamiento en p16 confirmaron que los ensembles permiten:  
- Reducir la varianza del rendimiento.  
- Mejorar recall en test (apropiado para cribado cl√≠nico).  
- Integrar backbones heterog√©neos en un modelo m√°s estable.

---

## Ensemble Calibration (p17)

### Metodolog√≠a
- Se construy√≥ un meta-ensemble con **stacking**, usando Logistic Regression sobre los outputs de modelos base (LR y HGB).  
- Posteriormente se aplic√≥ **Platt scaling** para calibrar las probabilidades, con selecci√≥n de umbral F1 en validaci√≥n (0.35).  
- Se evalu√≥ la calibraci√≥n mediante el **Brier Score** y curvas de calibraci√≥n.

### Resultados
- **Validaci√≥n (ALL):** AUC=0.78 | PRAUC=0.75 | Acc=0.74 | Recall=0.94 | F1=0.76 | Brier=0.176.  
- **Test (ALL):** AUC=0.70 | PRAUC=0.67 | Acc=0.63 | Recall=0.78 | F1=0.66 | Brier=0.227.  
- Cohortes:
  - **OAS1:** estable (VAL AUC‚âà0.84, TEST‚âà0.77).  
  - **OAS2:** bajo rendimiento (VAL‚âà0.31, TEST‚âà0.50) debido al tama√±o reducido (22‚Äì23 pacientes).  

### Conclusi√≥n
- La calibraci√≥n aporta **mejores probabilidades** y un modelo m√°s interpretable.  
- Se conserva la **sensibilidad (recall)**, clave en escenarios cl√≠nicos.  
- El dataset OAS2 sigue siendo un cuello de botella y deber√° reforzarse en futuros experimentos.

---

### Comparativa p16 vs p17

- **p16 (Blending):**
  - Meta-ensemble con LR + HGB, blending con Œ±=0.02.
  - Validaci√≥n muy fuerte (AUC‚âà0.95, Recall=1.0).
  - En test se mantuvo con AUC‚âà0.69 y Recall=0.78.
  - No incluye calibraci√≥n expl√≠cita ‚Üí probabilidades menos interpretables.

- **p17 (Calibration):**
  - Stacking con Logistic Regression + Platt scaling.
  - Validaci√≥n m√°s moderada (AUC‚âà0.78), pero con Brier Score=0.176 ‚Üí mejor calibraci√≥n.
  - En test: AUC‚âà0.70, Recall=0.78, F1‚âà0.66, Brier=0.227.
  - Probabilidades calibradas ‚Üí mayor confianza en la salida del modelo.

**Conclusi√≥n comparativa:**
- p16 maximiza m√©trica predictiva bruta (AUC) pero sin control de calibraci√≥n.
- p17 sacrifica algo de AUC, pero gana en **calidad de probabilidades y estabilidad cl√≠nica**.

 Pipeline | M√©todo principal                | VAL AUC | VAL Acc | VAL Recall | VAL F1 | TEST AUC | TEST Acc | TEST Recall | TEST F1 | Brier (Test) |
|----------|---------------------------------|---------|---------|------------|--------|----------|----------|-------------|---------|--------------|
| **p16**  | Blending (LR + HGB, Œ±=0.02)     | 0.95    | 0.84    | 1.00       | 0.84   | 0.69     | 0.64     | 0.78        | 0.64    | ‚Äì            |
| **p17**  | Stacking + Platt scaling (LR)   | 0.78    | 0.74    | 0.94       | 0.76   | 0.70     | 0.63     | 0.78        | 0.66    | 0.227        |

‚û°Ô∏è **p16** maximiz√≥ el AUC en validaci√≥n, pero con cierto riesgo de sobreajuste.  
‚û°Ô∏è **p17** ajust√≥ las probabilidades (Brier=0.227 en test) y mantuvo recall alto, ofreciendo **mejor calibraci√≥n** y utilidad cl√≠nica.

---

### Pipeline p18 ‚Äì Stacking multicapa

**Contexto:**  
Tras la calibraci√≥n en p17, se busc√≥ refinar la combinaci√≥n de modelos con un enfoque de **stacking multicapa**.  

**Dise√±o t√©cnico:**  
- **Modelos base:** Logistic Regression (L2), HistGradientBoosting, Gradient Boosting, Random Forest, Extra Trees.  
- **Meta-modelo:** regresi√≥n log√≠stica, con blending lineal optimizado (Œ±‚âà0.02).  
- **Entrenamiento:** OOF (out-of-fold) con 5 folds en validaci√≥n, evitando fuga de informaci√≥n.  
- **Evaluaci√≥n:** m√©tricas separadas por cohortes (OAS1, OAS2) y global.

**Resultados clave:**  
- **VAL (n=69):** AUC=0.92, Recall‚âà0.90, Precision‚âà0.78, F1=0.83.  
- **TEST (n=70):** AUC=0.67, Recall‚âà0.78, Precision‚âà0.59, F1=0.67.  
- Cohorte OAS1 mostr√≥ m√©tricas s√≥lidas (AUC‚âà0.67‚Äì0.68 en test), mientras que OAS2 sufri√≥ de escasez de datos (AUC=0.5).  
- Brier Score: 0.117 (VAL), 0.270 (TEST).

**Interpretaci√≥n:**  
El stacking avanz√≥ hacia un modelo m√°s **robusto y flexible**, destacando la contribuci√≥n de GB y RF como base learners m√°s relevantes.  
Sin embargo, la generalizaci√≥n en OAS2 sigue limitada por la baja cobertura de pacientes etiquetados.

---

## P19 ‚Äì Meta-Ensemble apilado

**Dise√±o y m√©todo**  
- Conjunto de 56 features por paciente (tras filtrado NaN) que integran variantes de pooling (mean, trimmed20, top-k, pmean_2) de m√∫ltiples backbones (incluyendo oas2_effb3, oas2_effb3_p14, Swin/ConvNeXt, etc.).  
- **Base learners:** LR (L2), √Årboles (GB/RF/ET), HistGradientBoosting, LightGBM y XGBoost.  
- **Validaci√≥n:** OOF KFold estratificado a nivel paciente (sin fuga). El meta-XGB se entrena sobre OOF; en TEST consume las predicciones de los base learners.  
- **Regularizaci√≥n y NaN:** filtrado de columnas con alta fracci√≥n de NaN; imputaci√≥n en modelos que lo exigen; √°rboles toleraron faltantes nativamente.  
- **Umbral:** punto de decisi√≥n basado en F1 en VAL; aplicado a TEST.  

**Resultados**  
- VAL (n=69): AUC=0.964; PRAUC=0.966; Acc=0.913; F1=0.897; Brier=0.071.  
- TEST (n=70): AUC=0.729; PRAUC=0.688; Acc=0.714; F1=0.630; Brier=0.226.  

**Interpretaci√≥n**  
- Alta capacidad discriminativa en validaci√≥n y buena calibraci√≥n (Brier bajo).  
- En TEST, el recall cae con precisi√≥n alta ‚Üí umbral sub-√≥ptimo bajo shift y posible sobreajuste leve del meta.  

**Observaciones**  
- LightGBM reporta *‚ÄúNo further splits with positive gain‚Äù*; esperable con dataset peque√±o y features ya decantadas. Conviene reducir complejidad o seleccionar features en meta.  

**Acciones siguientes (p20)**  
- Calibraci√≥n del meta (Platt/isot√≥nica) con OOF.  
- Umbrales por cohorte (global + espec√≠fico OAS2).  
- Meta simplificado (Elastic-Net) y selecci√≥n de features.  
- Stacking doble (blend LR/HGB ‚Üí meta-XGB).  
- Repeated KFold y agregaci√≥n.  
- Optimizaci√≥n de umbral por coste cl√≠nico (FN‚â´FP).

---

## P20: Meta-calibraci√≥n y umbrales por cohorte

**Dise√±o y m√©todo**
- Partiendo de las predicciones OOF del meta-ensemble (p19), se aplic√≥ calibraci√≥n de salida:
  - **Platt scaling (sigmoide)**
  - **Isotonic regression**
- Estrategias aplicadas:
  - **Global**: un √∫nico calibrador para todo el conjunto.
  - **Per-cohort**: calibradores independientes para OAS1 y OAS2.  
- Modelos meta evaluados: **HistGradientBoosting** (HGB) y **Logistic Regression** (LR).  
- Se fij√≥ el umbral en el punto F1-m√°ximo en VAL.

**Resultados**
- **HGB-Platt-Global:**  
  - VAL: AUC=0.789 | Acc=0.710 | F1=0.744 | Brier=0.186  
  - TEST: AUC=0.680 | Acc=0.600 | F1=0.641 | Brier=0.225  
- **HGB-Isotonic-PerC:**  
  - VAL: AUC=0.840 | Acc=0.725 | F1=0.753 | Brier=0.156  
  - TEST: AUC=0.679 | Acc=0.600 | F1=0.641 | Brier=0.253  
- **LR-Platt-Global:**  
  - VAL: AUC=0.743 | Acc=0.638 | F1=0.691 | Brier=0.209  
  - TEST: AUC=0.686 | Acc=0.629 | F1=0.658 | Brier=0.221  

**Interpretaci√≥n**
- La calibraci√≥n isot√≥nica en HGB dio las mejores m√©tricas en validaci√≥n (AUC‚âà0.84, Brier bajo).  
- En TEST, el recall se mantiene alto (‚âà0.78) pero la precisi√≥n baja ‚Üí trade-off esperado en cribado cl√≠nico.  
- La calibraci√≥n por cohortes apenas mejora respecto a global, pero confirma la heterogeneidad entre OAS1 y OAS2.

**Acciones siguientes**
- Usar estas calibraciones en combinaci√≥n con meta-stacking (p19) para mejorar la robustez.  
- Evaluar selecci√≥n de umbrales con coste cl√≠nico (penalizaci√≥n mayor a FN).  
- Explorar Elastic-Net como meta m√°s interpretable.  

---

## P21: Meta-refine con stacking compacto

**Dise√±o.** Se redujo el conjunto de se√±ales de p19/p20 a un meta-stacking compacto:
- **Filtrado de NaN:** de 56 ‚Üí 36 columnas (umbral 40%).
- **Base learners:** LR (penalizaci√≥n L2), HGB, LightGBM (LGBM), XGBoost (XGB).
- **Validaci√≥n:** OOF estratificado a nivel paciente (sin fuga), construcci√≥n de meta-features (VAL: 69√ó4, TEST: 70√ó4).
- **Umbral:** F1-m√°ximo en validaci√≥n (**0.45**).

**Resultados.**
- **Validaci√≥n (n=69):** AUC 0.955, PRAUC 0.931, Acc 0.870, F1 0.862, Brier 0.082.
- **Test (n=70):** AUC 0.653, PRAUC 0.587, Acc 0.643, F1 0.627, Brier 0.285.

**Interpretaci√≥n.**
- Excelente discriminaci√≥n/calibraci√≥n en VAL (Brier bajo); degradaci√≥n en TEST sugiere **shift de distribuci√≥n** (OAS1/OAS2) y/o sensibilidad del umbral global.
- Mensajes de LGBM (*no positive gain*) coherentes con tama√±o de muestra y features ya ‚Äúresumidas‚Äù; conviene regularizar y/o limitar profundidad/hojas.
- El meta compacto facilita **calibraci√≥n por cohorte** y **optimizaci√≥n de umbral por coste** en el siguiente paso.

**Acciones siguientes.**
1. **Calibraci√≥n por cohorte** (OAS1/OAS2) y por coste (FN‚â´FP) con umbrales espec√≠ficos.
2. **Regularizaci√≥n del meta** (Elastic-Net o LR con C bajo) y reducci√≥n de complejidad en √°rboles (m√°x profundidad/hojas).
3. **Robustez**: Repeated KFold (5√ó5) + agregaci√≥n para reducir varianza.
4. **Ablaciones**: retirar se√±ales altamente correlacionadas y medir impacto en TEST.

---

## P22: Meta-Ablation con calibraci√≥n avanzada

**Dise√±o:**  
- Se utilizaron 56 features derivadas de m√∫ltiples backbones, de las que se mantuvieron 36 tras filtrar NaN>40%.  
- Dos modelos base: **Logistic Regression (LR)** con imputaci√≥n+escalado y **HistGradientBoosting (HGB)** tolerante a NaNs.  
- Calibraci√≥n aplicada:  
  - **Platt scaling (sigmoid).**  
  - **Isotonic regression.**  
- Validaci√≥n con **Stratified KFold OOF** en validaci√≥n (69 pacientes). Predicciones finales sobre 70 pacientes en test.  
- Umbral √≥ptimo seleccionado en validaci√≥n maximizando F1 (‚âà0.30‚Äì0.35).  

**Resultados:**  

| Modelo         | VAL AUC | VAL F1 | TEST AUC | TEST F1 | Brier (VAL/TEST) |
|----------------|---------|--------|----------|---------|------------------|
| LR-Platt       | 0.73    | 0.68   | 0.67     | 0.69    | 0.208 / 0.219    |
| LR-Isotonic    | 0.86    | 0.75   | 0.67     | 0.65    | 0.145 / 0.231    |
| HGB-Platt      | 0.82    | 0.75   | 0.70     | 0.63    | 0.174 / 0.222    |
| HGB-Isotonic   | 0.89    | 0.77   | 0.67     | 0.64    | 0.133 / 0.239    |
| Blend-Isotonic | 0.90    | 0.79   | 0.68     | 0.62    | 0.130 / 0.229    |

**Interpretaci√≥n:**  
- La **isot√≥nica** mejora la calibraci√≥n (menor Brier en VAL), aunque en test tiende a sobreajustar.  
- La **sigmoide** mantiene recall alto, √∫til en escenarios de cribado.  
- El **blend isot√≥nico** en validaci√≥n se acerca a un meta-modelo ideal, pero en test muestra la fragilidad del shift entre cohortes.  

**Conclusi√≥n:**  
P22 constituye un *estudio de ablaci√≥n* para analizar calibraci√≥n y combinar predicciones calibradas. Confirma que la elecci√≥n de m√©todo depende de la prioridad cl√≠nica (recall vs calibraci√≥n probabil√≠stica). Los resultados se usar√°n como referencia en p23 para meta-ensembles m√°s robustos.

---

## 4. Comparativa Global  

(Tabla de consolidaci√≥n de pipelines, m√©tricas ya integrada en README).  

---

## 5. Principales Desaf√≠os  

1. **T√©cnicos:**  
   - Errores recurrentes de montaje de Google Drive.  
   - Saturaci√≥n de Colab por sesiones largas.  
   - Problemas de compatibilidad de pesos (`strict=False`).  
   - Colisiones de nombres de columnas en CSV.  

2. **Metodol√≥gicos:**  
   - Tama√±o reducido del dataset ‚Üí alto riesgo de sobreajuste.  
   - Dificultad para calibrar scores manteniendo discriminaci√≥n.  
   - Varianza alta entre seeds.  

3. **Pr√°cticos:**  
   - Limitaci√≥n de tiempo de GPU.  
   - Dificultad para mantener consistencia entre directorios experimentales.  
   - Saturaci√≥n de logs y necesidad de bit√°cora exhaustiva.  

---

## 6. Lecciones Aprendidas y Decisiones Clave  

- EffNet-B3 sigue siendo un backbone robusto.  
- Los ensembles intra-backbone mejoran resultados.  
- Los backbones alternativos no superan claramente a EffNet-B3, salvo Swin en configuraciones concretas.  
- La combinaci√≥n de enfoques (ensembles) es clave antes de saltar a multimodal.  

---

## 7. Conclusiones y Pr√≥ximos Pasos  

- Consolidar ensembles h√≠bridos.  
- Avanzar hacia multimodal integrando variables cl√≠nicas.  
- Documentar exhaustivamente para posible publicaci√≥n.  

Actualizado: 07/09/2025 15:44