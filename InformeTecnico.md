# üìë Informe T√©cnico ‚Äì Proyecto CognitivaAI  

---

## 1. Introducci√≥n General  

El presente informe constituye una documentaci√≥n exhaustiva, detallada y profundamente anal√≠tica del proyecto **CognitivaAI**, cuyo prop√≥sito ha sido explorar, dise√±ar, entrenar y evaluar modelos de aprendizaje profundo aplicados a la detecci√≥n de **deterioro cognitivo temprano** a partir de im√°genes de resonancia magn√©tica (MRI) de la base de datos **OASIS-2**, complementadas con informaci√≥n cl√≠nica.  

El proyecto surge con una doble motivaci√≥n:  

1. **Cient√≠fica y social:** el diagn√≥stico temprano de enfermedades neurodegenerativas, como la enfermedad de Alzheimer, es un desaf√≠o prioritario para la medicina actual. El uso de inteligencia artificial puede ayudar a identificar biomarcadores sutiles en im√°genes cerebrales y en datos cl√≠nicos que son dif√≠ciles de detectar por m√©todos tradicionales.  

2. **T√©cnica y experimental:** comprobar hasta qu√© punto distintas arquitecturas de redes neuronales convolucionales (CNNs) y transformadores visuales pueden capturar informaci√≥n discriminativa en un dataset limitado en tama√±o, enfrent√°ndose a problemas comunes como el sobreajuste, la calibraci√≥n de probabilidades y la estabilidad en validaci√≥n y test.  

La ejecuci√≥n del proyecto ha supuesto un recorrido iterativo y progresivo, articulado en una serie de **pipelines experimentales**, cada uno dise√±ado con objetivos concretos: desde probar un modelo sencillo con datos cl√≠nicos (P1) hasta explorar ensembles de arquitecturas avanzadas (P11).  

Cada pipeline ha sido registrado con su motivaci√≥n, configuraci√≥n, incidencias, m√©tricas y reflexi√≥n cr√≠tica, con el objetivo de construir no solo un conjunto de resultados, sino un **mapa de decisiones** que documenta los aprendizajes y trade-offs a lo largo del camino.  

---

## 2. Metodolog√≠a Global  

### 2.1 Dataset y preprocesamiento  

El dataset utilizado es **OASIS-2** (Open Access Series of Imaging Studies), que incluye im√°genes de resonancia magn√©tica (MRI) estructurales de individuos con y sin deterioro cognitivo, junto con informaci√≥n cl√≠nica asociada.  

- **MRI**: cada sujeto cuenta con una o m√°s adquisiciones de MRI estructural.  
- **Etiquetas cl√≠nicas**: la variable principal es el **CDR (Clinical Dementia Rating)**, dicotomizada en ‚Äúcontrol‚Äù vs ‚Äúdeterioro cognitivo‚Äù.  
- **Metadatos adicionales**: edad, sexo, puntuaciones cognitivas, entre otros.  

El preprocesamiento incluy√≥:  
- Mapeo de rutas a pacientes mediante ficheros `oas1_val_colab_mapped.csv` y `oas1_test_colab_mapped.csv`.  
- Normalizaci√≥n de intensidades.  
- Creaci√≥n de slices 2D a partir de vol√∫menes 3D para facilitar el entrenamiento en arquitecturas CNN est√°ndar.  
- Balanceo parcial mediante augmentaciones (flip horizontal, rotaciones leves).  

### 2.2 Infraestructura  

- **Google Colab Pro+**: principal entorno de ejecuci√≥n.  
- **GPU asignada**: NVIDIA T4 o A100, seg√∫n disponibilidad.  
- **Almacenamiento persistente**: Google Drive (`/MyDrive/CognitivaAI`).  
- **Librer√≠as clave**: PyTorch, timm (colecci√≥n de modelos), scikit-learn, pandas, matplotlib.  
- **Gesti√≥n de experimentos**: notebooks separados por pipeline, con guardado autom√°tico de m√©tricas y predicciones en CSV.  

### 2.3 M√©tricas de evaluaci√≥n  

Dada la naturaleza binaria y desbalanceada de la tarea, se utilizaron m√∫ltiples m√©tricas:  

- **AUC (ROC)**: medida global de discriminaci√≥n.  
- **PR-AUC**: m√°s informativa en datasets desbalanceados.  
- **Accuracy**: proporci√≥n de aciertos.  
- **Recall (sensibilidad)**: clave, ya que la tarea exige detectar la mayor cantidad de pacientes con deterioro cognitivo.  
- **Precision**: importante para evitar falsos positivos.  
- **Youden Index**: criterio alternativo de umbralizaci√≥n.  
- **@REC90 / @REC100**: configuraciones que maximizan el recall (sensibilidad) incluso a costa de la precisi√≥n.  

---

## 3. Evoluci√≥n por Pipelines  

### 3.1 Pipeline 1 ‚Äì Modelo cl√≠nico (XGB con OASIS-2)  

**Motivaci√≥n:**  
Probar un primer modelo utilizando exclusivamente variables cl√≠nicas del dataset OASIS-2, sin im√°genes. El objetivo era establecer un baseline puramente tabular.  

**Configuraci√≥n:**  
- Modelo: **XGBoost**.  
- Variables: edad, sexo, puntuaciones cognitivas, CDR.  
- Validaci√≥n cruzada interna.  

**Resultados:**  
- AUC (Test): 0.897.  
- Sin m√©tricas de PR-AUC reportadas, aunque se observ√≥ buen desempe√±o en t√©rminos de recall.  

**Reflexi√≥n:**  
Este baseline confirm√≥ que incluso con variables cl√≠nicas simples se puede alcanzar un nivel competitivo de discriminaci√≥n. Sin embargo, la dependencia exclusiva de variables cl√≠nicas limita la aplicabilidad general.  

---

### 3.2 Pipeline 2 ‚Äì Fusi√≥n cl√≠nica extendida  

**Motivaci√≥n:**  
Ampliar el modelo cl√≠nico con m√°s variables y verificar hasta qu√© punto un modelo tabular puede llegar cerca de la perfecci√≥n.  

**Configuraci√≥n:**  
- Modelo: **XGB**.  
- Variables: todas las disponibles en OASIS-2.  
- Validaci√≥n cruzada m√°s exhaustiva.  

**Resultados:**  
- AUC (Test): 0.991.  
- Recall cercano al 100%.  

**Reflexi√≥n:**  
El modelo cl√≠nico extendido alcanz√≥ una performance casi perfecta, aunque con riesgo evidente de **overfitting** dada la baja dimensionalidad del dataset. Sirvi√≥ como techo de referencia para comparar con modelos de MRI.  

---

### 3.3 Pipeline 3 ‚Äì MRI OASIS-2 con ResNet50  

**Motivaci√≥n:**  
Dar el salto a im√°genes MRI, usando ResNet50 como backbone en el dataset original de OASIS-2.  

**Configuraci√≥n:**  
- Modelo: **ResNet50 preentrenada en ImageNet**.  
- Dataset: MRI OASIS-2 (formato local).  
- Entrenamiento limitado por GPU local.  

**Resultados:**  
- AUC (Test): 0.938.  

**Reflexi√≥n:**  
Este pipeline demostr√≥ que el uso de im√°genes cerebrales puede ofrecer resultados competitivos con los cl√≠nicos, aunque a√∫n no superiores.  

---

### 3.4 Pipeline 5 ‚Äì ResNet18 + calibraci√≥n  

**Motivaci√≥n:**  
Probar una arquitectura m√°s ligera (ResNet18) con calibraci√≥n de probabilidades.  

**Configuraci√≥n:**  
- ResNet18 preentrenada.  
- Post-procesado: **Platt scaling**.  

**Resultados:**  
- AUC (Test): 0.724.  
- PR-AUC: 0.606.  
- Accuracy: 0.60.  
- Recall: 0.80.  
- Precision: 0.52.  

**Reflexi√≥n:**  
La calibraci√≥n mejor√≥ la interpretaci√≥n de scores, pero el backbone result√≥ limitado para esta tarea.  

---

### 3.5 Pipeline 6 ‚Äì EffNet-B3 en modo embeddings  

**Motivaci√≥n:**  
Usar EfficientNet-B3 como extractor de embeddings, sin fine-tuning completo.  

**Resultados:**  
- AUC (Test): 0.704.  
- PR-AUC: 0.623.  
- Accuracy: 0.70.  
- Recall: 0.90.  
- Precision: 0.60.  

**Reflexi√≥n:**  
Buen recall, aunque precision limitada.  

---

### 3.6 Pipeline 7 ‚Äì Fine-tuning completo de EfficientNet-B3  

**Motivaci√≥n:**  
Explotar la capacidad completa de EfficientNet-B3, ajustando todos los pesos.  

**Resultados:**  
- AUC (Test): 0.876.  
- PR-AUC: 0.762.  
- Accuracy: 0.745.  
- Recall: 1.0.  
- Precision: 0.625.  

**Reflexi√≥n:**  
Gran salto respecto a embeddings. El modelo captur√≥ patrones discriminativos m√°s profundos.  

---

### 3.7 Pipeline 9 ‚Äì EffNet-B3 stable  

**Motivaci√≥n:**  
Introducir t√©cnicas de estabilizaci√≥n para reducir variabilidad.  

**Resultados:**  
- AUC (Test): 0.740.  
- PR-AUC: 0.630.  
- Accuracy: 0.72.  
- Recall: 0.65.  
- Precision: 0.62.  

---

### 3.8 Pipeline 10 ‚Äì EffNet-B3 stable + calibraci√≥n  

**Motivaci√≥n:**  
Aplicar calibraci√≥n expl√≠cita de scores.  

**Resultados:**  
- AUC (Test): entre 0.546‚Äì0.583.  
- PR-AUC: 0.50‚Äì0.53.  
- Accuracy: 0.51‚Äì0.55.  
- Recall: 1.0.  
- Precision: 0.47‚Äì0.49.  

**Reflexi√≥n:**  
La calibraci√≥n no mejor√≥ la discriminaci√≥n; sacrific√≥ precision.  

---

### 3.9 Pipeline 10-ext ‚Äì Variantes TRIMMED y ensembles intra-backbone  

**Motivaci√≥n:**  
Explorar variantes de pooling de slices y ensembles dentro de EfficientNet-B3.  

**Resultados:**  
- TRIMMED: AUC (Test) 0.744, PR-AUC 0.746.  
- Ensemble (mean+trimmed+top7): AUC (Test) 0.754, PR-AUC 0.737.  

**Reflexi√≥n:**  
Los ensembles intra-backbone s√≠ ofrecieron mejoras.  

---

### 3.10 Pipeline 11 ‚Äì Backbones alternativos  

**Motivaci√≥n:**  
Explorar arquitecturas m√°s all√° de EfficientNet.  

**Modelos probados:**  
- ResNet-50.  
- DenseNet-121.  
- ConvNeXt-Tiny.  
- Swin-Tiny.  

**Resultados:**  
- **ResNet-50**: competitivo, AUC similar a EffNet-B3 estable.  
- **DenseNet-121**: resultados bajos (AUC ~0.34‚Äì0.46).  
- **ConvNeXt-Tiny**: desempe√±o bajo (AUC ~0.50).  
- **Swin-Tiny**: desempe√±o moderado, con variante top7 alcanzando AUC ~0.64.  

**Reflexi√≥n:**  
Ning√∫n backbone super√≥ claramente a EfficientNet-B3, aunque Swin mostr√≥ cierto potencial.  

---

### 3.11 Ensembles inter-backbone  

Se exploraron combinaciones entre diferentes backbones:  

- **Dirichlet ensembles**: priorizaron SwinTiny y ResNet, con mejoras modestas.  
- **Stacking con regresi√≥n log√≠stica**: no lograron generalizar bien, tendencia a sobreajuste.  
- **Isotonic calibration** sobre SwinTiny top7: mejor√≥ la estabilidad.  

---

## Experimentos en OASIS-2 (p13, p14 y p15)

### Preparaci√≥n de datos
- 367 exploraciones de OASIS-2, de las cuales 150 ten√≠an label cl√≠nico.  
- Generaci√≥n de 7340 slices (20 por volumen, edge_crop=0.08, z-score + CLAHE).  
- M√°scaras cerebrales FSL u Otsu.  
- Criterio de **una visita por paciente** para evitar leakage.  

---

### Pipeline p13
- Entrenamiento base con EfficientNet-B3 en cohortes reducidas.  
- 150 pacientes ‚Üí 105 train, 22 val, 23 test.  
- Resultados preliminares con recall alto, pero riesgo de sobreajuste.  

---

### Pipeline p14
- Reentrenamiento en GPU con **class weights** y datos copiados a **SSD local de Colab** para optimizar E/S.  
- M√©tricas robustas en validaci√≥n (AUC‚âà0.88) y recall=100% en test.  
- Integraci√≥n en cat√°logo de backbones como `oas2_effb3_p14`.  

---

### Pipeline p15 (Consolidaci√≥n y ensamble)
- Objetivo: consolidar resultados de p13/p14 con OASIS-1 (p11).  
- Se a√±adieron al cat√°logo de backbones (`oas2_effb3`, `oas2_effb3_p14`).  
- Generaci√≥n de features combinadas (69 pacientes en val, 70 en test).  
- Manejo de NaN: descarte de columnas con >40% y uso de modelos compatibles con NaN (HistGradientBoosting).  

**Resultados comparativos (VAL/TEST):**

| Pipeline | VAL AUC | VAL Acc | VAL Recall | TEST AUC | TEST Acc | TEST Recall |
|----------|---------|---------|------------|----------|----------|-------------|
| p13      | ~0.90   | 0.86    | 0.82       | ~0.77    | 0.78     | 0.83        |
| p14      | 0.88    | 0.86    | 0.82       | 0.71     | 0.70     | 1.00        |
| p15      | 0.94    | 0.84    | ~1.0       | 0.71     | 0.63‚Äì0.71 | 0.78‚Äì1.0   |

---

### Conclusi√≥n
- p13: prueba de integraci√≥n OASIS-2.  
- p14: optimizaci√≥n con balanceo y SSD local.  
- p15: consolidaci√≥n de resultados e integraci√≥n en ensambles OASIS-1+2, demostrando que la combinaci√≥n de backbones mejora recall y robustez del sistema.

---

## 4. Comparativa Global  

(Tabla de consolidaci√≥n de pipelines, m√©tricas ya integrada en README).  

---

## 5. Principales Desaf√≠os  

1. **T√©cnicos:**  
   - Errores recurrentes de montaje de Google Drive.  
   - Saturaci√≥n de Colab por sesiones largas.  
   - Problemas de compatibilidad de pesos (`strict=False`).  
   - Colisiones de nombres de columnas en CSV.  

2. **Metodol√≥gicos:**  
   - Tama√±o reducido del dataset ‚Üí alto riesgo de sobreajuste.  
   - Dificultad para calibrar scores manteniendo discriminaci√≥n.  
   - Varianza alta entre seeds.  

3. **Pr√°cticos:**  
   - Limitaci√≥n de tiempo de GPU.  
   - Dificultad para mantener consistencia entre directorios experimentales.  
   - Saturaci√≥n de logs y necesidad de bit√°cora exhaustiva.  

---

## 6. Lecciones Aprendidas y Decisiones Clave  

- EffNet-B3 sigue siendo un backbone robusto.  
- Los ensembles intra-backbone mejoran resultados.  
- Los backbones alternativos no superan claramente a EffNet-B3, salvo Swin en configuraciones concretas.  
- La combinaci√≥n de enfoques (ensembles) es clave antes de saltar a multimodal.  

---

## 7. Conclusiones y Pr√≥ximos Pasos  

- Consolidar ensembles h√≠bridos.  
- Avanzar hacia multimodal integrando variables cl√≠nicas.  
- Documentar exhaustivamente para posible publicaci√≥n.  

Actualizado: 05/09/2025 12:13